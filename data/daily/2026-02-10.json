{
  "date": "2026-02-10",
  "fetchedAt": "2026-02-10T07:41:53.475Z",
  "github": [
    {
      "id": "gh-vllm-project-vllm",
      "source": "github",
      "title": "vllm",
      "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
      "url": "https://github.com/vllm-project/vllm",
      "date": "2026-02-10",
      "authors": [
        "vllm-project"
      ],
      "tags": [
        "amd",
        "blackwell",
        "cuda",
        "deepseek",
        "deepseek-v3",
        "gpt",
        "gpt-oss",
        "inference",
        "kimi",
        "llama",
        "llm",
        "llm-serving",
        "model-serving",
        "moe",
        "openai",
        "pytorch",
        "qwen",
        "qwen3",
        "tpu",
        "transformer"
      ],
      "matchedGroup": "accelerators",
      "stars": 69940,
      "language": "Python",
      "forks": 13334,
      "fullName": "vllm-project/vllm"
    },
    {
      "id": "gh-FFmpeg-FFmpeg",
      "source": "github",
      "title": "FFmpeg",
      "description": "Mirror of https://git.ffmpeg.org/ffmpeg.git",
      "url": "https://github.com/FFmpeg/FFmpeg",
      "date": "2026-02-10",
      "authors": [
        "FFmpeg"
      ],
      "tags": [
        "audio",
        "c",
        "ffmpeg",
        "fft",
        "hevc",
        "hls",
        "matroska",
        "mp4",
        "mpeg",
        "multimedia",
        "rtmp",
        "rtsp",
        "streaming",
        "video",
        "webm"
      ],
      "matchedGroup": "optimization",
      "stars": 57060,
      "language": "C",
      "forks": 13465,
      "fullName": "FFmpeg/FFmpeg"
    },
    {
      "id": "gh-jax-ml-jax",
      "source": "github",
      "title": "jax",
      "description": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more",
      "url": "https://github.com/jax-ml/jax",
      "date": "2026-02-10",
      "authors": [
        "jax-ml"
      ],
      "tags": [
        "jax"
      ],
      "matchedGroup": "accelerators",
      "stars": 34828,
      "language": "Python",
      "forks": 3413,
      "fullName": "jax-ml/jax"
    },
    {
      "id": "gh-Tencent-ncnn",
      "source": "github",
      "title": "ncnn",
      "description": "ncnn is a high-performance neural network inference framework optimized for the mobile platform",
      "url": "https://github.com/Tencent/ncnn",
      "date": "2026-02-10",
      "authors": [
        "Tencent"
      ],
      "tags": [
        "android",
        "arm-neon",
        "artificial-intelligence",
        "caffe",
        "darknet",
        "deep-learning",
        "high-preformance",
        "inference",
        "ios",
        "keras",
        "mlir",
        "mxnet",
        "ncnn",
        "neural-network",
        "onnx",
        "pytorch",
        "riscv",
        "simd",
        "tensorflow",
        "vulkan"
      ],
      "matchedGroup": "frameworks",
      "stars": 22768,
      "language": "C++",
      "forks": 4395,
      "fullName": "Tencent/ncnn"
    },
    {
      "id": "gh-microsoft-onnxruntime",
      "source": "github",
      "title": "onnxruntime",
      "description": "ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator",
      "url": "https://github.com/microsoft/onnxruntime",
      "date": "2026-02-10",
      "authors": [
        "microsoft"
      ],
      "tags": [
        "ai-framework",
        "deep-learning",
        "hardware-acceleration",
        "machine-learning",
        "neural-networks",
        "onnx",
        "pytorch",
        "scikit-learn",
        "tensorflow"
      ],
      "matchedGroup": "frameworks",
      "stars": 19247,
      "language": "C++",
      "forks": 3693,
      "fullName": "microsoft/onnxruntime"
    },
    {
      "id": "gh-ZLMediaKit-ZLMediaKit",
      "source": "github",
      "title": "ZLMediaKit",
      "description": "WebRTC/RTSP/RTMP/HTTP/HLS/HTTP-FLV/WebSocket-FLV/HTTP-TS/HTTP-fMP4/WebSocket-TS/WebSocket-fMP4/GB28181/SRT/STUN/TURN server and client framework based on C++11",
      "url": "https://github.com/ZLMediaKit/ZLMediaKit",
      "date": "2026-02-10",
      "authors": [
        "ZLMediaKit"
      ],
      "tags": [
        "flv",
        "gb28181",
        "hls",
        "http",
        "http-flv",
        "http-fmp4",
        "http-ts",
        "live",
        "media-server",
        "mp4",
        "rtmp",
        "rtp",
        "rtsp",
        "srt",
        "stun",
        "ts",
        "turn",
        "webrtc",
        "websocket",
        "websocket-flv"
      ],
      "matchedGroup": "optimization",
      "stars": 16644,
      "language": "C++",
      "forks": 3946,
      "fullName": "ZLMediaKit/ZLMediaKit"
    },
    {
      "id": "gh-zephyrproject-rtos-zephyr",
      "source": "github",
      "title": "zephyr",
      "description": "Primary Git Repository for the Zephyr Project. Zephyr is a new generation, scalable, optimized, secure RTOS for multiple hardware architectures.",
      "url": "https://github.com/zephyrproject-rtos/zephyr",
      "date": "2026-02-10",
      "authors": [
        "zephyrproject-rtos"
      ],
      "tags": [
        "bluetooth",
        "bluetooth-le",
        "embedded",
        "embedded-c",
        "iot",
        "mcu",
        "microcontroller",
        "real-time",
        "rtos",
        "zephyr",
        "zephyr-rtos",
        "zephyros"
      ],
      "matchedGroup": "optimization",
      "stars": 14427,
      "language": "C",
      "forks": 8630,
      "fullName": "zephyrproject-rtos/zephyr"
    },
    {
      "id": "gh-apache-tvm",
      "source": "github",
      "title": "tvm",
      "description": "Open Machine Learning Compiler Framework",
      "url": "https://github.com/apache/tvm",
      "date": "2026-02-10",
      "authors": [
        "apache"
      ],
      "tags": [
        "compiler",
        "deep-learning",
        "gpu",
        "javascript",
        "machine-learning",
        "metal",
        "opencl",
        "performance",
        "rocm",
        "spirv",
        "tensor",
        "tvm",
        "vulkan"
      ],
      "matchedGroup": "frameworks",
      "stars": 13108,
      "language": "Python",
      "forks": 3789,
      "fullName": "apache/tvm"
    },
    {
      "id": "gh-NVIDIA-TensorRT-LLM",
      "source": "github",
      "title": "TensorRT-LLM",
      "description": "TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and supports state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in a performant way.",
      "url": "https://github.com/NVIDIA/TensorRT-LLM",
      "date": "2026-02-10",
      "authors": [
        "NVIDIA"
      ],
      "tags": [
        "blackwell",
        "cuda",
        "llm-serving",
        "moe",
        "pytorch"
      ],
      "matchedGroup": "frameworks",
      "stars": 12849,
      "language": "Python",
      "forks": 2095,
      "fullName": "NVIDIA/TensorRT-LLM"
    },
    {
      "id": "gh-owncast-owncast",
      "source": "github",
      "title": "owncast",
      "description": "Take control over your live stream video by running it yourself.  Streaming + chat out of the box.",
      "url": "https://github.com/owncast/owncast",
      "date": "2026-02-10",
      "authors": [
        "owncast"
      ],
      "tags": [
        "activitypub",
        "broadcasting",
        "chat",
        "decentralized",
        "federation",
        "fediverse",
        "golang",
        "hacktoberfest",
        "hls",
        "live",
        "livestream",
        "owncast",
        "rtmp",
        "self-hosted",
        "streaming-video",
        "video"
      ],
      "matchedGroup": "optimization",
      "stars": 10926,
      "language": "Go",
      "forks": 1176,
      "fullName": "owncast/owncast"
    },
    {
      "id": "gh-k2-fsa-sherpa-onnx",
      "source": "github",
      "title": "sherpa-onnx",
      "description": "Speech-to-text, text-to-speech, speaker diarization, speech enhancement, source separation, and VAD using next-gen Kaldi with onnxruntime without Internet connection. Support embedded systems, Android, iOS, HarmonyOS, Raspberry Pi, RISC-V, RK NPU, Axera NPU, Ascend NPU, x86_64 servers, websocket server/client, support 12 programming languages",
      "url": "https://github.com/k2-fsa/sherpa-onnx",
      "date": "2026-02-10",
      "authors": [
        "k2-fsa"
      ],
      "tags": [
        "aarch64",
        "android",
        "arm32",
        "asr",
        "cpp",
        "csharp",
        "dotnet",
        "ios",
        "lazarus",
        "linux",
        "macos",
        "mfc",
        "object-pascal",
        "onnx",
        "raspberry-pi",
        "risc-v",
        "speech-to-text",
        "text-to-speech",
        "vits",
        "windows"
      ],
      "matchedGroup": "accelerators",
      "stars": 10239,
      "language": "C++",
      "forks": 1151,
      "fullName": "k2-fsa/sherpa-onnx"
    },
    {
      "id": "gh-openvinotoolkit-openvino",
      "source": "github",
      "title": "openvino",
      "description": "OpenVINO‚Ñ¢ is an open source toolkit for optimizing and deploying AI inference",
      "url": "https://github.com/openvinotoolkit/openvino",
      "date": "2026-02-10",
      "authors": [
        "openvinotoolkit"
      ],
      "tags": [
        "ai",
        "computer-vision",
        "deep-learning",
        "deploy-ai",
        "diffusion-models",
        "generative-ai",
        "good-first-issue",
        "inference",
        "llm-inference",
        "natural-language-processing",
        "nlp",
        "openvino",
        "optimize-ai",
        "performance-boost",
        "recommendation-system",
        "speech-recognition",
        "stable-diffusion",
        "transformers",
        "yolo"
      ],
      "matchedGroup": "frameworks",
      "stars": 9679,
      "language": "C++",
      "forks": 3021,
      "fullName": "openvinotoolkit/openvino"
    },
    {
      "id": "gh-mikel-brostrom-boxmot",
      "source": "github",
      "title": "boxmot",
      "description": "BoxMOT: Pluggable SOTA multi-object tracking modules for segmentation, object detection and pose estimation models",
      "url": "https://github.com/mikel-brostrom/boxmot",
      "date": "2026-02-10",
      "authors": [
        "mikel-brostrom"
      ],
      "tags": [
        "boosttrack",
        "botsort",
        "bytetrack",
        "clip",
        "deep-learning",
        "deepocsort",
        "improvedassociation",
        "machine-learning",
        "mot",
        "mots",
        "multi-object-tracking",
        "multi-object-tracking-segmentation",
        "ocsort",
        "oriented-bounding-box-tracking",
        "osnet",
        "segmentation",
        "strongsort",
        "tensorrt",
        "tracking-by-detection",
        "yolo"
      ],
      "matchedGroup": "frameworks",
      "stars": 8005,
      "language": "Python",
      "forks": 1886,
      "fullName": "mikel-brostrom/boxmot"
    },
    {
      "id": "gh-NexaAI-nexa-sdk",
      "source": "github",
      "title": "nexa-sdk",
      "description": "Run frontier LLMs and VLMs with day-0 model support across GPU, NPU, and CPU, with comprehensive runtime coverage for PC (Python/C++), mobile (Android & iOS), and Linux/IoT (Arm64 & x86 Docker). Supporting OpenAI GPT-OSS, IBM Granite-4, Qwen-3-VL, Gemma-3n, Ministral-3, and more.",
      "url": "https://github.com/NexaAI/nexa-sdk",
      "date": "2026-02-10",
      "authors": [
        "NexaAI"
      ],
      "tags": [
        "gemma3",
        "go",
        "gpt-oss",
        "granite4",
        "llama",
        "llama3",
        "llm",
        "on-device-ai",
        "phi3",
        "qwen3",
        "qwen3vl",
        "sdk",
        "stable-diffusion",
        "vlm"
      ],
      "matchedGroup": "accelerators",
      "stars": 7685,
      "language": "Kotlin",
      "forks": 949,
      "fullName": "NexaAI/nexa-sdk"
    },
    {
      "id": "gh-wang-xinyu-tensorrtx",
      "source": "github",
      "title": "tensorrtx",
      "description": "Implementation of popular deep learning networks with TensorRT network definition API",
      "url": "https://github.com/wang-xinyu/tensorrtx",
      "date": "2026-02-10",
      "authors": [
        "wang-xinyu"
      ],
      "tags": [
        "arcface",
        "crnn",
        "detr",
        "mnasnet",
        "mobilenetv2",
        "mobilenetv3",
        "resnet",
        "retinaface",
        "squeezenet",
        "swin-transformer",
        "tensorrt",
        "yolo11",
        "yolov3",
        "yolov3-spp",
        "yolov4",
        "yolov5",
        "yolov7",
        "yolov8",
        "yolov9"
      ],
      "matchedGroup": "frameworks",
      "stars": 7675,
      "language": "C++",
      "forks": 1868,
      "fullName": "wang-xinyu/tensorrtx"
    },
    {
      "id": "gh-open-edge-platform-anomalib",
      "source": "github",
      "title": "anomalib",
      "description": "An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.",
      "url": "https://github.com/open-edge-platform/anomalib",
      "date": "2026-02-10",
      "authors": [
        "open-edge-platform"
      ],
      "tags": [
        "anomaly-detection",
        "anomaly-localization",
        "anomaly-segmentation",
        "geti",
        "neural-network-compression",
        "openvino",
        "unsupervised-learning"
      ],
      "matchedGroup": "frameworks",
      "stars": 5377,
      "language": "Python",
      "forks": 873,
      "fullName": "open-edge-platform/anomalib"
    },
    {
      "id": "gh-inngest-inngest",
      "source": "github",
      "title": "inngest",
      "description": "The leading workflow orchestration platform.  Run stateful step functions and AI workflows on serverless, servers, or the edge.",
      "url": "https://github.com/inngest/inngest",
      "date": "2026-02-10",
      "authors": [
        "inngest"
      ],
      "tags": [
        "cli",
        "event-driven",
        "event-driven-architecture",
        "queues",
        "serverless",
        "serverless-functions",
        "workflow-engine",
        "workflows"
      ],
      "matchedGroup": "edge-ai",
      "stars": 4785,
      "language": "Go",
      "forks": 246,
      "fullName": "inngest/inngest"
    },
    {
      "id": "gh-ant-media-Ant-Media-Server",
      "source": "github",
      "title": "Ant-Media-Server",
      "description": "Ant Media Server ‚Äî Ultra-low latency streaming engine with WebRTC (~0.5s), SRT, RTMP, HLS, CMAF, adaptive bitrate, transcoding & scaling",
      "url": "https://github.com/ant-media/Ant-Media-Server",
      "date": "2026-02-10",
      "authors": [
        "ant-media"
      ],
      "tags": [
        "abr",
        "android-sdk",
        "ant-media",
        "broadcast",
        "cmaf",
        "hls",
        "ios-sdk",
        "live-streaming",
        "low-latency",
        "media-server",
        "open-source",
        "recording",
        "rtmp",
        "rtsp",
        "streaming",
        "streaming-server",
        "ultra-low-latency",
        "video-streaming",
        "webrtc"
      ],
      "matchedGroup": "optimization",
      "stars": 4628,
      "language": "Java",
      "forks": 676,
      "fullName": "ant-media/Ant-Media-Server"
    },
    {
      "id": "gh-YosysHQ-yosys",
      "source": "github",
      "title": "yosys",
      "description": "Yosys Open SYnthesis Suite",
      "url": "https://github.com/YosysHQ/yosys",
      "date": "2026-02-10",
      "authors": [
        "YosysHQ"
      ],
      "tags": [],
      "matchedGroup": "synthesis-pnr",
      "stars": 4272,
      "language": "C++",
      "forks": 1036,
      "fullName": "YosysHQ/yosys"
    },
    {
      "id": "gh-pytorch-executorch",
      "source": "github",
      "title": "executorch",
      "description": "On-device AI across mobile, embedded and edge for PyTorch",
      "url": "https://github.com/pytorch/executorch",
      "date": "2026-02-10",
      "authors": [
        "pytorch"
      ],
      "tags": [
        "deep-learning",
        "embedded",
        "gpu",
        "machine-learning",
        "mobile",
        "neural-network",
        "tensor"
      ],
      "matchedGroup": "edge-ai",
      "stars": 4250,
      "language": "Python",
      "forks": 830,
      "fullName": "pytorch/executorch"
    },
    {
      "id": "gh-beclab-Olares",
      "source": "github",
      "title": "Olares",
      "description": "Olares: An Open-Source Personal Cloud to Reclaim Your Data",
      "url": "https://github.com/beclab/Olares",
      "date": "2026-02-10",
      "authors": [
        "beclab"
      ],
      "tags": [
        "ai-agents",
        "ai-privacy",
        "edge-ai",
        "home-automation",
        "home-cloud",
        "home-server",
        "homelab",
        "homeserver",
        "kubernetes",
        "local-ai",
        "mcp",
        "model-serving",
        "personal-cloud",
        "self-hosted"
      ],
      "matchedGroup": "edge-ai",
      "stars": 4040,
      "language": "Go",
      "forks": 205,
      "fullName": "beclab/Olares"
    },
    {
      "id": "gh-iree-org-iree",
      "source": "github",
      "title": "iree",
      "description": "A retargetable MLIR-based machine learning compiler and runtime toolkit.",
      "url": "https://github.com/iree-org/iree",
      "date": "2026-02-10",
      "authors": [
        "iree-org"
      ],
      "tags": [
        "compiler",
        "cuda",
        "jax",
        "machine-learning",
        "mlir",
        "onnx",
        "pytorch",
        "runtime",
        "spirv",
        "tensorflow",
        "vulkan"
      ],
      "matchedGroup": "frameworks",
      "stars": 3596,
      "language": "C++",
      "forks": 835,
      "fullName": "iree-org/iree"
    },
    {
      "id": "gh-dinoki-ai-osaurus",
      "source": "github",
      "title": "osaurus",
      "description": "AI edge infrastructure for macOS. Run local or cloud models, share tools across apps via MCP, and power AI workflows with a native, always-on runtime.",
      "url": "https://github.com/dinoki-ai/osaurus",
      "date": "2026-02-10",
      "authors": [
        "dinoki-ai"
      ],
      "tags": [
        "anthropic",
        "apple-foundation-models",
        "apple-intelligence",
        "apple-neural-engine",
        "llm",
        "mcp",
        "mcp-server",
        "mlx",
        "openai",
        "swift"
      ],
      "matchedGroup": "edge-ai",
      "stars": 3357,
      "language": "Swift",
      "forks": 139,
      "fullName": "dinoki-ai/osaurus"
    },
    {
      "id": "gh-OvenMediaLabs-OvenMediaEngine",
      "source": "github",
      "title": "OvenMediaEngine",
      "description": "OvenMediaEngine (OME) is a Sub-Second Latency Live Streaming Server with Large-Scale and High-Definition. #WebRTC #LLHLS",
      "url": "https://github.com/OvenMediaLabs/OvenMediaEngine",
      "date": "2026-02-10",
      "authors": [
        "OvenMediaLabs"
      ],
      "tags": [
        "broadcasting",
        "cmaf",
        "hls",
        "large-scale-streaming",
        "live-streaming-server",
        "lldash",
        "llhls",
        "low-latency",
        "low-latency-dash",
        "low-latency-hls",
        "low-latency-http",
        "ome",
        "ovenmediaengine",
        "rtmp",
        "rtmp-to-webrtc",
        "streaming",
        "streaming-server",
        "sub-second-latency",
        "ultra-low-latency",
        "webrtc"
      ],
      "matchedGroup": "optimization",
      "stars": 3059,
      "language": "C++",
      "forks": 1107,
      "fullName": "OvenMediaLabs/OvenMediaEngine"
    },
    {
      "id": "gh-atopile-atopile",
      "source": "github",
      "title": "atopile",
      "description": "Design circuit boards with code! ‚ú® Get software-like design reuse üöÄ, validation, version control and collaboration in hardware; starting with electronics ‚ö°Ô∏è",
      "url": "https://github.com/atopile/atopile",
      "date": "2026-02-10",
      "authors": [
        "atopile"
      ],
      "tags": [
        "cad",
        "eda",
        "electronics",
        "engineering",
        "tools-and-automation"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 3053,
      "language": "Python",
      "forks": 165,
      "fullName": "atopile/atopile"
    },
    {
      "id": "gh-tensorflow-tflite-micro",
      "source": "github",
      "title": "tflite-micro",
      "description": "Infrastructure to enable deployment of ML models to low-power resource-constrained embedded targets (including microcontrollers and digital signal processors).",
      "url": "https://github.com/tensorflow/tflite-micro",
      "date": "2026-02-10",
      "authors": [
        "tensorflow"
      ],
      "tags": [],
      "matchedGroup": "edge-ai",
      "stars": 2753,
      "language": "C++",
      "forks": 990,
      "fullName": "tensorflow/tflite-micro"
    },
    {
      "id": "gh-vllm-project-llm-compressor",
      "source": "github",
      "title": "llm-compressor",
      "description": "Transformers-compatible library for applying various compression algorithms to LLMs for optimized deployment with vLLM",
      "url": "https://github.com/vllm-project/llm-compressor",
      "date": "2026-02-10",
      "authors": [
        "vllm-project"
      ],
      "tags": [
        "compression",
        "quantization",
        "sparsity"
      ],
      "matchedGroup": "model-compression",
      "stars": 2716,
      "language": "Python",
      "forks": 388,
      "fullName": "vllm-project/llm-compressor"
    },
    {
      "id": "gh-pytorch-ao",
      "source": "github",
      "title": "ao",
      "description": "PyTorch native quantization and sparsity for training and inference",
      "url": "https://github.com/pytorch/ao",
      "date": "2026-02-10",
      "authors": [
        "pytorch"
      ],
      "tags": [
        "brrr",
        "cuda",
        "dtypes",
        "float8",
        "inference",
        "llama",
        "mx",
        "pytorch",
        "quantization",
        "sparsity",
        "training",
        "transformer"
      ],
      "matchedGroup": "model-compression",
      "stars": 2669,
      "language": "Python",
      "forks": 425,
      "fullName": "pytorch/ao"
    },
    {
      "id": "gh-stochasticai-xTuring",
      "source": "github",
      "title": "xTuring",
      "description": "Build, personalize and control your own LLMs. From data pre-processing to fine-tuning, xTuring provides an easy way to personalize open-source LLMs. Join our discord community: https://discord.gg/TgHXuSJEk6",
      "url": "https://github.com/stochasticai/xTuring",
      "date": "2026-02-10",
      "authors": [
        "stochasticai"
      ],
      "tags": [
        "adapter",
        "deep-learning",
        "fine-tuning",
        "finetuning",
        "gen-ai",
        "generative-ai",
        "gpt-2",
        "gpt-j",
        "language-model",
        "llama",
        "llm",
        "lora",
        "mistral",
        "mixed-precision",
        "peft",
        "quantization"
      ],
      "matchedGroup": "model-compression",
      "stars": 2664,
      "language": "Python",
      "forks": 207,
      "fullName": "stochasticai/xTuring"
    },
    {
      "id": "gh-intel-neural-compressor",
      "source": "github",
      "title": "neural-compressor",
      "description": "SOTA low-bit LLM quantization (INT8/FP8/MXFP8/INT4/MXFP4/NVFP4) & sparsity; leading model compression techniques on PyTorch, TensorFlow, and ONNX Runtime",
      "url": "https://github.com/intel/neural-compressor",
      "date": "2026-02-10",
      "authors": [
        "intel"
      ],
      "tags": [
        "auto-tuning",
        "awq",
        "fp4",
        "gptq",
        "int4",
        "int8",
        "knowledge-distillation",
        "large-language-models",
        "low-precision",
        "mxformat",
        "post-training-quantization",
        "pruning",
        "quantization",
        "quantization-aware-training",
        "smoothquant",
        "sparsegpt",
        "sparsity"
      ],
      "matchedGroup": "model-compression",
      "stars": 2582,
      "language": "Python",
      "forks": 296,
      "fullName": "intel/neural-compressor"
    },
    {
      "id": "gh-quic-aimet",
      "source": "github",
      "title": "aimet",
      "description": "AIMET is a library that provides advanced quantization and compression techniques for trained neural network models.",
      "url": "https://github.com/quic/aimet",
      "date": "2026-02-10",
      "authors": [
        "quic"
      ],
      "tags": [
        "auto-ml",
        "compression",
        "deep-learning",
        "deep-neural-networks",
        "machine-learning",
        "network-compression",
        "network-quantization",
        "open-source",
        "opensource",
        "pruning",
        "quantization"
      ],
      "matchedGroup": "model-compression",
      "stars": 2557,
      "language": "Python",
      "forks": 442,
      "fullName": "quic/aimet"
    },
    {
      "id": "gh-KiCad-kicad-source-mirror",
      "source": "github",
      "title": "kicad-source-mirror",
      "description": "This is an active mirror of the KiCad development branch, which is hosted at GitLab (updated every time something is pushed). Pull requests on GitHub are not accepted or watched.",
      "url": "https://github.com/KiCad/kicad-source-mirror",
      "date": "2026-02-10",
      "authors": [
        "KiCad"
      ],
      "tags": [
        "cad",
        "eda",
        "electronics",
        "engineering",
        "kicad",
        "linux",
        "macos",
        "pcb",
        "schematic",
        "windows",
        "wxwidgets"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 2512,
      "language": "C++",
      "forks": 592,
      "fullName": "KiCad/kicad-source-mirror"
    },
    {
      "id": "gh-google-coral-coralnpu",
      "source": "github",
      "title": "coralnpu",
      "description": "A machine learning accelerator core designed for energy-efficient AI at the edge.",
      "url": "https://github.com/google-coral/coralnpu",
      "date": "2026-02-10",
      "authors": [
        "google-coral"
      ],
      "tags": [],
      "matchedGroup": "accelerators",
      "stars": 2057,
      "language": "Emacs Lisp",
      "forks": 231,
      "fullName": "google-coral/coralnpu"
    },
    {
      "id": "gh-llvm-circt",
      "source": "github",
      "title": "circt",
      "description": "Circuit IR Compilers and Tools",
      "url": "https://github.com/llvm/circt",
      "date": "2026-02-10",
      "authors": [
        "llvm"
      ],
      "tags": [
        "circt",
        "llvm",
        "mlir"
      ],
      "matchedGroup": "frameworks",
      "stars": 2028,
      "language": "C++",
      "forks": 420,
      "fullName": "llvm/circt"
    },
    {
      "id": "gh-NVIDIA-Model-Optimizer",
      "source": "github",
      "title": "Model-Optimizer",
      "description": "A unified library of SOTA model optimization techniques like quantization, pruning, distillation, speculative decoding, etc. It compresses deep learning models for downstream deployment frameworks like TensorRT-LLM, TensorRT, vLLM, etc. to optimize inference speed.",
      "url": "https://github.com/NVIDIA/Model-Optimizer",
      "date": "2026-02-10",
      "authors": [
        "NVIDIA"
      ],
      "tags": [],
      "matchedGroup": "model-compression",
      "stars": 1964,
      "language": "Python",
      "forks": 265,
      "fullName": "NVIDIA/Model-Optimizer"
    },
    {
      "id": "gh-google-xls",
      "source": "github",
      "title": "xls",
      "description": "XLS: Accelerated HW Synthesis",
      "url": "https://github.com/google/xls",
      "date": "2026-02-10",
      "authors": [
        "google"
      ],
      "tags": [
        "compiler",
        "high-level-synthesis",
        "hls",
        "mid-level-synthesis",
        "open-source",
        "pipeline",
        "verilog"
      ],
      "matchedGroup": "optimization",
      "stars": 1425,
      "language": "C++",
      "forks": 221,
      "fullName": "google/xls"
    },
    {
      "id": "gh-tscircuit-tscircuit",
      "source": "github",
      "title": "tscircuit",
      "description": "Create real electronics with Typescript and React",
      "url": "https://github.com/tscircuit/tscircuit",
      "date": "2026-02-10",
      "authors": [
        "tscircuit"
      ],
      "tags": [
        "cad",
        "eda",
        "electronics",
        "engineering",
        "hacktoberfest",
        "kicad",
        "react",
        "typescript"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 1417,
      "language": "TypeScript",
      "forks": 127,
      "fullName": "tscircuit/tscircuit"
    },
    {
      "id": "gh-jd-opensource-xllm",
      "source": "github",
      "title": "xllm",
      "description": "A high-performance inference engine for LLMs, optimized for diverse AI accelerators.",
      "url": "https://github.com/jd-opensource/xllm",
      "date": "2026-02-10",
      "authors": [
        "jd-opensource"
      ],
      "tags": [
        "deepseek",
        "inference",
        "inference-engine",
        "large-language-models",
        "llm-inference",
        "qwen"
      ],
      "matchedGroup": "accelerators",
      "stars": 1023,
      "language": "C++",
      "forks": 140,
      "fullName": "jd-opensource/xllm"
    },
    {
      "id": "gh-ModelCloud-GPTQModel",
      "source": "github",
      "title": "GPTQModel",
      "description": "LLM model quantization (compression) toolkit with hw acceleration support for Nvidia CUDA, AMD ROCm, Intel XPU and Intel/AMD/Apple CPU via HF, vLLM, and SGLang.",
      "url": "https://github.com/ModelCloud/GPTQModel",
      "date": "2026-02-10",
      "authors": [
        "ModelCloud"
      ],
      "tags": [
        "gptq",
        "optimum",
        "peft",
        "quantization",
        "sglang",
        "transformers",
        "vllm"
      ],
      "matchedGroup": "model-compression",
      "stars": 1014,
      "language": "Python",
      "forks": 159,
      "fullName": "ModelCloud/GPTQModel"
    },
    {
      "id": "gh-kendryte-nncase",
      "source": "github",
      "title": "nncase",
      "description": "Open deep learning compiler stack for Kendryte AI accelerators ‚ú®",
      "url": "https://github.com/kendryte/nncase",
      "date": "2026-02-10",
      "authors": [
        "kendryte"
      ],
      "tags": [
        "ai-compiler",
        "compiler",
        "deep-learning",
        "k210",
        "k230",
        "k510",
        "kendryte",
        "llm",
        "neural-network"
      ],
      "matchedGroup": "accelerators",
      "stars": 859,
      "language": "C#",
      "forks": 205,
      "fullName": "kendryte/nncase"
    },
    {
      "id": "gh-sourcenetwork-defradb",
      "source": "github",
      "title": "defradb",
      "description": "DefraDB is a Peer-to-Peer Edge-First Database. It's the core data storage system for the Source Ecosystem, built with IPLD, LibP2P, CRDTs, and Semantic open web properties.",
      "url": "https://github.com/sourcenetwork/defradb",
      "date": "2026-02-10",
      "authors": [
        "sourcenetwork"
      ],
      "tags": [
        "crdt",
        "database",
        "distributed",
        "documentdb",
        "edge-ai",
        "edge-compute",
        "edge-first",
        "graphql",
        "linked-data",
        "local-first-software",
        "nosql",
        "peer-to-peer",
        "semantic-web"
      ],
      "matchedGroup": "edge-ai",
      "stars": 837,
      "language": "Go",
      "forks": 72,
      "fullName": "sourcenetwork/defradb"
    },
    {
      "id": "gh-Opencode-DCP-opencode-dynamic-context-pruning",
      "source": "github",
      "title": "opencode-dynamic-context-pruning",
      "description": "Dynamic context pruning plugin for OpenCode - intelligently manages conversation context to optimize token usage",
      "url": "https://github.com/Opencode-DCP/opencode-dynamic-context-pruning",
      "date": "2026-02-10",
      "authors": [
        "Opencode-DCP"
      ],
      "tags": [],
      "matchedGroup": "model-compression",
      "stars": 770,
      "language": "TypeScript",
      "forks": 53,
      "fullName": "Opencode-DCP/opencode-dynamic-context-pruning"
    },
    {
      "id": "gh-pytorch-helion",
      "source": "github",
      "title": "helion",
      "description": "A Python-embedded DSL that makes it easy to write fast, scalable ML kernels with minimal boilerplate.",
      "url": "https://github.com/pytorch/helion",
      "date": "2026-02-10",
      "authors": [
        "pytorch"
      ],
      "tags": [],
      "matchedGroup": "edge-ai",
      "stars": 744,
      "language": "Python",
      "forks": 103,
      "fullName": "pytorch/helion"
    },
    {
      "id": "gh-buddy-compiler-buddy-mlir",
      "source": "github",
      "title": "buddy-mlir",
      "description": "An MLIR-based compiler framework bridges DSLs (domain-specific languages) to DSAs (domain-specific architectures).",
      "url": "https://github.com/buddy-compiler/buddy-mlir",
      "date": "2026-02-10",
      "authors": [
        "buddy-compiler"
      ],
      "tags": [],
      "matchedGroup": "frameworks",
      "stars": 694,
      "language": "C++",
      "forks": 233,
      "fullName": "buddy-compiler/buddy-mlir"
    },
    {
      "id": "gh-google-heir",
      "source": "github",
      "title": "heir",
      "description": "A compiler for homomorphic encryption",
      "url": "https://github.com/google/heir",
      "date": "2026-02-10",
      "authors": [
        "google"
      ],
      "tags": [
        "homomorphic-encryption",
        "mlir"
      ],
      "matchedGroup": "frameworks",
      "stars": 678,
      "language": "C++",
      "forks": 122,
      "fullName": "google/heir"
    },
    {
      "id": "gh-lablup-backend.ai",
      "source": "github",
      "title": "backend.ai",
      "description": "Backend.AI is a streamlined, container-based computing cluster platform that hosts popular computing/ML frameworks and diverse programming languages, with pluggable heterogeneous accelerator support including CUDA GPU, ROCm GPU, Gaudi NPU, Google TPU, GraphCore IPU and other NPUs.",
      "url": "https://github.com/lablup/backend.ai",
      "date": "2026-02-10",
      "authors": [
        "lablup"
      ],
      "tags": [
        "api",
        "backendai",
        "cloud-computing",
        "containers",
        "distributed-computing",
        "docker",
        "documentation",
        "hpc",
        "monitoring",
        "paas",
        "python"
      ],
      "matchedGroup": "accelerators",
      "stars": 611,
      "language": "Python",
      "forks": 163,
      "fullName": "lablup/backend.ai"
    },
    {
      "id": "gh-Xilinx-mlir-aie",
      "source": "github",
      "title": "mlir-aie",
      "description": "An MLIR-based toolchain for AMD AI Engine-enabled devices.",
      "url": "https://github.com/Xilinx/mlir-aie",
      "date": "2026-02-10",
      "authors": [
        "Xilinx"
      ],
      "tags": [
        "iron",
        "llvm",
        "mlir",
        "npu",
        "python"
      ],
      "matchedGroup": "accelerators",
      "stars": 582,
      "language": "C",
      "forks": 168,
      "fullName": "Xilinx/mlir-aie"
    },
    {
      "id": "gh-OvenMediaLabs-OvenPlayer",
      "source": "github",
      "title": "OvenPlayer",
      "description": "OvenPlayer is JavaScript-based LLHLS and WebRTC Player for OvenMediaEngine.",
      "url": "https://github.com/OvenMediaLabs/OvenPlayer",
      "date": "2026-02-10",
      "authors": [
        "OvenMediaLabs"
      ],
      "tags": [
        "hls",
        "html5",
        "html5-player",
        "javascript",
        "javascript-player",
        "lldash",
        "llhls",
        "low-latency-dash",
        "low-latency-hls",
        "low-latency-http",
        "mpeg-dash",
        "ovenmediaengine",
        "ovenplayer",
        "streaming",
        "sub-second-latency-streaming",
        "webrtc"
      ],
      "matchedGroup": "optimization",
      "stars": 571,
      "language": "JavaScript",
      "forks": 147,
      "fullName": "OvenMediaLabs/OvenPlayer"
    },
    {
      "id": "gh-The-OpenROAD-Project-OpenROAD-flow-scripts",
      "source": "github",
      "title": "OpenROAD-flow-scripts",
      "description": "OpenROAD's scripts implementing an RTL-to-GDS Flow. Documentation at https://openroad-flow-scripts.readthedocs.io/en/latest/",
      "url": "https://github.com/The-OpenROAD-Project/OpenROAD-flow-scripts",
      "date": "2026-02-10",
      "authors": [
        "The-OpenROAD-Project"
      ],
      "tags": [
        "def",
        "eda",
        "gdsii",
        "lef",
        "opendb-database",
        "openroad",
        "rtl",
        "tcl",
        "timing-analysis",
        "verilog"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 561,
      "language": "Verilog",
      "forks": 427,
      "fullName": "The-OpenROAD-Project/OpenROAD-flow-scripts"
    },
    {
      "id": "gh-Ascend-pytorch",
      "source": "github",
      "title": "pytorch",
      "description": "Ascend PyTorch adapter (torch_npu). Mirror of https://gitcode.com/Ascend/pytorch",
      "url": "https://github.com/Ascend/pytorch",
      "date": "2026-02-10",
      "authors": [
        "Ascend"
      ],
      "tags": [
        "ascend",
        "deep-learning",
        "pytorch"
      ],
      "matchedGroup": "accelerators",
      "stars": 486,
      "language": "Python",
      "forks": 42,
      "fullName": "Ascend/pytorch"
    },
    {
      "id": "gh-lmk123568-StreamUI",
      "source": "github",
      "title": "StreamUI",
      "description": "A minimal and lightweight video streaming management platform ‰∏Ä‰∏™ÊûÅÁÆÄËΩªÈáèÁöÑËßÜÈ¢ëÊµÅÂ™í‰ΩìÁÆ°ÁêÜÂπ≥Âè∞",
      "url": "https://github.com/lmk123568/StreamUI",
      "date": "2026-02-10",
      "authors": [
        "lmk123568"
      ],
      "tags": [
        "flv",
        "fmp4",
        "hls",
        "nvr",
        "rtmp",
        "rtsp",
        "stream",
        "ts",
        "video",
        "webrtc",
        "zlmediakit"
      ],
      "matchedGroup": "optimization",
      "stars": 446,
      "language": "HTML",
      "forks": 81,
      "fullName": "lmk123568/StreamUI"
    },
    {
      "id": "gh-SonySemiconductorSolutions-mct-model-optimization",
      "source": "github",
      "title": "mct-model-optimization",
      "description": "Model Compression Toolkit (MCT) is an open source project for neural network model optimization under efficient, constrained hardware. This project provides researchers, developers, and engineers advanced quantization and compression tools for deploying state-of-the-art neural networks.",
      "url": "https://github.com/SonySemiconductorSolutions/mct-model-optimization",
      "date": "2026-02-10",
      "authors": [
        "SonySemiconductorSolutions"
      ],
      "tags": [
        "deep-learning",
        "deep-neural-networks",
        "edge-ai",
        "machine-learning",
        "network-compression",
        "network-quantization",
        "neural-network",
        "optimizer",
        "ptq",
        "pytorch",
        "qat",
        "quantization",
        "tensorflow"
      ],
      "matchedGroup": "model-compression",
      "stars": 432,
      "language": "Python",
      "forks": 81,
      "fullName": "SonySemiconductorSolutions/mct-model-optimization"
    },
    {
      "id": "gh-Tencent-AngelSlim",
      "source": "github",
      "title": "AngelSlim",
      "description": "Model compression toolkit engineered for enhanced usability, comprehensiveness, and efficiency.",
      "url": "https://github.com/Tencent/AngelSlim",
      "date": "2026-02-10",
      "authors": [
        "Tencent"
      ],
      "tags": [
        "audio",
        "deepseek",
        "diffusion",
        "eagle",
        "fp4",
        "hunyuan",
        "llm",
        "llm-compression",
        "quantization",
        "qwen",
        "speculative-decoding",
        "vlm"
      ],
      "matchedGroup": "model-compression",
      "stars": 325,
      "language": "Python",
      "forks": 40,
      "fullName": "Tencent/AngelSlim"
    },
    {
      "id": "gh-apache-ignite-3",
      "source": "github",
      "title": "ignite-3",
      "description": "Apache Ignite 3",
      "url": "https://github.com/apache/ignite-3",
      "date": "2026-02-10",
      "authors": [
        "apache"
      ],
      "tags": [
        "big-data",
        "cache",
        "cloud",
        "data-management-platform",
        "database",
        "distributed-sql-database",
        "ignite",
        "in-memory-computing",
        "in-memory-database",
        "iot",
        "network-client",
        "network-server",
        "sql"
      ],
      "matchedGroup": "ai-hardware",
      "stars": 305,
      "language": "Java",
      "forks": 135,
      "fullName": "apache/ignite-3"
    },
    {
      "id": "gh-EnzymeAD-Reactant.jl",
      "source": "github",
      "title": "Reactant.jl",
      "description": "Optimize Julia Functions With MLIR and XLA for High-Performance Execution on CPU, GPU, TPU and more.",
      "url": "https://github.com/EnzymeAD/Reactant.jl",
      "date": "2026-02-10",
      "authors": [
        "EnzymeAD"
      ],
      "tags": [],
      "matchedGroup": "accelerators",
      "stars": 295,
      "language": "Julia",
      "forks": 49,
      "fullName": "EnzymeAD/Reactant.jl"
    },
    {
      "id": "gh-ModelEngine-Group-unified-cache-management",
      "source": "github",
      "title": "unified-cache-management",
      "description": "Persist and reuse KV Cache to speedup your LLM.",
      "url": "https://github.com/ModelEngine-Group/unified-cache-management",
      "date": "2026-02-10",
      "authors": [
        "ModelEngine-Group"
      ],
      "tags": [
        "ascend",
        "cuda",
        "deepseek",
        "dram",
        "gpu",
        "hbm",
        "kvcache",
        "llm",
        "nfs",
        "npu",
        "ssd",
        "torch",
        "ucm",
        "vllm"
      ],
      "matchedGroup": "accelerators",
      "stars": 249,
      "language": "Python",
      "forks": 62,
      "fullName": "ModelEngine-Group/unified-cache-management"
    },
    {
      "id": "gh-tenstorrent-tt-mlir",
      "source": "github",
      "title": "tt-mlir",
      "description": "Tenstorrent MLIR compiler",
      "url": "https://github.com/tenstorrent/tt-mlir",
      "date": "2026-02-10",
      "authors": [
        "tenstorrent"
      ],
      "tags": [],
      "matchedGroup": "frameworks",
      "stars": 248,
      "language": "MLIR",
      "forks": 107,
      "fullName": "tenstorrent/tt-mlir"
    },
    {
      "id": "gh-vllm-project-compressed-tensors",
      "source": "github",
      "title": "compressed-tensors",
      "description": "A safetensors extension to efficiently store sparse quantized tensors on disk",
      "url": "https://github.com/vllm-project/compressed-tensors",
      "date": "2026-02-10",
      "authors": [
        "vllm-project"
      ],
      "tags": [],
      "matchedGroup": "model-compression",
      "stars": 243,
      "language": "Python",
      "forks": 56,
      "fullName": "vllm-project/compressed-tensors"
    },
    {
      "id": "gh-vllm-project-tpu-inference",
      "source": "github",
      "title": "tpu-inference",
      "description": "TPU inference for vLLM, with unified JAX and PyTorch support.",
      "url": "https://github.com/vllm-project/tpu-inference",
      "date": "2026-02-10",
      "authors": [
        "vllm-project"
      ],
      "tags": [],
      "matchedGroup": "accelerators",
      "stars": 231,
      "language": "Python",
      "forks": 96,
      "fullName": "vllm-project/tpu-inference"
    },
    {
      "id": "gh-PennyLaneAI-catalyst",
      "source": "github",
      "title": "catalyst",
      "description": "A JIT compiler for hybrid quantum programs in PennyLane",
      "url": "https://github.com/PennyLaneAI/catalyst",
      "date": "2026-02-10",
      "authors": [
        "PennyLaneAI"
      ],
      "tags": [
        "autodiff",
        "automatic-differentiation",
        "jax",
        "jit",
        "llvm",
        "mlir",
        "pennylane",
        "python",
        "qir",
        "quantum",
        "quantum-compiler",
        "quantum-computing"
      ],
      "matchedGroup": "frameworks",
      "stars": 197,
      "language": "Python",
      "forks": 63,
      "fullName": "PennyLaneAI/catalyst"
    },
    {
      "id": "gh-jortilles-EDA",
      "source": "github",
      "title": "EDA",
      "description": "Edalitcs",
      "url": "https://github.com/jortilles/EDA",
      "date": "2026-02-10",
      "authors": [
        "jortilles"
      ],
      "tags": [
        "analytics",
        "business-analytics",
        "business-intelligence",
        "data-visualization"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 178,
      "language": "TypeScript",
      "forks": 25,
      "fullName": "jortilles/EDA"
    },
    {
      "id": "gh-openxla-tokamax",
      "source": "github",
      "title": "tokamax",
      "description": "Tokamax: A GPU and TPU kernel library.",
      "url": "https://github.com/openxla/tokamax",
      "date": "2026-02-10",
      "authors": [
        "openxla"
      ],
      "tags": [],
      "matchedGroup": "accelerators",
      "stars": 171,
      "language": "Python",
      "forks": 12,
      "fullName": "openxla/tokamax"
    },
    {
      "id": "gh-openxla-shardy",
      "source": "github",
      "title": "shardy",
      "description": "MLIR-based partitioning system",
      "url": "https://github.com/openxla/shardy",
      "date": "2026-02-10",
      "authors": [
        "openxla"
      ],
      "tags": [],
      "matchedGroup": "frameworks",
      "stars": 164,
      "language": "MLIR",
      "forks": 32,
      "fullName": "openxla/shardy"
    },
    {
      "id": "gh-arc-research-lab-CHARM",
      "source": "github",
      "title": "CHARM",
      "description": "CHARM: Composing Heterogeneous Accelerators on Heterogeneous SoC Architecture",
      "url": "https://github.com/arc-research-lab/CHARM",
      "date": "2026-02-10",
      "authors": [
        "arc-research-lab"
      ],
      "tags": [
        "acap",
        "deeplearning",
        "design-space-exploration",
        "domain-specific-architecture",
        "electronic-design-automation",
        "fpga",
        "heterogeneous-computing",
        "high-level-synthesis",
        "versal",
        "versalacap"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 163,
      "language": "C++",
      "forks": 23,
      "fullName": "arc-research-lab/CHARM"
    },
    {
      "id": "gh-facebookexperimental-triton",
      "source": "github",
      "title": "triton",
      "description": "Github mirror of trition-lang/triton repo.",
      "url": "https://github.com/facebookexperimental/triton",
      "date": "2026-02-10",
      "authors": [
        "facebookexperimental"
      ],
      "tags": [],
      "matchedGroup": "frameworks",
      "stars": 128,
      "language": "MLIR",
      "forks": 37,
      "fullName": "facebookexperimental/triton"
    },
    {
      "id": "gh-open-edge-platform-edge-ai-libraries",
      "source": "github",
      "title": "edge-ai-libraries",
      "description": "Performance optimized libraries, microservices, and tools to support the development of Edge AI applications.",
      "url": "https://github.com/open-edge-platform/edge-ai-libraries",
      "date": "2026-02-10",
      "authors": [
        "open-edge-platform"
      ],
      "tags": [
        "libraries",
        "microservices",
        "samples",
        "tools"
      ],
      "matchedGroup": "edge-ai",
      "stars": 118,
      "language": "Python",
      "forks": 87,
      "fullName": "open-edge-platform/edge-ai-libraries"
    },
    {
      "id": "gh-mcychan-nQuantCpp",
      "source": "github",
      "title": "nQuantCpp",
      "description": "nQuantCpp includes top 6 color quantization algorithms for visual c++ producing high quality optimized images.",
      "url": "https://github.com/mcychan/nQuantCpp",
      "date": "2026-02-10",
      "authors": [
        "mcychan"
      ],
      "tags": [
        "blue-noise",
        "cielab",
        "color-quantization",
        "dithering",
        "dl3-quantization",
        "error-diffusion",
        "hilbert-curve",
        "image-processing",
        "median-cut",
        "otsu-threshold",
        "otsu-thresholding",
        "quality",
        "quantization-algorithms",
        "transparency",
        "unsupervised-clustering",
        "visual-cpp"
      ],
      "matchedGroup": "model-compression",
      "stars": 116,
      "language": "C++",
      "forks": 9,
      "fullName": "mcychan/nQuantCpp"
    },
    {
      "id": "gh-opensensor-lightNVR",
      "source": "github",
      "title": "lightNVR",
      "description": "lightweight Linux based NVR system",
      "url": "https://github.com/opensensor/lightNVR",
      "date": "2026-02-10",
      "authors": [
        "opensensor"
      ],
      "tags": [
        "hls",
        "ipcamera",
        "network-video-recorder",
        "nvr",
        "object-detection",
        "rtsp",
        "sod",
        "tflite",
        "webrtc"
      ],
      "matchedGroup": "optimization",
      "stars": 110,
      "language": "C",
      "forks": 25,
      "fullName": "opensensor/lightNVR"
    },
    {
      "id": "gh-metr0jw-Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA",
      "source": "github",
      "title": "Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA",
      "description": "Energy-efficient Event-driven Spiking Neural Network accelerator for FPGA with PyTorch integration",
      "url": "https://github.com/metr0jw/Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA",
      "date": "2026-02-10",
      "authors": [
        "metr0jw"
      ],
      "tags": [
        "computational-neuroscience",
        "embedded-systems",
        "hardware-accelerator",
        "leaky-integrate-and-fire-neuron",
        "lif-model",
        "lif-neuron",
        "neuromorphic-computing",
        "pynq",
        "pytorch",
        "snn",
        "spiking-neural-network",
        "spiking-neural-networks",
        "verilog",
        "verilog-hdl",
        "vitis",
        "vivado",
        "xilinx"
      ],
      "matchedGroup": "accelerators",
      "stars": 108,
      "language": "VHDL",
      "forks": 7,
      "fullName": "metr0jw/Event-Driven-Spiking-Neural-Network-Accelerator-for-FPGA"
    },
    {
      "id": "gh-omni-ai-npu-omni-infer",
      "source": "github",
      "title": "omni-infer",
      "description": "Omni_Infer is a suite of inference accelerators designed for the Ascend NPU platform, offering native support and an expanding feature set.",
      "url": "https://github.com/omni-ai-npu/omni-infer",
      "date": "2026-02-10",
      "authors": [
        "omni-ai-npu"
      ],
      "tags": [],
      "matchedGroup": "accelerators",
      "stars": 105,
      "language": "Python",
      "forks": 16,
      "fullName": "omni-ai-npu/omni-infer"
    },
    {
      "id": "gh-byte-capsule-FanCode-Hls-Fetcher",
      "source": "github",
      "title": "FanCode-Hls-Fetcher",
      "description": "Automatically updated list of FanCode Live Matches Links(provided as json-file ,PlayList)",
      "url": "https://github.com/byte-capsule/FanCode-Hls-Fetcher",
      "date": "2026-02-10",
      "authors": [
        "byte-capsule"
      ],
      "tags": [
        "cricket-live",
        "fancode",
        "fancode-live",
        "fancode-live-cricket",
        "fancode-mod-apk",
        "fancode-playlist",
        "fancode-script",
        "india-iptv",
        "ipl-live",
        "iptv",
        "iptv-channels",
        "iptv-m3u",
        "iptv-playlist",
        "iptv-script",
        "live-cricket",
        "live-sports",
        "live-tv",
        "m3u8",
        "tv",
        "tv-playlist"
      ],
      "matchedGroup": "optimization",
      "stars": 100,
      "language": "Python",
      "forks": 49,
      "fullName": "byte-capsule/FanCode-Hls-Fetcher"
    },
    {
      "id": "gh-lachlanchen-OpenHI",
      "source": "github",
      "title": "OpenHI",
      "description": "Self‚Äëcalibrated neuromorphic hyperspectral sensing pipeline for event cameras with diffractive  illumination. Includes end‚Äëto‚Äëend tools for RAW segmentation, multi‚Äëwindow time‚Äëwarping compensation, spectral visualization, and hardware control for synchronized event/frame capture and scanning.",
      "url": "https://github.com/lachlanchen/OpenHI",
      "date": "2026-02-10",
      "authors": [
        "lachlanchen"
      ],
      "tags": [
        "computer-vision",
        "event-camera",
        "hyperspectral-imaging",
        "microscopy",
        "neuromorphic-computing",
        "optics",
        "python",
        "pytorch"
      ],
      "matchedGroup": "ai-hardware",
      "stars": 100,
      "language": "Python",
      "forks": 32,
      "fullName": "lachlanchen/OpenHI"
    },
    {
      "id": "gh-ansible-eda-server",
      "source": "github",
      "title": "eda-server",
      "description": " Event Driven Ansible for AAP",
      "url": "https://github.com/ansible/eda-server",
      "date": "2026-02-10",
      "authors": [
        "ansible"
      ],
      "tags": [],
      "matchedGroup": "synthesis-pnr",
      "stars": 89,
      "language": "Python",
      "forks": 61,
      "fullName": "ansible/eda-server"
    },
    {
      "id": "gh-NVIDIA-OSMO",
      "source": "github",
      "title": "OSMO",
      "description": "The developer-first platform for scaling complex Physical AI workloads across heterogeneous compute‚Äîunifying training GPUs, simulation clusters, and edge devices in a simple YAML",
      "url": "https://github.com/NVIDIA/OSMO",
      "date": "2026-02-10",
      "authors": [
        "NVIDIA"
      ],
      "tags": [],
      "matchedGroup": "edge-ai",
      "stars": 89,
      "language": "Python",
      "forks": 13,
      "fullName": "NVIDIA/OSMO"
    },
    {
      "id": "gh-open-edge-platform-edge-ai-suites",
      "source": "github",
      "title": "edge-ai-suites",
      "description": "Curated collections of sample applications designed to help you develop optimized AI solutions. Tailored to specific use cases, covering retail, manufacturing, metro, and media & entertainment.",
      "url": "https://github.com/open-edge-platform/edge-ai-suites",
      "date": "2026-02-10",
      "authors": [
        "open-edge-platform"
      ],
      "tags": [
        "manufacturing-ai-suite",
        "media-and-entertainment-ai-suite",
        "metro-ai-suite",
        "retail-ai-suite",
        "robotics-ai-suite"
      ],
      "matchedGroup": "edge-ai",
      "stars": 84,
      "language": "Jupyter Notebook",
      "forks": 88,
      "fullName": "open-edge-platform/edge-ai-suites"
    },
    {
      "id": "gh-YoWASP-yosys",
      "source": "github",
      "title": "yosys",
      "description": "Unofficial Yosys WebAssembly packages",
      "url": "https://github.com/YoWASP/yosys",
      "date": "2026-02-10",
      "authors": [
        "YoWASP"
      ],
      "tags": [
        "fpga",
        "pypi",
        "python",
        "webassembly",
        "yosys",
        "yowasp"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 76,
      "language": "Python",
      "forks": 2,
      "fullName": "YoWASP/yosys"
    },
    {
      "id": "gh-gauravfs-14-awesome-tinyml",
      "source": "github",
      "title": "awesome-tinyml",
      "description": "A carefully curated collection of high-quality libraries, projects, tutorials, research papers, and other essential resources focused on TinyML ‚Äî the intersection of machine learning and ultra-low-power embedded systems.",
      "url": "https://github.com/gauravfs-14/awesome-tinyml",
      "date": "2026-02-10",
      "authors": [
        "gauravfs-14"
      ],
      "tags": [
        "machine-learning",
        "tiny-machine-learnig",
        "tinyml"
      ],
      "matchedGroup": "edge-ai",
      "stars": 64,
      "language": "JavaScript",
      "forks": 4,
      "fullName": "gauravfs-14/awesome-tinyml"
    },
    {
      "id": "gh-Dengyu-Wu-neuromorphics-daily-arxiv",
      "source": "github",
      "title": "neuromorphics-daily-arxiv",
      "description": "Neuromorphic paper list, automatically updating everyday at 8:00 am GMT.",
      "url": "https://github.com/Dengyu-Wu/neuromorphics-daily-arxiv",
      "date": "2026-02-10",
      "authors": [
        "Dengyu-Wu"
      ],
      "tags": [],
      "matchedGroup": "ai-hardware",
      "stars": 51,
      "language": "Python",
      "forks": 2,
      "fullName": "Dengyu-Wu/neuromorphics-daily-arxiv"
    },
    {
      "id": "gh-meta-pytorch-MSLK",
      "source": "github",
      "title": "MSLK",
      "description": "MSLK (Meta Superintelligence Labs Kernels) is a collection of PyTorch GPU operator libraries that are designed and optimized for GenAI training and inference, such as FP8 row-wise quantization and collective communications.",
      "url": "https://github.com/meta-pytorch/MSLK",
      "date": "2026-02-10",
      "authors": [
        "meta-pytorch"
      ],
      "tags": [],
      "matchedGroup": "model-compression",
      "stars": 50,
      "language": "Python",
      "forks": 22,
      "fullName": "meta-pytorch/MSLK"
    },
    {
      "id": "gh-microsoft-agentic-applications-for-unified-data-foundation-solution-accelerator",
      "source": "github",
      "title": "agentic-applications-for-unified-data-foundation-solution-accelerator",
      "description": "This accelerator unifies enterprise data with Microsoft Fabric and applies agentic AI to enable faster, smarter decisions. With Microsoft Foundry agents and the Microsoft Agent Framework orchestration, teams can automate workflows, query data in natural language, and uncover insights that fuel agility, collaboration, and innovation.",
      "url": "https://github.com/microsoft/agentic-applications-for-unified-data-foundation-solution-accelerator",
      "date": "2026-02-10",
      "authors": [
        "microsoft"
      ],
      "tags": [
        "ai-azd-templates",
        "azd-templates"
      ],
      "matchedGroup": "accelerators",
      "stars": 46,
      "language": "TSQL",
      "forks": 61,
      "fullName": "microsoft/agentic-applications-for-unified-data-foundation-solution-accelerator"
    },
    {
      "id": "gh-microsoft-edge-ai",
      "source": "github",
      "title": "edge-ai",
      "description": "Production-ready Infrastructure as Code, applications, pluggable components, and PlatformOps toolchains that empower organizations to achieve more with cloud and edge AI-powered solutions. Built by friendly geeks, for every team that needs edge solutions to achieve real production results.",
      "url": "https://github.com/microsoft/edge-ai",
      "date": "2026-02-10",
      "authors": [
        "microsoft"
      ],
      "tags": [],
      "matchedGroup": "edge-ai",
      "stars": 44,
      "language": "HCL",
      "forks": 21,
      "fullName": "microsoft/edge-ai"
    },
    {
      "id": "gh-alsk1992-CloddsBot",
      "source": "github",
      "title": "CloddsBot",
      "description": "AI trading agent that operates autonomously across 700+ markets - Polymarket, Kalshi, Binance, Hyperliquid, Solana DEXs, 5 EVM chains. Scans    for edge, executes instantly, manages risk while you sleep. Agent commerce protocol for machine-to-machine payments. Self-hosted. Built on Claude.https://x.com/cloddsbot",
      "url": "https://github.com/alsk1992/CloddsBot",
      "date": "2026-02-10",
      "authors": [
        "alsk1992"
      ],
      "tags": [
        "agi",
        "ai",
        "arbitrage",
        "claude",
        "crypto",
        "defi",
        "ethereum",
        "futures",
        "hft",
        "hyperliquid",
        "kalshi",
        "polymarket",
        "prediction-markets",
        "pumpswap",
        "solana",
        "telegram-bot",
        "trading",
        "trading-bot",
        "typescript",
        "x402"
      ],
      "matchedGroup": "edge-ai",
      "stars": 40,
      "language": "TypeScript",
      "forks": 6,
      "fullName": "alsk1992/CloddsBot"
    },
    {
      "id": "gh-open-neuromorphic-open-neuromorphic.github.io",
      "source": "github",
      "title": "open-neuromorphic.github.io",
      "description": "Open Neuromorphic Website",
      "url": "https://github.com/open-neuromorphic/open-neuromorphic.github.io",
      "date": "2026-02-10",
      "authors": [
        "open-neuromorphic"
      ],
      "tags": [
        "neuromorphic"
      ],
      "matchedGroup": "ai-hardware",
      "stars": 32,
      "language": "HTML",
      "forks": 29,
      "fullName": "open-neuromorphic/open-neuromorphic.github.io"
    },
    {
      "id": "gh-lwy2020-MicroMix",
      "source": "github",
      "title": "MicroMix",
      "description": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling Formats for Large Language Models",
      "url": "https://github.com/lwy2020/MicroMix",
      "date": "2026-02-10",
      "authors": [
        "lwy2020"
      ],
      "tags": [],
      "matchedGroup": "model-compression",
      "stars": 28,
      "language": "Cuda",
      "forks": 3,
      "fullName": "lwy2020/MicroMix"
    },
    {
      "id": "gh-bishaldahal-M3U8-HLS-Player-with-Custom-Controls",
      "source": "github",
      "title": "M3U8-HLS-Player-with-Custom-Controls",
      "description": "This extension features Youtube-like keyboard shortcuts for playing HLS or m3u8 content.",
      "url": "https://github.com/bishaldahal/M3U8-HLS-Player-with-Custom-Controls",
      "date": "2026-02-10",
      "authors": [
        "bishaldahal"
      ],
      "tags": [
        "chrome-extension",
        "edge-extension",
        "extension",
        "firefox-extension",
        "hls-player",
        "keyboard-shortcuts",
        "m3u8-player",
        "privacy",
        "productivity"
      ],
      "matchedGroup": "optimization",
      "stars": 23,
      "language": "JavaScript",
      "forks": 1,
      "fullName": "bishaldahal/M3U8-HLS-Player-with-Custom-Controls"
    },
    {
      "id": "gh-rupeshs-verity",
      "source": "github",
      "title": "verity",
      "description": "Perplexity style AI answer engine for AI PCs with CPU,GPU and NPU support ",
      "url": "https://github.com/rupeshs/verity",
      "date": "2026-02-10",
      "authors": [
        "rupeshs"
      ],
      "tags": [
        "llamacpp",
        "llm",
        "npu",
        "ollama",
        "openai",
        "openai-api",
        "openvino",
        "rag",
        "search"
      ],
      "matchedGroup": "accelerators",
      "stars": 22,
      "language": "Python",
      "forks": 2,
      "fullName": "rupeshs/verity"
    },
    {
      "id": "gh-IvanMurzak-Unity-AI-ProBuilder",
      "source": "github",
      "title": "Unity-AI-ProBuilder",
      "description": "AI-powered 3D modeling tools for Unity ProBuilder. Enables AI assistants to create and manipulate editable meshes through natural language commands. Create primitive shapes, extrude faces, bevel edges, apply materials, merge objects, and perform advanced mesh operations like bridging and subdivision.",
      "url": "https://github.com/IvanMurzak/Unity-AI-ProBuilder",
      "date": "2026-02-10",
      "authors": [
        "IvanMurzak"
      ],
      "tags": [
        "3d",
        "ai",
        "gamedev",
        "mcp",
        "model-context-protocol",
        "unity"
      ],
      "matchedGroup": "edge-ai",
      "stars": 22,
      "language": "C#",
      "forks": 4,
      "fullName": "IvanMurzak/Unity-AI-ProBuilder"
    },
    {
      "id": "gh-aitrios-aitrios-edge-device-core",
      "source": "github",
      "title": "aitrios-edge-device-core",
      "description": "Official module for connecting edge devices to AITRIOS console.",
      "url": "https://github.com/aitrios/aitrios-edge-device-core",
      "date": "2026-02-10",
      "authors": [
        "aitrios"
      ],
      "tags": [],
      "matchedGroup": "edge-ai",
      "stars": 19,
      "language": "C",
      "forks": 6,
      "fullName": "aitrios/aitrios-edge-device-core"
    },
    {
      "id": "gh-tscircuit-cli",
      "source": "github",
      "title": "cli",
      "description": "Create electronics with React with a local development server",
      "url": "https://github.com/tscircuit/cli",
      "date": "2026-02-10",
      "authors": [
        "tscircuit"
      ],
      "tags": [
        "eda",
        "electronics",
        "kicad",
        "pcb",
        "pcb-design",
        "react"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 16,
      "language": "TypeScript",
      "forks": 52,
      "fullName": "tscircuit/cli"
    },
    {
      "id": "gh-Samsung-TICO",
      "source": "github",
      "title": "TICO",
      "description": "A python library for converting Pytorch modules into a circle model that is a lightweight and efficient representation in ONE designed for optimized on-device neural network inference.",
      "url": "https://github.com/Samsung/TICO",
      "date": "2026-02-10",
      "authors": [
        "Samsung"
      ],
      "tags": [],
      "matchedGroup": "edge-ai",
      "stars": 16,
      "language": "Python",
      "forks": 24,
      "fullName": "Samsung/TICO"
    },
    {
      "id": "gh-Brionengine-Brion-Quantum-A.I.-General-System",
      "source": "github",
      "title": "Brion-Quantum-A.I.-General-System",
      "description": "Brion is the world‚Äôs first quantum AI model with the deepest integrations, combining the power of quantum computing and artificial intelligence. Built on cutting-edge quantum algorithms and secure protocols, it is designed for unmatched high-speed, parallel computation that solves complex problems possibly infinitely faster. ",
      "url": "https://github.com/Brionengine/Brion-Quantum-A.I.-General-System",
      "date": "2026-02-10",
      "authors": [
        "Brionengine"
      ],
      "tags": [],
      "matchedGroup": "edge-ai",
      "stars": 16,
      "language": "Python",
      "forks": 1,
      "fullName": "Brionengine/Brion-Quantum-A.I.-General-System"
    },
    {
      "id": "gh-arc-research-lab-SCARIF",
      "source": "github",
      "title": "SCARIF",
      "description": "SCARIF is a tool to estimate the embodied carbon emissions of data center servers with accelerator hardware (GPUs, FPGAs, etc.)",
      "url": "https://github.com/arc-research-lab/SCARIF",
      "date": "2026-02-10",
      "authors": [
        "arc-research-lab"
      ],
      "tags": [],
      "matchedGroup": "accelerators",
      "stars": 15,
      "language": "Python",
      "forks": 1,
      "fullName": "arc-research-lab/SCARIF"
    },
    {
      "id": "gh-mobilint-mblt-model-zoo",
      "source": "github",
      "title": "mblt-model-zoo",
      "description": "Mobilint Model Zoo Project",
      "url": "https://github.com/mobilint/mblt-model-zoo",
      "date": "2026-02-10",
      "authors": [
        "mobilint"
      ],
      "tags": [
        "ai-accelerators",
        "computer-vision",
        "edge-ai",
        "mobilint",
        "quantization",
        "quantized-neural-networks",
        "transformer"
      ],
      "matchedGroup": "model-compression",
      "stars": 15,
      "language": "Python",
      "forks": 0,
      "fullName": "mobilint/mblt-model-zoo"
    },
    {
      "id": "gh-kk43994-claw-desktop-pet",
      "source": "github",
      "title": "claw-desktop-pet",
      "description": "ü¶û ‰∏Ä‰∏™ÂèØÁà±ÁöÑÊ°åÈù¢ÈæôËôæAIÂä©Êâã - Desktop lobster pet with OpenClaw AI, Edge TTS voice, and emotion animations",
      "url": "https://github.com/kk43994/claw-desktop-pet",
      "date": "2026-02-10",
      "authors": [
        "kk43994"
      ],
      "tags": [],
      "matchedGroup": "edge-ai",
      "stars": 15,
      "language": "JavaScript",
      "forks": 0,
      "fullName": "kk43994/claw-desktop-pet"
    },
    {
      "id": "gh-MINT-SJTU-VLA-Pruner",
      "source": "github",
      "title": "VLA-Pruner",
      "description": "The official implementation of VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference.",
      "url": "https://github.com/MINT-SJTU/VLA-Pruner",
      "date": "2026-02-10",
      "authors": [
        "MINT-SJTU"
      ],
      "tags": [],
      "matchedGroup": "model-compression",
      "stars": 12,
      "language": "Python",
      "forks": 0,
      "fullName": "MINT-SJTU/VLA-Pruner"
    },
    {
      "id": "gh-quic-aimet-pages",
      "source": "github",
      "title": "aimet-pages",
      "description": "AIMET GitHub pages documentation",
      "url": "https://github.com/quic/aimet-pages",
      "date": "2026-02-10",
      "authors": [
        "quic"
      ],
      "tags": [
        "auto-ml",
        "compression",
        "deep-learning",
        "deep-neural-networks",
        "machine-learning",
        "network-compression",
        "network-quantization",
        "open-source",
        "opensource",
        "pruning",
        "quantization"
      ],
      "matchedGroup": "model-compression",
      "stars": 10,
      "language": "HTML",
      "forks": 4,
      "fullName": "quic/aimet-pages"
    },
    {
      "id": "gh-mcychan-nQuantGpp",
      "source": "github",
      "title": "nQuantGpp",
      "description": "nQuantGpp includes top 10 color quantization algorithms for g++ producing high quality optimized images.",
      "url": "https://github.com/mcychan/nQuantGpp",
      "date": "2026-02-10",
      "authors": [
        "mcychan"
      ],
      "tags": [
        "blue-noise",
        "cielab",
        "color-quantization",
        "dithering",
        "dl3-quantization",
        "error-diffusion",
        "hilbert-curve",
        "image-processing",
        "median-cut",
        "opencv",
        "otsu-threshold",
        "otsu-thresholding",
        "quantization-algorithms",
        "transparency",
        "unsupervised-clustering"
      ],
      "matchedGroup": "model-compression",
      "stars": 9,
      "language": "C++",
      "forks": 1,
      "fullName": "mcychan/nQuantGpp"
    },
    {
      "id": "gh-derekwisong-datui",
      "source": "github",
      "title": "datui",
      "description": "Data Exploration in the Terminal",
      "url": "https://github.com/derekwisong/datui",
      "date": "2026-02-10",
      "authors": [
        "derekwisong"
      ],
      "tags": [
        "analysis",
        "cli",
        "data",
        "eda",
        "research",
        "tui"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 8,
      "language": "Rust",
      "forks": 1,
      "fullName": "derekwisong/datui"
    },
    {
      "id": "gh-sliming-ai-sliming-ai.github.io",
      "source": "github",
      "title": "sliming-ai.github.io",
      "description": "üß† Singular values-driven automated filter pruning",
      "url": "https://github.com/sliming-ai/sliming-ai.github.io",
      "date": "2026-02-10",
      "authors": [
        "sliming-ai"
      ],
      "tags": [
        "filter-pruning",
        "hosvd",
        "network-compression",
        "network-pruning",
        "singular-value-decomposition",
        "singularity",
        "tinyml",
        "tucker-decomposition"
      ],
      "matchedGroup": "model-compression",
      "stars": 8,
      "language": "JavaScript",
      "forks": 4,
      "fullName": "sliming-ai/sliming-ai.github.io"
    },
    {
      "id": "gh-r-vage-ComfyUI_Eclipse",
      "source": "github",
      "title": "ComfyUI_Eclipse",
      "description": "Comprehensive ComfyUI custom node suite featuring Smart Loaders (multi-format checkpoint support with Nunchaku/GGUF quantization), Smart Prompt system with wildcards, sophisticated pipe ecosystem, universal type converters, image/video utilities, and workflow helpers.",
      "url": "https://github.com/r-vage/ComfyUI_Eclipse",
      "date": "2026-02-10",
      "authors": [
        "r-vage"
      ],
      "tags": [],
      "matchedGroup": "model-compression",
      "stars": 8,
      "language": "Python",
      "forks": 2,
      "fullName": "r-vage/ComfyUI_Eclipse"
    },
    {
      "id": "gh-playjp-tiny-rtmp",
      "source": "github",
      "title": "tiny-rtmp",
      "description": "Tiny RTMP Server written in Node.js",
      "url": "https://github.com/playjp/tiny-rtmp",
      "date": "2026-02-10",
      "authors": [
        "playjp"
      ],
      "tags": [
        "cenc",
        "fmp4",
        "hls",
        "ll-hls",
        "mpeg-dash",
        "mpeg-ts",
        "rtmp",
        "video-streaming"
      ],
      "matchedGroup": "optimization",
      "stars": 8,
      "language": "TypeScript",
      "forks": 0,
      "fullName": "playjp/tiny-rtmp"
    },
    {
      "id": "gh-Ascend-AscendNPU-IR",
      "source": "github",
      "title": "AscendNPU-IR",
      "description": "Mirror of https://gitcode.com/Ascend/AscendNPU-IR",
      "url": "https://github.com/Ascend/AscendNPU-IR",
      "date": "2026-02-10",
      "authors": [
        "Ascend"
      ],
      "tags": [],
      "matchedGroup": "accelerators",
      "stars": 7,
      "language": "C++",
      "forks": 2,
      "fullName": "Ascend/AscendNPU-IR"
    },
    {
      "id": "gh-TheJoshBrod-CGinS",
      "source": "github",
      "title": "CGinS",
      "description": "An LLM-driven GPU kernel generation and optimization framework designed to integrate with and improve existing PyTorch models",
      "url": "https://github.com/TheJoshBrod/CGinS",
      "date": "2026-02-10",
      "authors": [
        "TheJoshBrod"
      ],
      "tags": [],
      "matchedGroup": "optimization",
      "stars": 7,
      "language": "Cuda",
      "forks": 1,
      "fullName": "TheJoshBrod/CGinS"
    },
    {
      "id": "gh-edaaydinea-edaaydinea",
      "source": "github",
      "title": "edaaydinea",
      "description": "My personal respository",
      "url": "https://github.com/edaaydinea/edaaydinea",
      "date": "2026-02-10",
      "authors": [
        "edaaydinea"
      ],
      "tags": [],
      "matchedGroup": "synthesis-pnr",
      "stars": 6,
      "language": "",
      "forks": 0,
      "fullName": "edaaydinea/edaaydinea"
    },
    {
      "id": "gh-alainmarcel-uhdm2rtlil",
      "source": "github",
      "title": "uhdm2rtlil",
      "description": "UHDM 2 RTLIL Yosys Pass",
      "url": "https://github.com/alainmarcel/uhdm2rtlil",
      "date": "2026-02-10",
      "authors": [
        "alainmarcel"
      ],
      "tags": [],
      "matchedGroup": "synthesis-pnr",
      "stars": 4,
      "language": "Verilog",
      "forks": 0,
      "fullName": "alainmarcel/uhdm2rtlil"
    },
    {
      "id": "gh-matth2k-safety-net",
      "source": "github",
      "title": "safety-net",
      "description": "A reference-counted netlist library for EDA tool development",
      "url": "https://github.com/matth2k/safety-net",
      "date": "2026-02-10",
      "authors": [
        "matth2k"
      ],
      "tags": [
        "eda",
        "hdl",
        "netlist",
        "rtl",
        "rust",
        "verilog"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 4,
      "language": "Rust",
      "forks": 3,
      "fullName": "matth2k/safety-net"
    },
    {
      "id": "gh-ktbarrett-coconext",
      "source": "github",
      "title": "coconext",
      "description": "Staging area for new features of cocotb",
      "url": "https://github.com/ktbarrett/coconext",
      "date": "2026-02-10",
      "authors": [
        "ktbarrett"
      ],
      "tags": [
        "cocotb",
        "eda",
        "hdl",
        "python",
        "verification"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 4,
      "language": "Python",
      "forks": 1,
      "fullName": "ktbarrett/coconext"
    },
    {
      "id": "gh-davidsiaw-yosys-docker",
      "source": "github",
      "title": "yosys-docker",
      "description": "Automated dockerized oss-cad-suite",
      "url": "https://github.com/davidsiaw/yosys-docker",
      "date": "2026-02-10",
      "authors": [
        "davidsiaw"
      ],
      "tags": [],
      "matchedGroup": "synthesis-pnr",
      "stars": 3,
      "language": "Shell",
      "forks": 2,
      "fullName": "davidsiaw/yosys-docker"
    },
    {
      "id": "gh-ztreamhd-xtrakt",
      "source": "github",
      "title": "xtrakt",
      "description": "Private HLS grabber for YouTube. Original project and idea by @benmoose39.",
      "url": "https://github.com/ztreamhd/xtrakt",
      "date": "2026-02-10",
      "authors": [
        "ztreamhd"
      ],
      "tags": [],
      "matchedGroup": "optimization",
      "stars": 3,
      "language": "Python",
      "forks": 1,
      "fullName": "ztreamhd/xtrakt"
    },
    {
      "id": "gh-estebamod-osmmcp",
      "source": "github",
      "title": "osmmcp",
      "description": "OpenStreetMap MCP server providing precision geospatial tools for LLMs via Model Context Protocol. Features geocoding, routing, nearby places, neighborhood analysis, EV charging stations, and more.",
      "url": "https://github.com/estebamod/osmmcp",
      "date": "2026-02-10",
      "authors": [
        "estebamod"
      ],
      "tags": [
        "anthropic-claude",
        "ev-charging",
        "geocoding",
        "geospatial",
        "llm-tools",
        "mcp",
        "mcp-server",
        "model-context-protocol",
        "neighborhood-analysis",
        "openstreetmap",
        "routing"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 2,
      "language": "Go",
      "forks": 2,
      "fullName": "estebamod/osmmcp"
    },
    {
      "id": "gh-eda-labs-topo-builder",
      "source": "github",
      "title": "topo-builder",
      "description": "Graphically create NetworkTopology workflows for Nokia EDA.",
      "url": "https://github.com/eda-labs/topo-builder",
      "date": "2026-02-10",
      "authors": [
        "eda-labs"
      ],
      "tags": [
        "eda",
        "nokia-eda",
        "topology"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 2,
      "language": "TypeScript",
      "forks": 2,
      "fullName": "eda-labs/topo-builder"
    },
    {
      "id": "gh-KePinGimank-software-architecture_formation-course-luisdev-part-1_dotnet-8_csharp-12",
      "source": "github",
      "title": "software-architecture_formation-course-luisdev-part-1_dotnet-8_csharp-12",
      "description": "üöÄ Master software architecture with this complete course on .NET 8 and C# 12, covering fundamentals, SOLID principles, design patterns, and more.",
      "url": "https://github.com/KePinGimank/software-architecture_formation-course-luisdev-part-1_dotnet-8_csharp-12",
      "date": "2026-02-10",
      "authors": [
        "KePinGimank"
      ],
      "tags": [
        "architecture",
        "backend",
        "best-practices",
        "clean-architecture",
        "code-quality",
        "csharp",
        "design-patterns",
        "dotnet",
        "eda",
        "event-driven",
        "gof",
        "luisdev",
        "oop",
        "refactoring",
        "serverless",
        "software-architecture",
        "software-engineering",
        "solid"
      ],
      "matchedGroup": "synthesis-pnr",
      "stars": 2,
      "language": "C#",
      "forks": 0,
      "fullName": "KePinGimank/software-architecture_formation-course-luisdev-part-1_dotnet-8_csharp-12"
    },
    {
      "id": "gh-3052-maya",
      "source": "github",
      "title": "maya",
      "description": "concurrent DASH/HLS stream downloader",
      "url": "https://github.com/3052/maya",
      "date": "2026-02-10",
      "authors": [
        "3052"
      ],
      "tags": [],
      "matchedGroup": "optimization",
      "stars": 2,
      "language": "Go",
      "forks": 2,
      "fullName": "3052/maya"
    },
    {
      "id": "gh-Cavilach-lidar_odometry",
      "source": "github",
      "title": "lidar_odometry",
      "description": "üöÄ Achieve real-time LiDAR odometry with robust state estimation using Probabilistic Kernel Optimization for SLAM applications.",
      "url": "https://github.com/Cavilach/lidar_odometry",
      "date": "2026-02-10",
      "authors": [
        "Cavilach"
      ],
      "tags": [
        "colored-point-cloud",
        "ct-icp",
        "gaussian-splatting",
        "gtsam",
        "iros",
        "lidar-inertial",
        "lidar-inertial-odometry",
        "lidar-slam",
        "livox-avia-lidar",
        "loam",
        "loam-velodyne",
        "nerf",
        "ouster",
        "robotics",
        "sensor-fusion",
        "slam",
        "ugv",
        "visual-odometry"
      ],
      "matchedGroup": "optimization",
      "stars": 2,
      "language": "",
      "forks": 0,
      "fullName": "Cavilach/lidar_odometry"
    },
    {
      "id": "gh-Mazon-eda",
      "source": "github",
      "title": "eda",
      "description": "Tabletop RPG ",
      "url": "https://github.com/Mazon/eda",
      "date": "2026-02-10",
      "authors": [
        "Mazon"
      ],
      "tags": [],
      "matchedGroup": "synthesis-pnr",
      "stars": 1,
      "language": "HTML",
      "forks": 0,
      "fullName": "Mazon/eda"
    },
    {
      "id": "gh-ssrini22-neuromorphic_hardware",
      "source": "github",
      "title": "neuromorphic_hardware",
      "description": "Master's Thesis Work: SNN architecture, neuron array, DPE more",
      "url": "https://github.com/ssrini22/neuromorphic_hardware",
      "date": "2026-02-10",
      "authors": [
        "ssrini22"
      ],
      "tags": [],
      "matchedGroup": "ai-hardware",
      "stars": 1,
      "language": "SystemVerilog",
      "forks": 0,
      "fullName": "ssrini22/neuromorphic_hardware"
    },
    {
      "id": "gh-RohitLalu-Time-Domain-Compute-In-Memory-Accelerator-using-Memristor-Arrays",
      "source": "github",
      "title": "Time-Domain-Compute-In-Memory-Accelerator-using-Memristor-Arrays",
      "description": "Time Domain Compute In Memory Accelerator using Memristor Arrays",
      "url": "https://github.com/RohitLalu/Time-Domain-Compute-In-Memory-Accelerator-using-Memristor-Arrays",
      "date": "2026-02-10",
      "authors": [
        "RohitLalu"
      ],
      "tags": [],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "",
      "forks": 0,
      "fullName": "RohitLalu/Time-Domain-Compute-In-Memory-Accelerator-using-Memristor-Arrays"
    },
    {
      "id": "gh-deleekiir-number-float64-base-to-float16",
      "source": "github",
      "title": "number-float64-base-to-float16",
      "description": "üîÑ Convert Float64 base values to Float16 efficiently for optimized memory usage in numerical computing applications.",
      "url": "https://github.com/deleekiir/number-float64-base-to-float16",
      "date": "2026-02-10",
      "authors": [
        "deleekiir"
      ],
      "tags": [
        "base",
        "cast",
        "convert",
        "dbl",
        "double",
        "float",
        "float16",
        "float64",
        "javascript",
        "nodejs",
        "stdlib",
        "to",
        "type",
        "types",
        "util",
        "utilities",
        "utility",
        "utils"
      ],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "JavaScript",
      "forks": 0,
      "fullName": "deleekiir/number-float64-base-to-float16"
    },
    {
      "id": "gh-Teddiesarcosomal392-eye",
      "source": "github",
      "title": "eye",
      "description": "üëÅÔ∏è Preserve memories and engage in intelligent conversations with customizable AI models, designed for any computing capability.",
      "url": "https://github.com/Teddiesarcosomal392/eye",
      "date": "2026-02-10",
      "authors": [
        "Teddiesarcosomal392"
      ],
      "tags": [
        "android-jetpack",
        "appstartup",
        "computer-vision",
        "coroutines",
        "daemonize",
        "developer-kits",
        "dubbox",
        "eye-tracking",
        "hardware",
        "livedata-viewmodel",
        "mvvm",
        "mynteye",
        "pupil",
        "python",
        "red-team",
        "rpc-trace",
        "skyeye",
        "vipyinzhiwei"
      ],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "TypeScript",
      "forks": 0,
      "fullName": "Teddiesarcosomal392/eye"
    },
    {
      "id": "gh-1maite-hazelcast-xh9",
      "source": "github",
      "title": "hazelcast-xh9",
      "description": "üöÄ Accelerate data processing with Hazelcast XH9, a powerful framework for seamless in-memory data management and distributed computing solutions.",
      "url": "https://github.com/1maite/hazelcast-xh9",
      "date": "2026-02-10",
      "authors": [
        "1maite"
      ],
      "tags": [
        "caching",
        "cloud-native",
        "clustering",
        "data-structures",
        "debugging",
        "distributed-systems",
        "event-driven",
        "fault-tolerance",
        "hazelcast",
        "in-memory-data-grid",
        "integrations",
        "java",
        "microservices",
        "performance",
        "persistence",
        "scalability"
      ],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "",
      "forks": 0,
      "fullName": "1maite/hazelcast-xh9"
    },
    {
      "id": "gh-ghasemarianpour-neuromorphic",
      "source": "github",
      "title": "neuromorphic",
      "description": "This repository was created to house the files of the Neuromorphic School.",
      "url": "https://github.com/ghasemarianpour/neuromorphic",
      "date": "2026-02-10",
      "authors": [
        "ghasemarianpour"
      ],
      "tags": [],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "",
      "forks": 0,
      "fullName": "ghasemarianpour/neuromorphic"
    },
    {
      "id": "gh-alxdofficial-neuromorphic",
      "source": "github",
      "title": "neuromorphic",
      "description": "",
      "url": "https://github.com/alxdofficial/neuromorphic",
      "date": "2026-02-10",
      "authors": [
        "alxdofficial"
      ],
      "tags": [],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "Python",
      "forks": 0,
      "fullName": "alxdofficial/neuromorphic"
    },
    {
      "id": "gh-itservice5823-neuromorphic-bird-classifier-desktop-app-dvs-stream-cli-and-gui",
      "source": "github",
      "title": "neuromorphic-bird-classifier-desktop-app-dvs-stream-cli-and-gui",
      "description": "üê¶ Classify birds using neuromorphic computing with this desktop app, featuring both command line and graphical interfaces for streamlined data analysis.",
      "url": "https://github.com/itservice5823/neuromorphic-bird-classifier-desktop-app-dvs-stream-cli-and-gui",
      "date": "2026-02-10",
      "authors": [
        "itservice5823"
      ],
      "tags": [
        "birds",
        "customtkinter",
        "customtkinter-project",
        "dat-files",
        "event-data",
        "event-driven",
        "gui-application",
        "huggingface-models",
        "machine-learning",
        "neuromorphic-computing",
        "neuromorphic-engineering",
        "norse",
        "ornithology",
        "python",
        "spiking-neural-network"
      ],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "Python",
      "forks": 0,
      "fullName": "itservice5823/neuromorphic-bird-classifier-desktop-app-dvs-stream-cli-and-gui"
    },
    {
      "id": "gh-matsushibadenki-DORA",
      "source": "github",
      "title": "DORA",
      "description": "A comprehensive Python framework for Next-Generation Neuromorphic Computing and Artificial Brain Architecture. It bridges biological brains and AI using Spiking Neural Networks (SNNs), Spiking Transformers, and biologically plausible learning rules to simulate consciousness, sleep, and cognitive functions.",
      "url": "https://github.com/matsushibadenki/DORA",
      "date": "2026-02-10",
      "authors": [
        "matsushibadenki"
      ],
      "tags": [],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "Python",
      "forks": 1,
      "fullName": "matsushibadenki/DORA"
    },
    {
      "id": "gh-NirmalLimbachiya-ignite",
      "source": "github",
      "title": "ignite",
      "description": "üîí Execute AI-generated code and untrusted scripts safely in a secure sandbox environment with Ignite. Perfect for JS/TS microservices.",
      "url": "https://github.com/NirmalLimbachiya/ignite",
      "date": "2026-02-10",
      "authors": [
        "NirmalLimbachiya"
      ],
      "tags": [
        "big-data",
        "boilerplate",
        "cache",
        "cloud-native",
        "containers",
        "distributed-sql-database",
        "docker",
        "expo",
        "hacktoberfest",
        "ignite",
        "in-memory-computing",
        "in-memory-database",
        "iot",
        "kvm",
        "machine-learning",
        "php",
        "php-framework",
        "tastyigniter"
      ],
      "matchedGroup": "ai-hardware",
      "stars": 0,
      "language": "PHP",
      "forks": 0,
      "fullName": "NirmalLimbachiya/ignite"
    }
  ],
  "arxiv": [
    {
      "id": "arxiv-2602.09009",
      "source": "arxiv",
      "title": "ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling",
      "description": "Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead ($<1\\%$), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.",
      "url": "https://arxiv.org/abs/2602.09009",
      "date": "2026-02-09",
      "authors": [
        "Yilang Zhang",
        "Bingcong Li",
        "Niao He",
        "Georgios B. Giannakis"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.09009",
      "pdfUrl": "https://arxiv.org/pdf/2602.09009v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08984",
      "source": "arxiv",
      "title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models",
      "description": "We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.",
      "url": "https://arxiv.org/abs/2602.08984",
      "date": "2026-02-09",
      "authors": [
        "Yuliang Liu",
        "Yunchong Song",
        "Yixuan Wang",
        "Kewen Ge",
        "Alex Lamb",
        "Qipeng Guo",
        "Kai Chen",
        "Bowen Zhou",
        "Zhouhan Lin"
      ],
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08984",
      "pdfUrl": "https://arxiv.org/pdf/2602.08984v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.08923",
      "source": "arxiv",
      "title": "DynamiQ: Accelerating Gradient Synchronization using Compressed Multi-hop All-reduce",
      "description": "Multi-hop all-reduce is the de facto backbone of large model training. As the training scale increases, the network often becomes a bottleneck, motivating reducing the volume of transmitted data. Accordingly, recent systems demonstrated significant acceleration of the training process using gradient quantization. However, these systems are not optimized for multi-hop aggregation, where entries are partially summed multiple times along their aggregation topology. This paper presents DynamiQ, a quantization framework that bridges the gap between quantization best practices and multi-hop aggregation. DynamiQ introduces novel techniques to better represent partial sums, co-designed with a decompress-accumulate-recompress fused kernel to facilitate fast execution. We extended PyTorch DDP to support DynamiQ over NCCL P2P, and across different LLMs, tasks, and scales, we demonstrate consistent improvement of up to 34.2% over the best among state-of-the-art methods such as Omni-Reduce, THC, and emerging standards such as MXFP4, MXFP6, and MXFP8. Further, DynamiQ is the only evaluated method that consistently reaches near-baseline accuracy (e.g., 99.9% of the BF16 baseline) and does so while significantly accelerating the training.",
      "url": "https://arxiv.org/abs/2602.08923",
      "date": "2026-02-09",
      "authors": [
        "Wenchen Han",
        "Shay Vargaftik",
        "Michael Mitzenmacher",
        "Ran Ben Basat"
      ],
      "tags": [
        "cs.LG",
        "cs.DC",
        "cs.NI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08923",
      "pdfUrl": "https://arxiv.org/pdf/2602.08923v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08905",
      "source": "arxiv",
      "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models",
      "description": "Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \\textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \\textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.",
      "url": "https://arxiv.org/abs/2602.08905",
      "date": "2026-02-09",
      "authors": [
        "Jiawei Liu",
        "Xiting Wang",
        "Yuanyuan Zhong",
        "Defu Lian",
        "Yu Yang"
      ],
      "tags": [
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08905",
      "pdfUrl": "https://arxiv.org/pdf/2602.08905v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.08858",
      "source": "arxiv",
      "title": "FlattenGPT: Depth Compression for Transformer with Layer Flattening",
      "description": "Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \\textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\\% of zero-shot performance with a compression ratio of 20\\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.",
      "url": "https://arxiv.org/abs/2602.08858",
      "date": "2026-02-09",
      "authors": [
        "Ruihan Xu",
        "Qingpei Guo",
        "Yao Zhu",
        "Xiangyang Ji",
        "Ming Yang",
        "Shiliang Zhang"
      ],
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08858",
      "pdfUrl": "https://arxiv.org/pdf/2602.08858v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.08817",
      "source": "arxiv",
      "title": "Kirin: Improving ANN efficiency with SNN Hybridization",
      "description": "Artificial neural networks (ANNs), particularly large language models (LLMs), demonstrate powerful inference capabilities but consume substantial energy. Conversely, spiking neural networks (SNNs) exhibit exceptional energy efficiency due to their binary and event-driven characteristics, thus motivating the study of ANN-to-SNN conversion. In this process, quantization plays a pivotal role, mapping LLMs' floating-point parameters to discrete SNN parameters via the temporal dimension of the time window. However, several challenges remain in the conversion process: (i) converting high bit-width quantization values into binary spikes requires longer time windows, increasing system latency; and (ii) the inherent trade-off between the information loss of single-spike schemes and the energy costs of multi-spike ones in SNN. To address these challenges, we propose Kirin, a integer and spike hybrid based SNN to achieve accuracy lossless ANN-to-SNN conversion with time and energy efficiency. Specifically, we first propose a Spike Matrix Hybridization strategy that encoding low bit-width parameters that leading to small time window size into binary spikes while preserving the rest in integer format, thereby reducing the overall latency of SNN execution. Second, we introduce a silence threshold mechanism to regulate the timing of single-spike firing, ensuring the output is mathematically equivalent to the LLM's output and preserves accuracy. Experimental results demonstrate that Kirin, under a W4A4\\&8 quantization setting, achieves near-FP16 accuracy while reducing energy consumption by up to 84.66\\% and shortening time steps by 93.75\\%.",
      "url": "https://arxiv.org/abs/2602.08817",
      "date": "2026-02-09",
      "authors": [
        "Chenyu Wang",
        "Zhanglu Yan",
        "Zhi Zhou",
        "Xu Chen",
        "Weng-Fai Wong"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.08817",
      "pdfUrl": "https://arxiv.org/pdf/2602.08817v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08810",
      "source": "arxiv",
      "title": "$\\texttt{lrnnx}$: A library for Linear RNNs",
      "description": "Linear recurrent neural networks (LRNNs) provide a structured approach to sequence modeling that bridges classical linear dynamical systems and modern deep learning, offering both expressive power and theoretical guarantees on stability and trainability. In recent years, multiple LRNN-based architectures have been proposed, each introducing distinct parameterizations, discretization schemes, and implementation constraints. However, existing implementations are fragmented across different software frameworks, often rely on framework-specific optimizations, and in some cases require custom CUDA kernels or lack publicly available code altogether. As a result, using, comparing, or extending LRNNs requires substantial implementation effort. To address this, we introduce $\\texttt{lrnnx}$, a unified software library that implements several modern LRNN architectures under a common interface. The library exposes multiple levels of control, allowing users to work directly with core components or higher-level model abstractions. $\\texttt{lrnnx}$ aims to improve accessibility, reproducibility, and extensibility of LRNN research and applications. We make our code available under a permissive MIT license.",
      "url": "https://arxiv.org/abs/2602.08810",
      "date": "2026-02-09",
      "authors": [
        "Karan Bania",
        "Soham Kalburgi",
        "Manit Tanwar",
        "Dhruthi",
        "Aditya Nagarsekar",
        "Harshvardhan Mestha",
        "Naman Chibber",
        "Raj Deshmukh",
        "Anish Sathyanarayanan",
        "Aarush Rathore",
        "Pratham Chheda"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "optimization",
      "arxivId": "2602.08810",
      "pdfUrl": "https://arxiv.org/pdf/2602.08810v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08809",
      "source": "arxiv",
      "title": "Efficient Deep Learning for Biometrics: Overview, Challenges and Trends in Ear of Frugal AI",
      "description": "Recent advances in deep learning, whether on discriminative or generative tasks have been beneficial for various applications, among which security and defense. However, their increasing computational demands during training and deployment translates directly into high energy consumption. As a consequence, this induces a heavy carbon footprint which hinders their widespread use and scalability, but also a limitation when deployed on resource-constrained edge devices for real-time use. In this paper, we briefly survey efficient deep learning methods for biometric applications. Specifically, we tackle the challenges one might incur when training and deploying deep learning approaches, and provide a taxonomy of the various efficient deep learning families. Additionally, we discuss complementary metrics for evaluating the efficiency of these models such as memory, computation, latency, throughput, and advocate for universal and reproducible metrics for better comparison. Last, we give future research directions to consider.",
      "url": "https://arxiv.org/abs/2602.08809",
      "date": "2026-02-09",
      "authors": [
        "Karim Haroun",
        "Aya Zitouni",
        "Aicha Zenakhri",
        "Meriem Amel Guessoum",
        "Larbi Boubchir"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.08809",
      "pdfUrl": "https://arxiv.org/pdf/2602.08809v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08596",
      "source": "arxiv",
      "title": "RFSoC-Based Integrated Navigation and Sensing Using NavIC",
      "description": "Prior art has proposed a secondary application for Global Navigation Satellite System (GNSS) infrastructure for remote sensing of ground-based and maritime targets. Here, a passive radar receiver is deployed to detect uncooperative targets on Earth's surface by capturing ground-reflected satellite signals. This work demonstrates a hardware prototype of an L-band Navigation with Indian Constellation (NavIC) satellite-based remote sensing receiver system mounted on an AMD Zynq radio frequency system-on-chip (RFSoC) platform. Two synchronized receiver channels are introduced for capturing the direct signal (DS) from the satellite and ground-reflected signal (GRS) returns from targets. These signals are processed on the ARM processor and field programmable gate array (FPGA) of the RFSoC to generate delay-Doppler maps of the ground-based targets. The performance is first validated in a loop-back configuration of the RFSoC. Next, the DS and GRS signals are emulated by the output from two ports of the Keysight Arbitrary Waveform Generator (AWG) and interfaced with the RFSoC where the signals are subsequently processed to obtain the delay-Doppler maps. The performance is validated for different signal-to-noise ratios (SNR).",
      "url": "https://arxiv.org/abs/2602.08596",
      "date": "2026-02-09",
      "authors": [
        "Riya Sachdeva",
        "Aakanksha Tewari",
        "Sumit J. Darak",
        "Shobha Sundar Ram",
        "Sanat K. Biswas"
      ],
      "tags": [
        "eess.SP"
      ],
      "matchedGroup": "hardware-design",
      "arxivId": "2602.08596",
      "pdfUrl": "https://arxiv.org/pdf/2602.08596v1",
      "primaryCategory": "eess.SP"
    },
    {
      "id": "arxiv-2602.08564",
      "source": "arxiv",
      "title": "M-Loss: Quantifying Model Merging Compatibility with Limited Unlabeled Data",
      "description": "Training of large-scale models is both computationally intensive and often constrained by the availability of labeled data. Model merging offers a compelling alternative by directly integrating the weights of multiple source models without requiring additional data or extensive training. However, conventional model merging techniques, such as parameter averaging, often suffer from the unintended combination of non-generalizable features, especially when source models exhibit significant weight disparities. Comparatively, model ensembling generally provides more stable and superior performance that aggregates multiple models by averaging outputs. However, it incurs higher inference costs and increased storage requirements. While previous studies experimentally showed the similarities between model merging and ensembling, theoretical evidence and evaluation metrics remain lacking. To address this gap, we introduce Merging-ensembling loss (M-Loss), a novel evaluation metric that quantifies the compatibility of merging source models using very limited unlabeled data. By measuring the discrepancy between parameter averaging and model ensembling at layer and node levels, M-Loss facilitates more effective merging strategies. Specifically, M-Loss serves both as a quantitative criterion of the theoretical feasibility of model merging, and a guide for parameter significance in model pruning. Our theoretical analysis and empirical evaluations demonstrate that incorporating M-Loss into the merging process significantly improves the alignment between merged models and model ensembling, providing a scalable and efficient framework for accurate model consolidation.",
      "url": "https://arxiv.org/abs/2602.08564",
      "date": "2026-02-09",
      "authors": [
        "Tiantong Wang",
        "Yiyang Duan",
        "Haoyu Chen",
        "Tiantong Wu",
        "Wei Yang Bryan Lim"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.08564",
      "pdfUrl": "https://arxiv.org/pdf/2602.08564v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08513",
      "source": "arxiv",
      "title": "A Multi-objective Evolutionary Algorithm Based on Bi-population with Uniform Sampling for Neural Architecture Search",
      "description": "Neural architecture search (NAS) automates neural network design, improving efficiency over manual approaches. However, efficiently discovering high-performance neural network architectures that simultaneously optimize multiple objectives remains a significant challenge in NAS. Existing methods often suffer from limited population diversity and inadequate exploration of the search space, particularly in regions with extreme complexity values. To address these challenges, we propose MOEA-BUS, an innovative multi-objective evolutionary algorithm based on bi-population with uniform sampling for neural architecture search, aimed at simultaneously optimizing both accuracy and network complexity. In MOEA-BUS, a novel uniform sampling method is proposed to initialize the population, ensuring that architectures are distributed uniformly across the objective space. Furthermore, to enhance exploration, we deploy a bi-population framework where two populations evolve synergistically, facilitating comprehensive search space coverage. Experiments on CIFAR-10 and ImageNet demonstrate MOEA-BUS's superiority, achieving top-1 accuracies of 98.39% on CIFAR-10, and 80.03% on ImageNet. Notably, it achieves 78.28% accuracy on ImageNet with only 446M MAdds. Ablation studies confirm that both uniform sampling and bi-population mechanisms enhance population diversity and performance. Additionally, in terms of the Kendall's tau coefficient, the SVM achieves an improvement of at least 0.035 compared to the other three commonly used machine learning models, and uniform sampling provided an enhancement of approximately 0.07.",
      "url": "https://arxiv.org/abs/2602.08513",
      "date": "2026-02-09",
      "authors": [
        "Yu Xue",
        "Pengcheng Jiang",
        "Chenchen Zhu",
        "Yong Zhang",
        "Ran Cheng",
        "Kaizhou Gao",
        "Dunwei Gong"
      ],
      "tags": [
        "cs.NE"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08513",
      "pdfUrl": "https://arxiv.org/pdf/2602.08513v1",
      "primaryCategory": "cs.NE"
    },
    {
      "id": "arxiv-2602.08488",
      "source": "arxiv",
      "title": "A Comparative Analysis of the CERN ATLAS ITk MOPS Readout: A Feasibility Study on Production and Development Setups",
      "description": "The upcoming High-Luminosity upgrade of the Large Hadron Collider (LHC) necessitates a complete replacement of the ATLAS Inner Detector with the new Inner Tracker (ITk). This upgrade imposes stringent requirements on the associated Detector Control System (DCS), which is responsible for the monitoring, control, and safety of the detector. A critical component of the ITk DCS is the Monitoring of Pixel System (MOPS), which supervises the local voltages and temperatures of the new pixel detector modules. This paper introduces a dedicated testbed and verification methodology for the MOPS readout, defining a structured set of test cases for two DCS-readout architectures: a preliminary Raspberry Pi-based controller, the \"MOPS-Hub Mock-up\"(MH Mock-up), and the final production FPGA-based \"MOPS-Hub\" (MH). The methodology specifies the measurement chain for end-to-end latency, jitter, and data integrity across CAN and UART interfaces, including a unified time-stamping scheme, non-intrusive signal taps, and a consistent data-logging and analysis pipeline. This work details the load profiles and scalability scenarios (baseline operation, full-crate stress, and CAN Interface Card channel isolation), together with acceptance criteria and considerations for measurement uncertainty to ensure reproducibility. The objective is to provide a clear, repeatable procedure to qualify the MH architecture for production and deployment in the ATLAS ITk DCS. A companion paper will present the experimental results and the comparative analysis obtained using this testbed.",
      "url": "https://arxiv.org/abs/2602.08488",
      "date": "2026-02-09",
      "authors": [
        "Lukas Flad",
        "Felix Sebastian Nitz",
        "Tobias Krawutschke"
      ],
      "tags": [
        "physics.ins-det",
        "eess.SY"
      ],
      "matchedGroup": "hardware-design",
      "arxivId": "2602.08488",
      "pdfUrl": "https://arxiv.org/pdf/2602.08488v1",
      "primaryCategory": "physics.ins-det"
    },
    {
      "id": "arxiv-2602.08446",
      "source": "arxiv",
      "title": "RIFLE: Robust Distillation-based FL for Deep Model Deployment on Resource-Constrained IoT Networks",
      "description": "Federated learning (FL) is a decentralized learning paradigm widely adopted in resource-constrained Internet of Things (IoT) environments. These devices, typically relying on TinyML models, collaboratively train global models by sharing gradients with a central server while preserving data privacy. However, as data heterogeneity and task complexity increase, TinyML models often become insufficient to capture intricate patterns, especially under extreme non-IID (non-independent and identically distributed) conditions. Moreover, ensuring robustness against malicious clients and poisoned updates remains a major challenge. Accordingly, this paper introduces RIFLE - a Robust, distillation-based Federated Learning framework that replaces gradient sharing with logit-based knowledge transfer. By leveraging a knowledge distillation aggregation scheme, RIFLE enables the training of deep models such as VGG-19 and Resnet18 within constrained IoT systems. Furthermore, a Kullback-Leibler (KL) divergence-based validation mechanism quantifies the reliability of client updates without exposing raw data, achieving high trust and privacy preservation simultaneously. Experiments on three benchmark datasets (MNIST, CIFAR-10, and CIFAR-100) under heterogeneous non-IID conditions demonstrate that RIFLE reduces false-positive detections by up to 87.5%, enhances poisoning attack mitigation by 62.5%, and achieves up to 28.3% higher accuracy compared to conventional federated learning baselines within only 10 rounds. Notably, RIFLE reduces VGG19 training time from over 600 days to just 1.39 hours on typical IoT devices (0.3 GFLOPS), making deep learning practical in resource-constrained networks.",
      "url": "https://arxiv.org/abs/2602.08446",
      "date": "2026-02-09",
      "authors": [
        "Pouria Arefijamal",
        "Mahdi Ahmadlou",
        "Bardia Safaei",
        "J√∂rg Henkel"
      ],
      "tags": [
        "cs.LG",
        "cs.CR",
        "cs.DC",
        "cs.NI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08446",
      "pdfUrl": "https://arxiv.org/pdf/2602.08446v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08376",
      "source": "arxiv",
      "title": "OJBKQ: Objective-Joint Babai-Klein Quantization",
      "description": "Post-training quantization (PTQ) is widely used to compress large language models without retraining. However, many existing weight-only methods rely on heuristic objectives and greedy rounding, thus leading to noticeable degradation under low-bit quantization. In this work, we introduce OJBKQ (Objective-Joint Babai-Klein Quantization with K-Best Sampling), a layer-wise PTQ method that formulates weight quantization as a joint optimization problem over activations and weights. This formulation results in a multiple-right-hand-side box-constrained integer least squares (BILS) problem in each layer, which is NP-hard. For each column of the weight matrix, we apply an extended Babai nearest-plane algorithm and an extended version of Klein's randomized Babai algorithm to find the minimum-residual Babai-Klein point, a sub-optimal solution to the BILS problem. Experimental results on large language models show that OJBKQ achieves lower perplexity at 3-4 bits compared to existing PTQ approaches, while maintaining comparable computational cost.",
      "url": "https://arxiv.org/abs/2602.08376",
      "date": "2026-02-09",
      "authors": [
        "Xinyu Wang",
        "Ziyu Zhao",
        "Peng Lu",
        "Yu Gu",
        "Xiao-Wen Chang"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08376",
      "pdfUrl": "https://arxiv.org/pdf/2602.08376v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08323",
      "source": "arxiv",
      "title": "Antiferromagnetic Tunnel Junctions (AFMTJs) for In-Memory Computing: Modeling and Case Study",
      "description": "Antiferromagnetic Tunnel Junctions (AFMTJs) enable picosecond switching and femtojoule writes through ultrafast sublattice dynamics. We present the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling. SPICE-based simulations show that AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into an in-memory computing architecture, AFMTJs deliver 17.5x average speedup and nearly 20x energy savings versus a CPU baseline-significantly outperforming MTJ-based IMC. These results establish AFMTJs as a compelling primitive for scalable, low-power computing.",
      "url": "https://arxiv.org/abs/2602.08323",
      "date": "2026-02-09",
      "authors": [
        "Yousuf Choudhary",
        "Tosiron Adegbija"
      ],
      "tags": [
        "cs.AR",
        "cs.ET"
      ],
      "matchedGroup": "ai-hardware",
      "arxivId": "2602.08323",
      "pdfUrl": "https://arxiv.org/pdf/2602.08323v1",
      "primaryCategory": "cs.AR"
    },
    {
      "id": "arxiv-2602.08269",
      "source": "arxiv",
      "title": "Quantization-aware Photonic Homodyne computing for Accelerated Artificial Intelligence and Scientific Simulation",
      "description": "Modern problems in high-performance computing, ranging from training and inferencing deep learning models in computer vision and language models to simulating complex physical systems with nonlinearly-coupled equations, require exponential growth of computational resources. Photonic analog systems are emerging with solutions of intrinsic parallelism, high bandwidth, and low propagation loss. However, their application has been hindered by the low analog accuracy due to the electro-optic distortion, material nonlinearities, and signal-to-noise ratios. Here we overcome this barrier with a quantization-aware digital-photonic mixed-precision framework across chiplets for accelerated AI processing and physical simulation. Using Lithium Niobate photonics with channel equalization techniques, we demonstrate linear multiplication (9-bit amplitude-phase decoupling) in homodyne optical logics with 6-bit precision at the clock rate of 128 giga-symbol-per-second (128 GS/s), enabling AI processing with 6 ns latency. Codesign hardware-algorithms, including iterative solvers, sparse-dense quantization, and bit-sliced matrix multiplication, explore photonic amplitude and phase coherence for complex-valued, physics-inspired computation. In electromagnetic problems, our approach yields 12-bit solutions for partial differential equations (PDEs) in scattering problems that would conventionally require up to 32-bit and often even 64-bit precision. These results preserve digital-level fidelity while leveraging the high-speed low-energy photonic hardware, establishing a pathway toward general-purpose optical acceleration for generative artificial intelligence, real-time robotics, and accurate simulation for climate challenges and biological discoveries.",
      "url": "https://arxiv.org/abs/2602.08269",
      "date": "2026-02-09",
      "authors": [
        "Lian Zhou",
        "Kaiwen Xue",
        "Amirhossein Fallah",
        "Lijin Liu",
        "Chun-Ho Lee",
        "Kiwon Kwon",
        "Clayton Cheung",
        "Yuan Li",
        "Yue Yu",
        "Yun-Jhu Lee",
        "Songlin Zhao",
        "Ryan Hamerly",
        "Edo Waks",
        "Dirk Englund",
        "Constantine Sideris",
        "Mengjie Yu",
        "Zaijun Chen"
      ],
      "tags": [
        "cs.ET"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08269",
      "pdfUrl": "https://arxiv.org/pdf/2602.08269v1",
      "primaryCategory": "cs.ET"
    },
    {
      "id": "arxiv-2602.08240",
      "source": "arxiv",
      "title": "PTS-SNN: A Prompt-Tuned Temporal Shift Spiking Neural Networks for Efficient Speech Emotion Recognition",
      "description": "Speech Emotion Recognition (SER) is widely deployed in Human-Computer Interaction, yet the high computational cost of conventional models hinders their implementation on resource-constrained edge devices. Spiking Neural Networks (SNNs) offer an energy-efficient alternative due to their event-driven nature; however, their integration with continuous Self-Supervised Learning (SSL) representations is fundamentally challenged by distribution mismatch, where high-dynamic-range embeddings degrade the information coding capacity of threshold-based neurons. To resolve this, we propose Prompt-Tuned Spiking Neural Networks (PTS-SNN), a parameter-efficient neuromorphic adaptation framework that aligns frozen SSL backbones with spiking dynamics. Specifically, we introduce a Temporal Shift Spiking Encoder to capture local temporal dependencies via parameter-free channel shifts, establishing a stable feature basis. To bridge the domain gap, we devise a Context-Aware Membrane Potential Calibration strategy. This mechanism leverages a Spiking Sparse Linear Attention module to aggregate global semantic context into learnable soft prompts, which dynamically regulate the bias voltages of Parametric Leaky Integrate-and-Fire (PLIF) neurons. This regulation effectively centers the heterogeneous input distribution within the responsive firing range, mitigating functional silence or saturation. Extensive experiments on five multilingual datasets (e.g., IEMOCAP, CASIA, EMODB) demonstrate that PTS-SNN achieves 73.34\\% accuracy on IEMOCAP, comparable to competitive Artificial Neural Networks (ANNs), while requiring only 1.19M trainable parameters and 0.35 mJ inference energy per sample.",
      "url": "https://arxiv.org/abs/2602.08240",
      "date": "2026-02-09",
      "authors": [
        "Xun Su",
        "Huamin Wang",
        "Qi Zhang"
      ],
      "tags": [
        "cs.AI",
        "cs.SD"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.08240",
      "pdfUrl": "https://arxiv.org/pdf/2602.08240v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.08218",
      "source": "arxiv",
      "title": "Sparsity-Aware Evolution for Model Merging",
      "description": "We propose a sparsity-aware evolutionary (SAE) framework for model merging that involves iterative pruning-merging cycles to act as a novel mutation operator. We incorporate the sparsity constraints into the score function, which steers the evolutionary process to favor more sparse models, in addition to other conventional performance scores. Interestingly, the by-product of \\textit{competition} for sparsity introduces an extra local \\textit{attraction} and interplay into the evolutionary process: if one competitor has more zero elements, the other competitor's non-zero elements will occupy those positions, even though the less sparse competitor loses to the more sparse competitor in other positions. The proposed pipeline is evaluated on a variety of large-scale LLM benchmarks. Experiments demonstrate that our approach can improve model merging reliability across multiple benchmarks, and is easy to incorporate due to its simplicity and being orthogonal to most existing approaches.",
      "url": "https://arxiv.org/abs/2602.08218",
      "date": "2026-02-09",
      "authors": [
        "Huan Zhang",
        "Yanjian Zhang",
        "Guillaume Wisniewski",
        "Nadi Tomeh",
        "Bang Liu"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08218",
      "pdfUrl": "https://arxiv.org/pdf/2602.08218v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08136",
      "source": "arxiv",
      "title": "Robustness of Vision Language Models Against Split-Image Harmful Input Attacks",
      "description": "Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.",
      "url": "https://arxiv.org/abs/2602.08136",
      "date": "2026-02-08",
      "authors": [
        "Md Rafi Ur Rashid",
        "MD Sadik Hossain Shanto",
        "Vishnu Asutosh Dasu",
        "Shagufta Mehnaz"
      ],
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.08136",
      "pdfUrl": "https://arxiv.org/pdf/2602.08136v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.08081",
      "source": "arxiv",
      "title": "Investigating Energy Bounds of Analog Compute-in-Memory with Local Normalization",
      "description": "Modern edge AI workloads demand maximum energy efficiency, motivating the pursuit of analog Compute-in-Memory (CIM) architectures. Simultaneously, the popularity of Large-Language-Models (LLMs) drives the adoption of low-bit floating-point formats which prioritize dynamic range. However, the conventional direct-accumulation CIM accommodates floating-points by normalizing them to a shared widened fixed-point scale. Consequently, hardware resolution is dictated by the input's dynamic range rather than its precision, and energy consumption is dominated by the ADC. We address this limitation by introducing local normalization for each input, weight, and multiply-accumulate (MAC) output via a Gain-Ranging MAC (GR-MAC). Normalization overhead is handled by low-power digital logic, enabling the computationally expensive MAC operation to remain in the energy-efficient low-precision analog regime. Energy modelling shows that the addition of a gain-ranging Stage to the MAC enables a 4-bit increase in input dynamic range without increased energy consumption at a 35 dB SQNR standard. Additionally, the ADC resolution requirement becomes invariant to input distribution assumptions, allowing construction of an upper bound with a 1.5-bit reduction compared to the conventional lower bound. These results establish a pathway towards unlocking favourable energy scaling trends of analog CIM for modern AI workloads.",
      "url": "https://arxiv.org/abs/2602.08081",
      "date": "2026-02-08",
      "authors": [
        "Brian Rojkov",
        "Shubham Ranjan",
        "Derek Wright",
        "Manoj Sachdev"
      ],
      "tags": [
        "cs.AR"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.08081",
      "pdfUrl": "https://arxiv.org/pdf/2602.08081v1",
      "primaryCategory": "cs.AR"
    },
    {
      "id": "arxiv-2602.08080",
      "source": "arxiv",
      "title": "The CAPSARII Approach to Cyber-Secure Wearable, Ultra-Low-Power Networked Sensors for Soldier Health Monitoring",
      "description": "The European Defence Agency's revised Capability Development Plan (CDP) identifies as a priority improving ground combat capabilities by enhancing soldiers' equipment for better protection. The CAPSARII project proposes in innovative wearable system and Internet of Battlefield Things (IoBT) framework to monitor soldiers' physiological and psychological status, aiding tactical decisions and medical support. The CAPSARII system will enhance situational awareness and operational effectiveness by monitoring physiological, movement and environmental parameters, providing real-time tactical decision support through AI models deployed on edge nodes and enable data analysis and comparative studies via cloud-based analytics. CAPSARII also aims at improving usability through smart textile integration, longer battery life, reducing energy consumption through software and hardware optimizations, and address security concerns with efficient encryption and strong authentication methods. This innovative approach aims to transform military operations by providing a robust, data-driven decision support tool.",
      "url": "https://arxiv.org/abs/2602.08080",
      "date": "2026-02-08",
      "authors": [
        "Luciano Bozzi",
        "Christian Celidonio",
        "Umberto Nuzzi",
        "Massimo Biagini",
        "Stefano Cherubin",
        "Asbj√∏rn Djupdal",
        "Tor Andre Haugdahl",
        "Andrea Aliverti",
        "Alessandra Angelucci",
        "Giovanni Agosta",
        "Gerardo Pelosi",
        "Paolo Belluco",
        "Samuele Polistina",
        "Riccardo Volpi",
        "Luigi Malag√≤",
        "Michael Schneider",
        "Florian Wieczorek",
        "Xabier Eguiluz"
      ],
      "tags": [
        "cs.ET",
        "cs.DC",
        "cs.LG"
      ],
      "matchedGroup": "optimization",
      "arxivId": "2602.08080",
      "pdfUrl": "https://arxiv.org/pdf/2602.08080v1",
      "primaryCategory": "cs.ET"
    },
    {
      "id": "arxiv-2602.08050",
      "source": "arxiv",
      "title": "Interpretable Fuzzy Systems For Forward Osmosis Desalination",
      "description": "Preserving interpretability in fuzzy rule-based systems (FRBS) is vital for water treatment, where decisions impact public health. While structural interpretability has been addressed using multi-objective algorithms, semantic interpretability often suffers due to fuzzy sets with low distinguishability. We propose a human-in-the-loop approach for developing interpretable FRBS to predict forward osmosis desalination productivity. Our method integrates expert-driven grid partitioning for distinguishable membership functions, domain-guided feature engineering to reduce redundancy, and rule pruning based on firing strength. This approach achieved comparable predictive performance to cluster-based FRBS while maintaining semantic interpretability and meeting structural complexity constraints, providing an explainable solution for water treatment applications.",
      "url": "https://arxiv.org/abs/2602.08050",
      "date": "2026-02-08",
      "authors": [
        "Qusai Khaled",
        "Uzay Kaymak",
        "Laura Genga"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.08050",
      "pdfUrl": "https://arxiv.org/pdf/2602.08050v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08043",
      "source": "arxiv",
      "title": "V-ABFT: Variance-Based Adaptive Threshold for Fault-Tolerant Matrix Multiplication in Mixed-Precision Deep Learning",
      "description": "Algorithm-Based Fault Tolerance (ABFT) is widely adopted to detect silent data corruptions (SDCs) in matrix multiplication, a cornerstone operation in deep learning systems. However, existing threshold determination methods face critical challenges: analytical bounds are overly conservative, while probabilistic approaches like A-ABFT yield thresholds $160$--$4200\\times$ larger than actual rounding errors. We present V-ABFT, a variance-based adaptive threshold algorithm that achieves tighter error bounds by directly modeling the verification difference. By leveraging statistical variance estimation, V-ABFT reduces the threshold-to-actual-error ratio to approximately $7$--$20\\times$ for FP32/FP64 and $48$--$158\\times$ for BF16, representing a \\textbf{6--48$\\times$ improvement} over A-ABFT while maintaining zero false positive rate across BF16, FP16, FP32, and FP64 precisions. Furthermore, we demonstrate that for fused-kernel ABFT implementations that verify before output quantization, low-precision GEMM can use FP32-level thresholds ($e_{\\max} \\approx 10^{-6}$), enabling \\textbf{$\\sim$1000$\\times$ finer detection granularity} compared to offline verification with low-precision output ($e_{\\max} \\approx 10^{-3}$). We reproduce A-ABFT's experimental setup and validate our implementation against the original paper's results. Our method requires only $O(n)$ complexity using max/min/mean statistics, compared to A-ABFT's $O(pn)$ for finding $p$ largest values. Extensive experiments on synthetic data and real model weights (LLaMA-7B, GPT-2, ViT) demonstrate V-ABFT's effectiveness across diverse distributions. V-ABFT is platform-agnostic and has been integrated into fault-tolerant GEMM implementations on both NPUs and GPUs.",
      "url": "https://arxiv.org/abs/2602.08043",
      "date": "2026-02-08",
      "authors": [
        "Yiheng Gao",
        "Qin Hua",
        "Zizhong Chen"
      ],
      "tags": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.08043",
      "pdfUrl": "https://arxiv.org/pdf/2602.08043v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.08030",
      "source": "arxiv",
      "title": "Free(): Learning to Forget in Malloc-Only Reasoning Models",
      "description": "Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as \"malloc-only\" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state. Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.",
      "url": "https://arxiv.org/abs/2602.08030",
      "date": "2026-02-08",
      "authors": [
        "Yilun Zheng",
        "Dongyang Ma",
        "Tian Liang",
        "Jiahao Xu",
        "Xinting Huang",
        "Lijie Chen",
        "Haitao Mi",
        "Yan Wang"
      ],
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.08030",
      "pdfUrl": "https://arxiv.org/pdf/2602.08030v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.08005",
      "source": "arxiv",
      "title": "DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity",
      "description": "The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.",
      "url": "https://arxiv.org/abs/2602.08005",
      "date": "2026-02-08",
      "authors": [
        "Jitai Hao",
        "Qiang Huang",
        "Yaowei Wang",
        "Min Zhang",
        "Jun Yu"
      ],
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.08005",
      "pdfUrl": "https://arxiv.org/pdf/2602.08005v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.07973",
      "source": "arxiv",
      "title": "On Improving Neurosymbolic Learning by Exploiting the Representation Space",
      "description": "We study the problem of learning neural classifiers in a neurosymbolic setting where the hidden gold labels of input instances must satisfy a logical formula. Learning in this setting proceeds by first computing (a subset of) the possible combinations of labels that satisfy the formula and then computing a loss using those combinations and the classifiers' scores. One challenge is that the space of label combinations can grow exponentially, making learning difficult. We propose a technique that prunes this space by exploiting the intuition that instances with similar latent representations are likely to share the same label. While this intuition has been widely used in weakly supervised learning, its application in our setting is challenging due to label dependencies imposed by logical constraints. We formulate the pruning process as an integer linear program that discards inconsistent label combinations while respecting logical structure. Our approach, CLIPPER, is orthogonal to existing training algorithms and can be seamlessly integrated with them. Across 16 benchmarks over complex neurosymbolic tasks, we demonstrate that CLIPPER boosts the performance of state-of-the-art neurosymbolic engines like Scallop, Dolphin, and ISED by up to 48%, 53%, and 8%, leading to state-of-the-art accuracies.",
      "url": "https://arxiv.org/abs/2602.07973",
      "date": "2026-02-08",
      "authors": [
        "Aaditya Naik",
        "Efthymia Tsamoura",
        "Shibo Jin",
        "Mayur Naik",
        "Dan Roth"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.07973",
      "pdfUrl": "https://arxiv.org/pdf/2602.07973v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07963",
      "source": "arxiv",
      "title": "Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms",
      "description": "Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.",
      "url": "https://arxiv.org/abs/2602.07963",
      "date": "2026-02-08",
      "authors": [
        "Vaibhav Shukla",
        "Hardik Sharma",
        "Adith N Reganti",
        "Soham Wasmatkar",
        "Bagesh Kumar",
        "Vrijendra Singh"
      ],
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.07963",
      "pdfUrl": "https://arxiv.org/pdf/2602.07963v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.07889",
      "source": "arxiv",
      "title": "Efficient Anti-exploration via VQVAE and Fuzzy Clustering in Offline Reinforcement Learning",
      "description": "Pseudo-count is an effective anti-exploration method in offline reinforcement learning (RL) by counting state-action pairs and imposing a large penalty on rare or unseen state-action pair data. Existing anti-exploration methods count continuous state-action pairs by discretizing these data, but often suffer from the issues of dimension disaster and information loss in the discretization process, leading to efficiency and performance reduction, and even failure of policy learning. In this paper, a novel anti-exploration method based on Vector Quantized Variational Autoencoder (VQVAE) and fuzzy clustering in offline RL is proposed. We first propose an efficient pseudo-count method based on the multi-codebook VQVAE to discretize state-action pairs, and design an offline RL anti-exploitation method based on the proposed pseudo-count method to handle the dimension disaster issue and improve the learning efficiency. In addition, a codebook update mechanism based on fuzzy C-means (FCM) clustering is developed to improve the use rate of vectors in codebooks, addressing the information loss issue in the discretization process. The proposed method is evaluated on the benchmark of Datasets for Deep Data-Driven Reinforcement Learning (D4RL), and experimental results show that the proposed method performs better and requires less computing cost in multiple complex tasks compared to state-of-the-art (SOTA) methods.",
      "url": "https://arxiv.org/abs/2602.07889",
      "date": "2026-02-08",
      "authors": [
        "Long Chen",
        "Yinkui Liu",
        "Shen Li",
        "Bo Tang",
        "Xuemin Hu"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.07889",
      "pdfUrl": "https://arxiv.org/pdf/2602.07889v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07849",
      "source": "arxiv",
      "title": "LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge",
      "description": "Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.",
      "url": "https://arxiv.org/abs/2602.07849",
      "date": "2026-02-08",
      "authors": [
        "Xin Wang",
        "Hualin Zhou",
        "Sheng Guang Wang",
        "Ting Dang",
        "Yu Zhang",
        "Hong Jia",
        "Tao Gu"
      ],
      "tags": [
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.07849",
      "pdfUrl": "https://arxiv.org/pdf/2602.07849v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.07804",
      "source": "arxiv",
      "title": "Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models",
      "description": "While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge. However, existing approaches generally depend on static heuristic rules and fail to account for the interdependencies among layers, thereby limiting the effectiveness of the pruning process. To this end, this paper proposes a game-theoretic framework that formulates layer pruning as a cooperative game in which each layer acts as a player and model performance serves as the utility. As computing exact Shapley values is computationally infeasible for large language models (LLMs), we propose using a lightweight surrogate network to estimate layer-wise marginal contributions. This network can predict LLM performance for arbitrary layer combinations at a low computational cost. Additionally, we employ stratified Monte Carlo mask sampling to further reduce the cost of Sharpley value estimation. This approach captures inter-layer dependencies and dynamically identifies critical layers for pruning. Extensive experiments demonstrate the consistent superiority of our method in terms of perplexity and zero-shot accuracy, achieving more efficient and effective layer-wise pruning for large language models.",
      "url": "https://arxiv.org/abs/2602.07804",
      "date": "2026-02-08",
      "authors": [
        "Xuan Ding",
        "Pengyu Tong",
        "Ranjie Duan",
        "Yunjian Zhang",
        "Rui Sun",
        "Yao Zhu"
      ],
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.07804",
      "pdfUrl": "https://arxiv.org/pdf/2602.07804v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.07721",
      "source": "arxiv",
      "title": "ParisKV: Fast and Drift-Robust KV-Cache Retrieval for Long-Context LLMs",
      "description": "KV-cache retrieval is essential for long-context LLM inference, yet existing methods struggle with distribution drift and high latency at scale. We introduce ParisKV, a drift-robust, GPU-native KV-cache retrieval framework based on collision-based candidate selection, followed by a quantized inner-product reranking estimator. For million-token contexts, ParisKV supports CPU-offloaded KV caches via Unified Virtual Addressing (UVA), enabling on-demand top-$k$ fetching with minimal overhead. ParisKV matches or outperforms full attention quality on long-input and long-generation benchmarks. It achieves state-of-the-art long-context decoding efficiency: it matches or exceeds full attention speed even at batch size 1 for long contexts, delivers up to 2.8$\\times$ higher throughput within full attention's runnable range, and scales to million-token contexts where full attention runs out of memory. At million-token scale, ParisKV reduces decode latency by 17$\\times$ and 44$\\times$ compared to MagicPIG and PQCache, respectively, two state-of-the-art KV-cache Top-$k$ retrieval baselines.",
      "url": "https://arxiv.org/abs/2602.07721",
      "date": "2026-02-07",
      "authors": [
        "Yanlin Qi",
        "Xinhang Chen",
        "Huiqiang Jiang",
        "Qitong Wang",
        "Botao Peng",
        "Themis Palpanas"
      ],
      "tags": [
        "cs.LG",
        "cs.CL",
        "cs.DB"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.07721",
      "pdfUrl": "https://arxiv.org/pdf/2602.07721v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07670",
      "source": "arxiv",
      "title": "Surprisal-Guided Selection: Compute-Optimal Test-Time Strategies for Execution-Grounded Code Generation",
      "description": "Test-time training (TTT) adapts language models through gradient-based updates at inference. But is adaptation the right strategy? We study compute-optimal test-time strategies for verifiable execution-grounded (VEG) tasks, domains like GPU kernel optimization where a deterministic evaluator provides dense, continuous reward signals. Using KernelBench as our testbed and a 120B-parameter model (GPT-OSS-120B with LoRA adaptation), we find that search outperforms minimal adaptation (1-5 gradient steps): Best-of-N sampling achieves 90% task success (18/20 tasks) at K=64 across the full KernelBench L1 eval set while TTT's best checkpoint reaches only 30.6% (3-seed mean), with TTT's \"equivalent K\" falling below 1, worse than single-sample inference. The failure mode is over-sharpening: gradient updates collapse diversity toward mediocre solutions rather than discovering optimal ones. Our main contribution is surprisal-guided selection: selecting the highest-surprisal (lowest-confidence) correct sample yields 80% success vs. 50% for most-confident selection, a 30% improvement. Extending to surprisal-guided-top3 matches oracle performance at 100%. This zero-cost strategy, validated through length-controlled analysis, recovers oracle performance. For dense-reward VEG tasks, compute should be allocated to sample diversity and intelligent selection rather than gradient adaptation. The surprisal-guided selection principle may generalize to other execution-grounded domains where optimal solutions occupy the distribution tail.",
      "url": "https://arxiv.org/abs/2602.07670",
      "date": "2026-02-07",
      "authors": [
        "Jarrod Barnes"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "optimization",
      "arxivId": "2602.07670",
      "pdfUrl": "https://arxiv.org/pdf/2602.07670v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07618",
      "source": "arxiv",
      "title": "Dense Neural Networks are not Universal Approximators",
      "description": "We investigate the approximation capabilities of dense neural networks. While universal approximation theorems establish that sufficiently large architectures can approximate arbitrary continuous functions if there are no restrictions on the weight values, we show that dense neural networks do not possess this universality. Our argument is based on a model compression approach, combining the weak regularity lemma with an interpretation of feedforward networks as message passing graph neural networks. We consider ReLU neural networks subject to natural constraints on weights and input and output dimensions, which model a notion of dense connectivity. Within this setting, we demonstrate the existence of Lipschitz continuous functions that cannot be approximated by such networks. This highlights intrinsic limitations of neural networks with dense layers and motivates the use of sparse connectivity as a necessary ingredient for achieving true universality.",
      "url": "https://arxiv.org/abs/2602.07618",
      "date": "2026-02-07",
      "authors": [
        "Levi Rauchwerger",
        "Stefanie Jegelka",
        "Ron Levie"
      ],
      "tags": [
        "cs.LG",
        "stat.ML"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.07618",
      "pdfUrl": "https://arxiv.org/pdf/2602.07618v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07616",
      "source": "arxiv",
      "title": "SERE: Similarity-based Expert Re-routing for Efficient Batch Decoding in MoE Models",
      "description": "Mixture-of-Experts (MoE) architectures employ sparse activation to deliver faster training and inference with higher accuracy than dense LLMs. However, in production serving, MoE models require batch inference to optimize hardware efficiency, which may cause excessive expert activation and thus slow the memory-bound decoding stage. To address the fundamental tension between batch decoding and expert sparsity, we present SERE, a Similarity-based Expert Re-routing method for Efficient batch decoding in MoE models. SERE dynamically reduces the number of active experts in an input-aware manner by re-routing tokens from secondary experts to their most similar primary counterparts. It also leverages similarity patterns to identify and preserve critical experts, thereby preventing capability loss. Notably, SERE avoids static expert pruning or merging, instead enabling dynamic expert skipping based on batch-level expert redundancy. Additionally, we provide an efficient custom CUDA kernel for SERE, enabling plug-and-play use in vLLM with only a single-line code change. Extensive experiments on various complex reasoning benchmarks demonstrate that SERE achieves up to 2.0x speedup with minimal quality loss, providing a practical solution for cost-efficient and latency-sensitive large-scale MoE deployment. Code implementation of SERE can be found in https://github.com/JL-Cheng/SERE.",
      "url": "https://arxiv.org/abs/2602.07616",
      "date": "2026-02-07",
      "authors": [
        "Juntong Wu",
        "Jialiang Cheng",
        "Fuyu Lv",
        "Ou Dan",
        "Li Yuan"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.07616",
      "pdfUrl": "https://arxiv.org/pdf/2602.07616v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07596",
      "source": "arxiv",
      "title": "Astro: Activation-guided Structured Regularization for Outlier-Robust LLM Post-Training Quantization",
      "description": "Weight-only post-training quantization (PTQ) is crucial for efficient Large Language Model (LLM) deployment but suffers from accuracy degradation caused by weight and activation outliers. Existing mitigation strategies often face critical limitations: they either yield insufficient outlier suppression or incur significant deployment inefficiencies, such as inference latency, heavy preprocessing, or reliance on complex operator fusion. To resolve these limitations, we leverage a key insight: over-parameterized LLMs often converge to Flat Minima, implying a vast equivalent solution space where weights can be adjusted without compromising accuracy. Building on this, we propose Astro, an Activation-guided Structured Regularization framework designed to suppress the negative effects of outliers in a hardware-friendly and efficient manner. Leveraging the activation-guided regularization objective, Astro actively reconstructs intrinsically robust weights, aggressively suppressing weight outliers corresponding to high-magnitude activations without sacrificing model accuracy. Crucially, Astro introduces zero inference latency and is orthogonal to mainstream quantization methods like GPTQ. Extensive experiments show that Astro achieves highly competitive performance; notably, on LLaMA-2-7B, it achieves better performance than complex learning-based rotation methods with almost 1/3 of the quantization time.",
      "url": "https://arxiv.org/abs/2602.07596",
      "date": "2026-02-07",
      "authors": [
        "Xi Chen",
        "Ming Li",
        "Junxi Li",
        "Changsheng Li",
        "Peisong Wang",
        "Lizhong Ding",
        "Ye Yuan",
        "Guoren Wang"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.07596",
      "pdfUrl": "https://arxiv.org/pdf/2602.07596v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07547",
      "source": "arxiv",
      "title": "Linguistic properties and model scale in brain encoding: from small to compressed language models",
      "description": "Recent work has shown that scaling large language models (LLMs) improves their alignment with human brain activity, yet it remains unclear what drives these gains and which representational properties are responsible. Although larger models often yield better task performance and brain alignment, they are increasingly difficult to analyze mechanistically. This raises a fundamental question: what is the minimal model capacity required to capture brain-relevant representations? To address this question, we systematically investigate how constraining model scale and numerical precision affects brain alignment. We compare full-precision LLMs, small language models (SLMs), and compressed variants (quantized and pruned) by predicting fMRI responses during naturalistic language comprehension. Across model families up to 14B parameters, we find that 3B SLMs achieve brain predictivity indistinguishable from larger LLMs, whereas 1B models degrade substantially, particularly in semantic language regions. Brain alignment is remarkably robust to compression: most quantization and pruning methods preserve neural predictivity, with GPTQ as a consistent exception. Linguistic probing reveals a dissociation between task performance and brain predictivity: compression degrades discourse, syntax, and morphology, yet brain predictivity remains largely unchanged. Overall, brain alignment saturates at modest model scales and is resilient to compression, challenging common assumptions about neural scaling and motivating compact models for brain-aligned language modeling.",
      "url": "https://arxiv.org/abs/2602.07547",
      "date": "2026-02-07",
      "authors": [
        "Subba Reddy Oota",
        "Vijay Rowtula",
        "Satya Sai Srinath Namburi",
        "Khushbu Pahwa",
        "Anant Khandelwal",
        "Manish Gupta",
        "Tanmoy Chakraborty",
        "Bapi S. Raju"
      ],
      "tags": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.07547",
      "pdfUrl": "https://arxiv.org/pdf/2602.07547v1",
      "primaryCategory": "q-bio.NC"
    },
    {
      "id": "arxiv-2602.07518",
      "source": "arxiv",
      "title": "Physical Analog Kolmogorov-Arnold Networks based on Reconfigurable Nonlinear-Processing Units",
      "description": "Kolmogorov-Arnold Networks (KANs) shift neural computation from linear layers to learnable nonlinear edge functions, but implementing these nonlinearities efficiently in hardware remains an open challenge. Here we introduce a physical analog KAN architecture in which edge functions are realized in materia using reconfigurable nonlinear-processing units (RNPUs): multi-terminal nanoscale silicon devices whose input-output characteristics are tuned via control voltages. By combining multiple RNPUs into an edge processor and assembling these blocks into a reconfigurable analog KAN (aKAN) architecture with integrated mixed-signal interfacing, we establish a realistic system-level hardware implementation that enables compact KAN-style regression and classification with programmable nonlinear transformations. Using experimentally calibrated RNPU models and hardware measurements, we demonstrate accurate function approximation across increasing task complexity while requiring fewer or comparable trainable parameters than multilayer perceptrons (MLPs). System-level estimates indicate an energy per inference of $\\sim$250 pJ and an end-to-end inference latency of $\\sim$600 ns for a representative workload, corresponding to a $\\sim$10$^{2}$-10$^{3}\\times$ reduction in energy accompanied by a $\\sim$10$\\times$ reduction in area compared to a digital fixed-point MLP at similar approximation error. These results establish RNPUs as scalable, hardware-native nonlinear computing primitives and identify analog KAN architectures as a realistic silicon-based pathway toward energy-, latency-, and footprint-efficient analog neural-network hardware, particularly for edge inference.",
      "url": "https://arxiv.org/abs/2602.07518",
      "date": "2026-02-07",
      "authors": [
        "Manuel Escudero",
        "Mohamadreza Zolfagharinejad",
        "Sjoerd van den Belt",
        "Nikolaos Alachiotis",
        "Wilfred G. van der Wiel"
      ],
      "tags": [
        "cs.ET",
        "cs.AR",
        "cs.LG",
        "nlin.AO"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.07518",
      "pdfUrl": "https://arxiv.org/pdf/2602.07518v1",
      "primaryCategory": "cs.ET"
    },
    {
      "id": "arxiv-2602.07465",
      "source": "arxiv",
      "title": "On the Importance of a Multi-Scale Calibration for Quantization",
      "description": "Post-training quantization (PTQ) is a cornerstone for efficiently deploying large language models (LLMs), where a small calibration set critically affects quantization performance. However, conventional practices rely on random sequences of fixed length, overlooking the variable-length nature of LLM inputs. Input length directly influences the activation distribution and, consequently, the weight importance captured by the Hessian, which in turn affects quantization outcomes. As a result, Hessian estimates derived from fixed-length calibration may fail to represent the true importance of weights across diverse input scenarios. We propose MaCa (Matryoshka Calibration), a simple yet effective method for length-aware Hessian construction. MaCa (i) incorporates multi-scale sequence length information into Hessian estimation and (ii) regularizes each sequence as an independent sample, yielding a more stable and fruitful Hessian for accurate quantization. Experiments on state-of-the-art LLMs (e.g., Qwen3, Gemma3, LLaMA3) demonstrate that MaCa consistently improves accuracy under low bit quantization, offering a lightweight enhancement compatible with existing PTQ frameworks. To the best of our knowledge, this is the first work to systematically highlight the role of multi-scale calibration in LLM quantization.",
      "url": "https://arxiv.org/abs/2602.07465",
      "date": "2026-02-07",
      "authors": [
        "Seungwoo Son",
        "Ingyu Seong",
        "Junhan Kim",
        "Hyemi Jang",
        "Yongkweon Jeon"
      ],
      "tags": [
        "cs.LG",
        "cs.CL"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.07465",
      "pdfUrl": "https://arxiv.org/pdf/2602.07465v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07434",
      "source": "arxiv",
      "title": "Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots",
      "description": "Effective human-robot interaction requires emotionally rich multimodal expressions, yet most humanoid robots lack coordinated speech, facial expressions, and gestures. Meanwhile, real-world deployment demands on-device solutions that can operate autonomously without continuous cloud connectivity. To bridging \\underline{\\textit{S}}peech, \\underline{\\textit{E}}motion, and \\underline{\\textit{M}}otion, we present \\textit{SeM$^2$}, a Vision Language Model-based framework that orchestrates emotionally coherent multimodal interactions through three key components: a multimodal perception module capturing user contextual cues, a Chain-of-Thought reasoning for response planning, and a novel Semantic-Sequence Aligning Mechanism (SSAM) that ensures precise temporal coordination between verbal content and physical expressions. We implement both cloud-based and \\underline{\\textit{e}}dge-deployed versions (\\textit{SeM$^2_e$}), with the latter knowledge distilled to operate efficiently on edge hardware while maintaining 95\\% of the relative performance. Comprehensive evaluations demonstrate that our approach significantly outperforms unimodal baselines in naturalness, emotional clarity, and modal coherence, advancing socially expressive humanoid robotics for diverse real-world environments.",
      "url": "https://arxiv.org/abs/2602.07434",
      "date": "2026-02-07",
      "authors": [
        "Songhua Yang",
        "Xuetao Li",
        "Xuanye Fei",
        "Mengde Li",
        "Miao Li"
      ],
      "tags": [
        "cs.RO",
        "cs.AI"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.07434",
      "pdfUrl": "https://arxiv.org/pdf/2602.07434v1",
      "primaryCategory": "cs.RO"
    },
    {
      "id": "arxiv-2602.07400",
      "source": "arxiv",
      "title": "BitLogic: Training Framework for Gradient-Based FPGA-Native Neural Networks",
      "description": "The energy and latency costs of deep neural network inference are increasingly driven by deployment rather than training, motivating hardware-specialized alternatives to arithmetic-heavy models. Field-Programmable Gate Arrays (FPGAs) provide an attractive substrate for such specialization, yet existing FPGA-based neural approaches are fragmented and difficult to compare. We present BitLogic, a fully gradient-based, end-to-end trainable framework for FPGA-native neural networks built around Lookup Table (LUT) computation. BitLogic replaces multiply-accumulate operations with differentiable LUT nodes that map directly to FPGA primitives, enabling native binary computation, sparse connectivity, and efficient hardware realization. The framework offers a modular functional API supporting diverse architectures, along with learned encoders, hardware-aware heads, and multiple boundary-consistent LUT relaxations. An automated Register Transfer Level (RTL) export pipeline translates trained PyTorch models into synthesizable HDL, ensuring equivalence between software and hardware inference. Experiments across standard vision benchmarks and heterogeneous hardware platforms demonstrate competitive accuracy and substantial gains in FPGA efficiency, including 72.3% test accuracy on CIFAR-10 achieved with fewer than 0.3M logic gates, while attaining sub-20 ns single-sample inference using only LUT resources.",
      "url": "https://arxiv.org/abs/2602.07400",
      "date": "2026-02-07",
      "authors": [
        "Simon B√ºhrer",
        "Andreas Plesner",
        "Aczel Till",
        "Roger Wattenhofer"
      ],
      "tags": [
        "cs.LG",
        "cs.ET",
        "cs.PF"
      ],
      "matchedGroup": "hardware-design",
      "arxivId": "2602.07400",
      "pdfUrl": "https://arxiv.org/pdf/2602.07400v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07397",
      "source": "arxiv",
      "title": "Scout Before You Attend: Sketch-and-Walk Sparse Attention for Efficient LLM Inference",
      "description": "Self-attention dominates the computational and memory cost of long-context LLM inference across both prefill and decode phases. To address this challenge, we introduce Sketch&Walk Attention, a training-free sparse attention method that determines sparsity with lightweight sketches and deterministic walk. Sketch&Walk applies Hadamard sketching to get inexpensive approximations of attention scores, then aggregates these estimates across layers via a walk mechanism that captures attention influence beyond direct interactions between tokens. The accumulated walk scores are used to select top-k attention blocks, enabling dynamic sparsity with a single training-free algorithm that applies uniformly to both the prefill and decode phases, together with custom sparse attention kernels. Across a wide range of models and tasks, Sketch&Walk maintains near-lossless accuracy at 20% attention density and can slightly outperform dense attention in some settings, while achieving up to 6x inference speedup.",
      "url": "https://arxiv.org/abs/2602.07397",
      "date": "2026-02-07",
      "authors": [
        "Hoang Anh Duy Le",
        "Sahil Joshi",
        "Zeyu Yang",
        "Zhaozhuo Xu",
        "Anshumali Shrivastava"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.07397",
      "pdfUrl": "https://arxiv.org/pdf/2602.07397v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.07375",
      "source": "arxiv",
      "title": "Efficient Post-Training Pruning of Large Language Models with Statistical Correction",
      "description": "Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.",
      "url": "https://arxiv.org/abs/2602.07375",
      "date": "2026-02-07",
      "authors": [
        "Peiqi Yu",
        "Jinhao Wang",
        "Xinyi Sui",
        "Nam Ling",
        "Wei Wang",
        "Wei Jiang"
      ],
      "tags": [
        "cs.CL",
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.07375",
      "pdfUrl": "https://arxiv.org/pdf/2602.07375v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.07374",
      "source": "arxiv",
      "title": "TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling",
      "description": "Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.",
      "url": "https://arxiv.org/abs/2602.07374",
      "date": "2026-02-07",
      "authors": [
        "Nisharg Nargund",
        "Priyesh Shukla"
      ],
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.07374",
      "pdfUrl": "https://arxiv.org/pdf/2602.07374v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.07321",
      "source": "arxiv",
      "title": "Wireless Context Engineering for Efficient Mobile Agentic AI and Edge General Intelligence",
      "description": "Future wireless networks demand increasingly powerful intelligence to support sensing, communication, and autonomous decision-making. While scaling laws suggest improving performance by enlarging model capacity, practical edge deployments are fundamentally constrained by latency, energy, and memory, making unlimited model scaling infeasible. This creates a critical need to maximize the utility of limited inference-time inputs by filtering redundant observations and focusing on high-impact data. In large language models and generative artificial intelligence (AI), context engineering has emerged as a key paradigm to guide inference by selectively structuring and injecting task-relevant information. Inspired by this success, we extend context engineering to wireless systems, providing a systematic way to enhance edge AI performance without increasing model complexity. In dynamic environments, for example, beam prediction can benefit from augmenting instantaneous channel measurements with contextual cues such as user mobility trends or environment-aware propagation priors. We formally introduce wireless context engineering and propose a Wireless Context Communication Framework (WCCF) to adaptively orchestrate wireless context under inference-time constraints. This work provides researchers with a foundational perspective and practical design dimensions to manage the wireless context of wireless edge intelligence. An ISAC-enabled beam prediction case study illustrates the effectiveness of the proposed paradigm under constrained sensing budgets.",
      "url": "https://arxiv.org/abs/2602.07321",
      "date": "2026-02-07",
      "authors": [
        "Changyuan Zhao",
        "Jiacheng Wang",
        "Yunting Xu",
        "Geng Sun",
        "Dusit Niyato",
        "Zan Li",
        "Abbas Jamalipour",
        "Dong In Kim"
      ],
      "tags": [
        "eess.SP"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.07321",
      "pdfUrl": "https://arxiv.org/pdf/2602.07321v1",
      "primaryCategory": "eess.SP"
    },
    {
      "id": "arxiv-2602.07309",
      "source": "arxiv",
      "title": "Semantic Search At LinkedIn",
      "description": "Semantic search with large language models (LLMs) enables retrieval by meaning rather than keyword overlap, but scaling it requires major inference efficiency advances. We present LinkedIn's LLM-based semantic search framework for AI Job Search and AI People Search, combining an LLM relevance judge, embedding-based retrieval, and a compact Small Language Model trained via multi-teacher distillation to jointly optimize relevance and engagement. A prefill-oriented inference architecture co-designed with model pruning, context compression, and text-embedding hybrid interactions boosts ranking throughput by over 75x under a fixed latency constraint while preserving near-teacher-level NDCG, enabling one of the first production LLM-based ranking systems with efficiency comparable to traditional approaches and delivering significant gains in quality and user engagement.",
      "url": "https://arxiv.org/abs/2602.07309",
      "date": "2026-02-07",
      "authors": [
        "Fedor Borisyuk",
        "Sriram Vasudevan",
        "Muchen Wu",
        "Guoyao Li",
        "Benjamin Le",
        "Shaobo Zhang",
        "Qianqi Kay Shen",
        "Yuchin Juan",
        "Kayhan Behdin",
        "Liming Dong",
        "Kaixu Yang",
        "Shusen Jing",
        "Ravi Pothamsetty",
        "Rajat Arora",
        "Sophie Yanying Sheng",
        "Vitaly Abdrashitov",
        "Yang Zhao",
        "Lin Su",
        "Xiaoqing Wang",
        "Chujie Zheng",
        "Sarang Metkar",
        "Rupesh Gupta",
        "Igor Lapchuk",
        "David N. Racca",
        "Madhumitha Mohan",
        "Yanbo Li",
        "Haojun Li",
        "Saloni Gandhi",
        "Xueying Lu",
        "Chetan Bhole",
        "Ali Hooshmand",
        "Xin Yang",
        "Raghavan Muthuregunathan",
        "Jiajun Zhang",
        "Mathew Teoh",
        "Adam Coler",
        "Abhinav Gupta",
        "Xiaojing Ma",
        "Sundara Raman Ramachandran",
        "Morteza Ramezani",
        "Yubo Wang",
        "Lijuan Zhang",
        "Richard Li",
        "Jian Sheng",
        "Chanh Nguyen",
        "Yen-Chi Chen",
        "Chuanrui Zhu",
        "Claire Zhang",
        "Jiahao Xu",
        "Deepti Kulkarni",
        "Qing Lan",
        "Arvind Subramaniam",
        "Ata Fatahibaarzi",
        "Steven Shimizu",
        "Yanning Chen",
        "Zhipeng Wang",
        "Ran He",
        "Zhengze Zhou",
        "Qingquan Song",
        "Yun Dai",
        "Caleb Johnson",
        "Ping Liu",
        "Shaghayegh Gharghabi",
        "Gokulraj Mohanasundaram",
        "Juan Bottaro",
        "Santhosh Sachindran",
        "Qi Guo",
        "Yunxiang Ren",
        "Chengming Jiang",
        "Di Mo",
        "Luke Simon",
        "Jianqiang Shen",
        "Jingwei Wu",
        "Wenjing Zhang"
      ],
      "tags": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.07309",
      "pdfUrl": "https://arxiv.org/pdf/2602.07309v1",
      "primaryCategory": "cs.IR"
    },
    {
      "id": "arxiv-2602.07164",
      "source": "arxiv",
      "title": "Your Language Model Secretly Contains Personality Subnetworks",
      "description": "Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.",
      "url": "https://arxiv.org/abs/2602.07164",
      "date": "2026-02-06",
      "authors": [
        "Ruimeng Ye",
        "Zihan Wang",
        "Zinan Ling",
        "Yang Xiao",
        "Manling Li",
        "Xiaolong Ma",
        "Bo Hui"
      ],
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.07164",
      "pdfUrl": "https://arxiv.org/pdf/2602.07164v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.07144",
      "source": "arxiv",
      "title": "BONSAI: Bayesian Optimization with Natural Simplicity and Interpretability",
      "description": "Bayesian optimization (BO) is a popular technique for sample-efficient optimization of black-box functions. In many applications, the parameters being tuned come with a carefully engineered default configuration, and practitioners only want to deviate from this default when necessary. Standard BO, however, does not aim to minimize deviation from the default and, in practice, often pushes weakly relevant parameters to the boundary of the search space. This makes it difficult to distinguish between important and spurious changes and increases the burden of vetting recommendations when the optimization objective omits relevant operational considerations. We introduce BONSAI, a default-aware BO policy that prunes low-impact deviations from a default configuration while explicitly controlling the loss in acquisition value. BONSAI is compatible with a variety of acquisition functions, including expected improvement and upper confidence bound (GP-UCB). We theoretically bound the regret incurred by BONSAI, showing that, under certain conditions, it enjoys the same no-regret property as vanilla GP-UCB. Across many real-world applications, we empirically find that BONSAI substantially reduces the number of non-default parameters in recommended configurations while maintaining competitive optimization performance, with little effect on wall time.",
      "url": "https://arxiv.org/abs/2602.07144",
      "date": "2026-02-06",
      "authors": [
        "Samuel Daulton",
        "David Eriksson",
        "Maximilian Balandat",
        "Eytan Bakshy"
      ],
      "tags": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.07144",
      "pdfUrl": "https://arxiv.org/pdf/2602.07144v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.06879",
      "source": "arxiv",
      "title": "NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices",
      "description": "While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.",
      "url": "https://arxiv.org/abs/2602.06879",
      "date": "2026-02-06",
      "authors": [
        "Ruchika Chavhan",
        "Malcolm Chadwick",
        "Alberto Gil Couto Pimentel Ramos",
        "Luca Morreale",
        "Mehdi Noroozi",
        "Abhinav Mehrotra"
      ],
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06879",
      "pdfUrl": "https://arxiv.org/pdf/2602.06879v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.06850",
      "source": "arxiv",
      "title": "Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping",
      "description": "While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\\times$ inference speedup and a 5.1$\\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.",
      "url": "https://arxiv.org/abs/2602.06850",
      "date": "2026-02-06",
      "authors": [
        "Chao Zhou",
        "Tianyi Wei",
        "Yiling Chen",
        "Wenbo Zhou",
        "Nenghai Yu"
      ],
      "tags": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.06850",
      "pdfUrl": "https://arxiv.org/pdf/2602.06850v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.06822",
      "source": "arxiv",
      "title": "POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models",
      "description": "Large foundation models (LFMs) achieve strong performance through scaling, yet current structural pruning methods derive fixed pruning decisions during inference, overlooking sparsity patterns that emerge in the autoregressive token generation. In this paper, we propose POP (Partition-guided Online Pruning), an efficient online structural pruning framework that enables context-conditioned dynamic pruning with minimal computational overhead. POP partitions model channels into retained, candidate, and pruned regions, where prefilling defines a coarse pruning partition, and the decoding stage generates a fine-grained mask within the candidate region, avoiding full-channel re-evaluation. The coarse pruning partition preserves consistently important weights, while the fine-grained masking provides context-conditioned variation during decoding. Moreover, POP is a lightweight, plug-and-play method that requires no preprocessing, including offline calibration, retraining, or learning predictors. Extensive evaluations across diverse LFMs, including large language models (LLMs), mixture-of-experts models (MoEs), and vision-language models (VLMs), demonstrate that POP consistently delivers higher accuracy than existing pruning approaches while incurring smaller computational overhead and minimizing inference latency.",
      "url": "https://arxiv.org/abs/2602.06822",
      "date": "2026-02-06",
      "authors": [
        "Yi Chen",
        "Wonjin Shin",
        "Shuhong Liu",
        "Tho Mai",
        "Jeongmo Lee",
        "Chuanbo Hua",
        "Kun Wang",
        "Jun Liu",
        "Joo-Young Kim"
      ],
      "tags": [
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06822",
      "pdfUrl": "https://arxiv.org/pdf/2602.06822v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.06777",
      "source": "arxiv",
      "title": "Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs",
      "description": "This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.",
      "url": "https://arxiv.org/abs/2602.06777",
      "date": "2026-02-06",
      "authors": [
        "Yassine Chagna",
        "Antal Goldschmidt"
      ],
      "tags": [
        "cs.CR",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06777",
      "pdfUrl": "https://arxiv.org/pdf/2602.06777v1",
      "primaryCategory": "cs.CR"
    },
    {
      "id": "arxiv-2602.06694",
      "source": "arxiv",
      "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
      "description": "Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8$\\times$ in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
      "url": "https://arxiv.org/abs/2602.06694",
      "date": "2026-02-06",
      "authors": [
        "Hyochan Chong",
        "Dongkyu Kim",
        "Changdong Kim",
        "Minseop Choi"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06694",
      "pdfUrl": "https://arxiv.org/pdf/2602.06694v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.06675",
      "source": "arxiv",
      "title": "Pruning at Initialisation through the lens of Graphon Limit: Convergence, Expressivity, and Generalisation",
      "description": "Pruning at Initialisation methods discover sparse, trainable subnetworks before training, but their theoretical mechanisms remain elusive. Existing analyses are often limited to finite-width statistics, lacking a rigorous characterisation of the global sparsity patterns that emerge as networks grow large. In this work, we connect discrete pruning heuristics to graph limit theory via graphons, establishing the graphon limit of PaI masks. We introduce a Factorised Saliency Model that encompasses popular pruning criteria and prove that, under regularity conditions, the discrete masks generated by these algorithms converge to deterministic bipartite graphons. This limit framework establishes a novel topological taxonomy for sparse networks: while unstructured methods (e.g., Random, Magnitude) converge to homogeneous graphons representing uniform connectivity, data-driven methods (e.g., SNIP, GraSP) converge to heterogeneous graphons that encode implicit feature selection. Leveraging this continuous characterisation, we derive two fundamental theoretical results: (i) a Universal Approximation Theorem for sparse networks that depends only on the intrinsic dimension of active coordinate subspaces; and (ii) a Graphon-NTK generalisation bound demonstrating how the limit graphon modulates the kernel geometry to align with informative features. Our results transform the study of sparse neural networks from combinatorial graph problems into a rigorous framework of continuous operators, offering a new mechanism for analysing expressivity and generalisation in sparse neural networks.",
      "url": "https://arxiv.org/abs/2602.06675",
      "date": "2026-02-06",
      "authors": [
        "Hoang Pham",
        "The-Anh Ta",
        "Long Tran-Thanh"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06675",
      "pdfUrl": "https://arxiv.org/pdf/2602.06675v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.06592",
      "source": "arxiv",
      "title": "ProtoQuant: Quantization of Prototypical Parts For General and Fine-Grained Image Classification",
      "description": "Prototypical parts-based models offer a \"this looks like that\" paradigm for intrinsic interpretability, yet they typically struggle with ImageNet-scale generalization and often require computationally expensive backbone finetuning. Furthermore, existing methods frequently suffer from \"prototype drift,\" where learned prototypes lack tangible grounding in the training distribution and change their activation under small perturbations. We present ProtoQuant, a novel architecture that achieves prototype stability and grounded interpretability through latent vector quantization. By constraining prototypes to a discrete learned codebook within the latent space, we ensure they remain faithful representations of the training data without the need to update the backbone. This design allows ProtoQuant to function as an efficient, interpretable head that scales to large-scale datasets. We evaluate ProtoQuant on ImageNet and several fine-grained benchmarks (CUB-200, Cars-196). Our results demonstrate that ProtoQuant achieves competitive classification accuracy while generalizing to ImageNet and comparable interpretability metrics to other prototypical-parts-based methods.",
      "url": "https://arxiv.org/abs/2602.06592",
      "date": "2026-02-06",
      "authors": [
        "Miko≈Çaj Janusz",
        "Adam Wr√≥bel",
        "Bartosz Zieli≈Ñski",
        "Dawid Rymarczyk"
      ],
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06592",
      "pdfUrl": "https://arxiv.org/pdf/2602.06592v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.06550",
      "source": "arxiv",
      "title": "Dynamics-Aligned Shared Hypernetworks for Zero-Shot Actuator Inversion",
      "description": "Zero-shot generalization in contextual reinforcement learning remains a core challenge, particularly when the context is latent and must be inferred from data. A canonical failure mode is actuator inversion, where identical actions produce opposite physical effects under a latent binary context. We propose DMA*-SH, a framework where a single hypernetwork, trained solely via dynamics prediction, generates a small set of adapter weights shared across the dynamics model, policy, and action-value function. This shared modulation imparts an inductive bias matched to actuator inversion, while input/output normalization and random input masking stabilize context inference, promoting directionally concentrated representations. We provide theoretical support via an expressivity separation result for hypernetwork modulation, and a variance decomposition with policy-gradient variance bounds that formalize how within-mode compression improves learning under actuator inversion. For evaluation, we introduce the Actuator Inversion Benchmark (AIB), a suite of environments designed to isolate discontinuous context-to-dynamics interactions. On AIB's held-out actuator-inversion tasks, DMA*-SH achieves zero-shot generalization, outperforming domain randomization by 111.8% and surpassing a standard context-aware baseline by 16.1%.",
      "url": "https://arxiv.org/abs/2602.06550",
      "date": "2026-02-06",
      "authors": [
        "Jan Benad",
        "Pradeep Kr. Banerjee",
        "Frank R√∂der",
        "Nihat Ay",
        "Martin V. Butz",
        "Manfred Eppe"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.06550",
      "pdfUrl": "https://arxiv.org/pdf/2602.06550v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.06476",
      "source": "arxiv",
      "title": "Prism: Spectral Parameter Sharing for Multi-Agent Reinforcement Learning",
      "description": "Parameter sharing is a key strategy in multi-agent reinforcement learning (MARL) for improving scalability, yet conventional fully shared architectures often collapse into homogeneous behaviors. Recent methods introduce diversity through clustering, pruning, or masking, but typically compromise resource efficiency. We propose Prism, a parameter sharing framework that induces inter-agent diversity by representing shared networks in the spectral domain via singular value decomposition (SVD). All agents share the singular vector directions while learning distinct spectral masks on singular values. This mechanism encourages inter-agent diversity and preserves scalability. Extensive experiments on both homogeneous (LBF, SMACv2) and heterogeneous (MaMuJoCo) benchmarks show that Prism achieves competitive performance with superior resource efficiency.",
      "url": "https://arxiv.org/abs/2602.06476",
      "date": "2026-02-06",
      "authors": [
        "Kyungbeom Kim",
        "Seungwon Oh",
        "Kyung-Joong Kim"
      ],
      "tags": [
        "cs.MA",
        "cs.AI",
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06476",
      "pdfUrl": "https://arxiv.org/pdf/2602.06476v1",
      "primaryCategory": "cs.MA"
    },
    {
      "id": "arxiv-2602.06405",
      "source": "arxiv",
      "title": "A neuromorphic model of the insect visual system for natural image processing",
      "description": "Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.",
      "url": "https://arxiv.org/abs/2602.06405",
      "date": "2026-02-06",
      "authors": [
        "Adam D. Hines",
        "Karin Nordstr√∂m",
        "Andrew B. Barron"
      ],
      "tags": [
        "cs.CV",
        "cs.NE"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.06405",
      "pdfUrl": "https://arxiv.org/pdf/2602.06405v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.06358",
      "source": "arxiv",
      "title": "SHINE: A Scalable In-Context Hypernetwork for Mapping Context to LoRA in a Single Pass",
      "description": "We propose SHINE (Scalable Hyper In-context NEtwork), a scalable hypernetwork that can map diverse meaningful contexts into high-quality LoRA adapters for large language models (LLM). By reusing the frozen LLM's own parameters in an in-context hypernetwork design and introducing architectural innovations, SHINE overcomes key limitations of prior hypernetworks and achieves strong expressive power with a relatively small number of parameters. We introduce a pretraining and instruction fine-tuning pipeline, and train our hypernetwork to generate high quality LoRA adapters from diverse meaningful contexts in a single forward pass. It updates LLM parameters without any fine-tuning, and immediately enables complex question answering tasks related to the context without directly accessing the context, effectively transforming in-context knowledge to in-parameter knowledge in one pass. Our work achieves outstanding results on various tasks, greatly saves time, computation and memory costs compared to SFT-based LLM adaptation, and shows great potential for scaling. Our code is available at https://github.com/Yewei-Liu/SHINE",
      "url": "https://arxiv.org/abs/2602.06358",
      "date": "2026-02-06",
      "authors": [
        "Yewei Liu",
        "Xiyuan Wang",
        "Yansheng Mao",
        "Yoav Gelbery",
        "Haggai Maron",
        "Muhan Zhang"
      ],
      "tags": [
        "cs.CL",
        "cs.AI"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.06358",
      "pdfUrl": "https://arxiv.org/pdf/2602.06358v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.06300",
      "source": "arxiv",
      "title": "Accelerating Vision Transformers on Brain Processing Unit",
      "description": "With the advancement of deep learning technologies, specialized neural processing hardware such as Brain Processing Units (BPUs) have emerged as dedicated platforms for CNN acceleration, offering optimized INT8 computation capabilities for convolutional operations. Meanwhile, Vision Transformer (ViT) models, such as the Data-efficient Image Transformer (DeiT), have demonstrated superior performance and play increasingly crucial roles in computer vision tasks. However, due to the architectural mismatch between CNN-optimized hardware and Vision Transformer computation characteristics--namely, that linear layers in Transformers operate on three-dimensional data while BPU acceleration is designed for four-dimensional convolution operations-it is difficult or even impossible to leverage BPU's advantages when deploying Vision Transformers. To address this challenge, we propose a novel approach that restructures the Vision Transformer by replacing linear layers and layer normalization operations with carefully designed convolutional operators. This enables DeiT to fully utilize the acceleration capabilities of BPUs, while allowing the original weight parameters to be inherited by the restructured models without retraining or fine-tuning. To the best of our knowledge, this is the first successful deployment of Vision Transformers that fully leverages BPU classification datasets demonstrate the effectiveness of our approach. Specifically, the quantized DeiT-Base model achieves 80.4% accuracy on ImageNet, compared to the original 81.8%, while obtaining up to a 3.8* inference speedup. Our finetuned DeiT model on the flower classification dataset also achieves excellent performance, with only a 0.5% accuracy drop for the DeiT-Base model, further demonstrating the effectiveness of our method.",
      "url": "https://arxiv.org/abs/2602.06300",
      "date": "2026-02-06",
      "authors": [
        "Jinchi Tang",
        "Yan Guo"
      ],
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.06300",
      "pdfUrl": "https://arxiv.org/pdf/2602.06300v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.06283",
      "source": "arxiv",
      "title": "SOCKET: SOft Collison Kernel EsTimator for Sparse Attention",
      "description": "Exploiting sparsity during long-context inference is central to scaling large language models, as attention dominates the cost of autoregressive decoding. Sparse attention reduces this cost by restricting computation to a subset of tokens, but its effectiveness depends critically on efficient scoring and selection of relevant tokens at inference time. We revisit Locality-Sensitive Hashing (LSH) as a sparsification primitive and introduce SOCKET, a SOft Collision Kernel EsTimator that replaces hard bucket matches with probabilistic, similarity-aware aggregation. Our key insight is that hard LSH produces discrete collision signals and is therefore poorly suited for ranking. In contrast, soft LSH aggregates graded collision evidence across hash tables, preserving the stability of relative ordering among the true top-$k$ tokens. This transformation elevates LSH from a candidate-generation heuristic to a principled and mathematically grounded scoring kernel for sparse attention. Leveraging this property, SOCKET enables efficient token selection without ad-hoc voting mechanism, and matches or surpasses established sparse attention baselines across multiple long-context benchmarks using diverse set of models. With a custom CUDA kernel for scoring keys and a Flash Decode Triton backend for sparse attention, SOCKET achieves up to 1.5$\\times$ higher throughput than FlashAttention, making it an effective tool for long-context inference. Code is open-sourced at https://github.com/amarka8/SOCKET.",
      "url": "https://arxiv.org/abs/2602.06283",
      "date": "2026-02-06",
      "authors": [
        "Sahil Joshi",
        "Agniva Chowdhury",
        "Wyatt Bellinger",
        "Amar Kanakamedala",
        "Ekam Singh",
        "Hoang Anh Duy Le",
        "Aditya Desai",
        "Anshumali Shrivastava"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "optimization",
      "arxivId": "2602.06283",
      "pdfUrl": "https://arxiv.org/pdf/2602.06283v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.06252",
      "source": "arxiv",
      "title": "D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs",
      "description": "The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\\times$ lower latency, up to 3.8$\\times$ higher memory savings, and up to 3$\\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\\times$ lower total latency, up to 2.3$\\times$ higher total throughput, and up to 2.7$\\times$ higher total memory savings.",
      "url": "https://arxiv.org/abs/2602.06252",
      "date": "2026-02-05",
      "authors": [
        "Ahmed J. Abdelmaksoud",
        "Cristian Sestito",
        "Shiwei Wang",
        "Themis Prodromakis"
      ],
      "tags": [
        "cs.AR"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.06252",
      "pdfUrl": "https://arxiv.org/pdf/2602.06252v1",
      "primaryCategory": "cs.AR"
    },
    {
      "id": "arxiv-2602.06251",
      "source": "arxiv",
      "title": "ASMa: Asymmetric Spatio-temporal Masking for Skeleton Action Representation Learning",
      "description": "Self-supervised learning (SSL) has shown remarkable success in skeleton-based action recognition by leveraging data augmentations to learn meaningful representations. However, existing SSL methods rely on data augmentations that predominantly focus on masking high-motion frames and high-degree joints such as joints with degree 3 or 4. This results in biased and incomplete feature representations that struggle to generalize across varied motion patterns. To address this, we propose Asymmetric Spatio-temporal Masking (ASMa) for Skeleton Action Representation Learning, a novel combination of masking to learn a full spectrum of spatio-temporal dynamics inherent in human actions. ASMa employs two complementary masking strategies: one that selectively masks high-degree joints and low-motion, and another that masks low-degree joints and high-motion frames. These masking strategies ensure a more balanced and comprehensive skeleton representation learning. Furthermore, we introduce a learnable feature alignment module to effectively align the representations learned from both masked views. To facilitate deployment in resource-constrained settings and on low-resource devices, we compress the learned and aligned representation into a lightweight model using knowledge distillation. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our approach outperforms existing SSL methods with an average improvement of 2.7-4.4% in fine-tuning and up to 5.9% in transfer learning to noisy datasets and achieves competitive performance compared to fully supervised baselines. Our distilled model achieves 91.4% parameter reduction and 3x faster inference on edge devices while maintaining competitive accuracy, enabling practical deployment in resource-constrained scenarios.",
      "url": "https://arxiv.org/abs/2602.06251",
      "date": "2026-02-05",
      "authors": [
        "Aman Anand",
        "Amir Eskandari",
        "Elyas Rahsno",
        "Farhana Zulkernine"
      ],
      "tags": [
        "cs.CV",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06251",
      "pdfUrl": "https://arxiv.org/pdf/2602.06251v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.06183",
      "source": "arxiv",
      "title": "To 2:4 Sparsity and Beyond: Neuron-level Activation Function to Accelerate LLM Pre-Training",
      "description": "Trainings of Large Language Models are generally bottlenecked by matrix multiplications. In the Transformer architecture, a large portion of these operations happens in the Feed Forward Network (FFN), and this portion increases for larger models, up to 50% of the total pretraining floating point operations. We show that we can leverage hardware-accelerated sparsity to accelerate all matrix multiplications in the FFN, with 2:4 sparsity for weights and v:n:m (Venom) sparsity for activations. Our recipe relies on sparse training steps to accelerate a large part of the pretraining, associated with regular dense training steps towards the end. Overall, models trained with this approach exhibit the same performance on our quality benchmarks, and can speed up training end-to-end by 1.4 to 1.7x. This approach is applicable to all NVIDIA GPUs starting with the A100 generation, and is orthogonal to common optimization techniques, such as, quantization, and can also be applied to mixture-of-experts model architectures.",
      "url": "https://arxiv.org/abs/2602.06183",
      "date": "2026-02-05",
      "authors": [
        "Meghana Madhyastha",
        "Daniel Haziza",
        "Jesse Cai",
        "Newsha Ardalani",
        "Zhiqi Bu",
        "Carole-Jean Wu"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06183",
      "pdfUrl": "https://arxiv.org/pdf/2602.06183v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.06138",
      "source": "arxiv",
      "title": "Flow Matching for Offline Reinforcement Learning with Discrete Actions",
      "description": "Generative policies based on diffusion models and flow matching have shown strong promise for offline reinforcement learning (RL), but their applicability remains largely confined to continuous action spaces. To address a broader range of offline RL settings, we extend flow matching to a general framework that supports discrete action spaces with multiple objectives. Specifically, we replace continuous flows with continuous-time Markov chains, trained using a Q-weighted flow matching objective. We then extend our design to multi-agent settings, mitigating the exponential growth of joint action spaces via a factorized conditional path. We theoretically show that, under idealized conditions, optimizing this objective recovers the optimal policy. Extensive experiments further demonstrate that our method performs robustly in practical scenarios, including high-dimensional control, multi-modal decision-making, and dynamically changing preferences over multiple objectives. Our discrete framework can also be applied to continuous-control problems through action quantization, providing a flexible trade-off between representational complexity and performance.",
      "url": "https://arxiv.org/abs/2602.06138",
      "date": "2026-02-05",
      "authors": [
        "Fairoz Nower Khan",
        "Nabuat Zaman Nahim",
        "Ruiquan Huang",
        "Haibo Yang",
        "Peizhong Ju"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06138",
      "pdfUrl": "https://arxiv.org/pdf/2602.06138v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.06127",
      "source": "arxiv",
      "title": "Compressing LLMs with MoP: Mixture of Pruners",
      "description": "The high computational demands of Large Language Models (LLMs) motivate methods that reduce parameter count and accelerate inference. In response, model pruning emerges as an effective strategy, yet current methods typically focus on a single dimension-depth or width. We introduce MoP (Mixture of Pruners), an iterative framework that unifies these dimensions. At each iteration, MoP generates two branches-pruning in depth versus pruning in width-and selects a candidate to advance the path. On LLaMA-2 and LLaMA-3, MoP advances the frontier of structured pruning, exceeding the accuracy of competing methods across a broad set of compression regimes. It also consistently outperforms depth-only and width-only pruning. Furthermore, MoP translates structural pruning into real speedup, reducing end-to-end latency by 39% at 40% compression. Finally, extending MoP to the vision-language model LLaVA-1.5, we notably improve computational efficiency and demonstrate that text-only recovery fine-tuning can restore performance even on visual tasks.",
      "url": "https://arxiv.org/abs/2602.06127",
      "date": "2026-02-05",
      "authors": [
        "Bruno Lopes Yamamoto",
        "Lucas Lauton de Alcantara",
        "Victor Zacarias",
        "Leandro Giusti Mugnaini",
        "Keith Ando Ogawa",
        "Lucas Pellicer",
        "Rosimeire Pereira Costa",
        "Edson Bollis",
        "Anna Helena Reali Costa",
        "Artur Jordao"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06127",
      "pdfUrl": "https://arxiv.org/pdf/2602.06127v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.05902",
      "source": "arxiv",
      "title": "Regularized Calibration with Successive Rounding for Post-Training Quantization",
      "description": "Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.",
      "url": "https://arxiv.org/abs/2602.05902",
      "date": "2026-02-05",
      "authors": [
        "Seohyeon Cha",
        "Huancheng Chen",
        "Dongjun Kim",
        "Haoran Zhang",
        "Kevin Chan",
        "Gustavo de Veciana",
        "Haris Vikalo"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.05902",
      "pdfUrl": "https://arxiv.org/pdf/2602.05902v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.05885",
      "source": "arxiv",
      "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
      "description": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.",
      "url": "https://arxiv.org/abs/2602.05885",
      "date": "2026-02-05",
      "authors": [
        "Wei Liu",
        "Jiawei Xu",
        "Yingru Li",
        "Longtao Zheng",
        "Tianjian Li",
        "Qian Liu",
        "Junxian He"
      ],
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "matchedGroup": "frameworks",
      "arxivId": "2602.05885",
      "pdfUrl": "https://arxiv.org/pdf/2602.05885v2",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.05790",
      "source": "arxiv",
      "title": "Price of universality in vector quantization is at most 0.11 bit",
      "description": "Fast computation of a matrix product $W^\\top X$ is a workhorse of modern LLMs. To make their deployment more efficient, a popular approach is that of using a low-precision approximation $\\widehat W$ in place of true $W$ (\"weight-only quantization''). Information theory demonstrates that an optimal algorithm for reducing precision of $W$ depends on the (second order) statistics of $X$ and requires a careful alignment of vector quantization codebook with PCA directions of $X$ (a process known as \"waterfilling allocation''). Dependence of the codebook on statistics of $X$, however, is highly impractical. This paper proves that there exist a universal codebook that is simultaneously near-optimal for all possible statistics of $X$, in the sense of being at least as good as an $X$-adapted waterfilling codebook with rate reduced by 0.11 bit per dimension. Such universal codebook would be an ideal candidate for the low-precision storage format, a topic of active modern research, but alas the existence proof is non-constructive. Equivalently, our result shows existence of a net in $\\mathbb{R}^n$ that is a nearly-optimal covering of a sphere simultaneously with respect to all Hilbert norms.",
      "url": "https://arxiv.org/abs/2602.05790",
      "date": "2026-02-05",
      "authors": [
        "Alina Harbuzova",
        "Or Ordentlich",
        "Yury Polyanskiy"
      ],
      "tags": [
        "cs.IT",
        "cs.LG",
        "stat.ML"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.05790",
      "pdfUrl": "https://arxiv.org/pdf/2602.05790v1",
      "primaryCategory": "cs.IT"
    },
    {
      "id": "arxiv-2602.05743",
      "source": "arxiv",
      "title": "Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction",
      "description": "FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.",
      "url": "https://arxiv.org/abs/2602.05743",
      "date": "2026-02-05",
      "authors": [
        "Liang Zhao",
        "Kunming Shao",
        "Zhipeng Liao",
        "Xijie Huang",
        "Tim Kwang-Ting Cheng",
        "Chi-Ying Tsui",
        "Yi Zou"
      ],
      "tags": [
        "cs.AR"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.05743",
      "pdfUrl": "https://arxiv.org/pdf/2602.05743v1",
      "primaryCategory": "cs.AR"
    },
    {
      "id": "arxiv-2602.05737",
      "source": "arxiv",
      "title": "Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing",
      "description": "In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.",
      "url": "https://arxiv.org/abs/2602.05737",
      "date": "2026-02-05",
      "authors": [
        "Luca Ciampi",
        "Ludovico Iannello",
        "Fabrizio Tonelli",
        "Gabriele Lagani",
        "Angelo Di Garbo",
        "Federico Cremisi",
        "Giuseppe Amato"
      ],
      "tags": [
        "cs.CV",
        "cs.NE"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.05737",
      "pdfUrl": "https://arxiv.org/pdf/2602.05737v1",
      "primaryCategory": "cs.CV"
    },
    {
      "id": "arxiv-2602.05735",
      "source": "arxiv",
      "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
      "description": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.",
      "url": "https://arxiv.org/abs/2602.05735",
      "date": "2026-02-05",
      "authors": [
        "Lixuan Guo",
        "Yifei Wang",
        "Tiansheng Wen",
        "Yifan Wang",
        "Aosong Feng",
        "Bo Chen",
        "Stefanie Jegelka",
        "Chenyu You"
      ],
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.IR",
        "cs.IT"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.05735",
      "pdfUrl": "https://arxiv.org/pdf/2602.05735v2",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.05717",
      "source": "arxiv",
      "title": "Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification",
      "description": "Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.",
      "url": "https://arxiv.org/abs/2602.05717",
      "date": "2026-02-05",
      "authors": [
        "Tianyi Wang",
        "Long Li",
        "Hongcan Guo",
        "Yibiao Chen",
        "Yixia Li",
        "Yong Wang",
        "Yun Chen",
        "Guanhua Chen"
      ],
      "tags": [
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.05717",
      "pdfUrl": "https://arxiv.org/pdf/2602.05717v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.05695",
      "source": "arxiv",
      "title": "Determining Energy Efficiency Sweet Spots in Production LLM Inference",
      "description": "Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"Sweet Spots\" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.",
      "url": "https://arxiv.org/abs/2602.05695",
      "date": "2026-02-05",
      "authors": [
        "Hiari Pizzini Cavagna",
        "Andrea Proia",
        "Giacomo Madella",
        "Giovanni B. Esposito",
        "Francesco Antici",
        "Daniele Cesarini",
        "Zeynep Kiziltan",
        "Andrea Bartolini"
      ],
      "tags": [
        "cs.AI",
        "cs.PF"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.05695",
      "pdfUrl": "https://arxiv.org/pdf/2602.05695v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.05683",
      "source": "arxiv",
      "title": "From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking",
      "description": "Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.",
      "url": "https://arxiv.org/abs/2602.05683",
      "date": "2026-02-05",
      "authors": [
        "Chuwei Wang",
        "Eduardo Sebasti√°n",
        "Amanda Prorok",
        "Anastasia Bizyaeva"
      ],
      "tags": [
        "cs.RO",
        "eess.SY"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.05683",
      "pdfUrl": "https://arxiv.org/pdf/2602.05683v1",
      "primaryCategory": "cs.RO"
    },
    {
      "id": "arxiv-2602.05605",
      "source": "arxiv",
      "title": "Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers",
      "description": "Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.",
      "url": "https://arxiv.org/abs/2602.05605",
      "date": "2026-02-05",
      "authors": [
        "Jiaji Zhang",
        "Hailiang Zhao",
        "Guoxuan Zhu",
        "Ruichao Sun",
        "Jiaju Wu",
        "Xinkui Zhao",
        "Hanlin Tang",
        "Weiyi Lu",
        "Kan Liu",
        "Tao Lan",
        "Lin Qu",
        "Shuiguang Deng"
      ],
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.05605",
      "pdfUrl": "https://arxiv.org/pdf/2602.05605v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.05595",
      "source": "arxiv",
      "title": "Adaptive controllable architecture of analog Ising machine",
      "description": "As a quantum-inspired, non-traditional analog solver architecture, the analog Ising machine (AIM) has emerged as a distinctive computational paradigm to address the rapidly growing demand for computational power. However, the mathematical understanding of its principles, as well as the optimization of its solution speed and accuracy, remain unclear. In this work, we for the first time systematically discuss multiple implementations of AIM and establish a unified mathematical formulation. On this basis, by treating the binarization constraint of AIM (such as injection locking) as a Lagrange multiplier in optimization theory and combining it with a Lyapunov analysis from dynamical systems theory, an analytical framework for evaluating solution speed and accuracy is constructed, and further demonstrate that conventional AIMs possess a theoretical performance upper bound. Subsequently, by elevating the binarization constraint to a control variable, we propose the controllable analog Ising machine (CAIM), which integrates control Lyapunov functions and momentum-based optimization algorithms to realize adaptive sampling-feedback control, thereby surpassing the performance limits of conventional AIMs. In a proof-of-concept CAIM demonstration implemented using an FPGA-controlled LC-oscillator Ising machine, CAIM achieves a twofold speedup and a 7\\% improvement in accuracy over AIM on a 50-node all-to-all weighted MaxCut problem, validating both the effectiveness and interpretability of the proposed theoretical framework.",
      "url": "https://arxiv.org/abs/2602.05595",
      "date": "2026-02-05",
      "authors": [
        "Langyu Li",
        "Ruoyu Wu",
        "Yong Wang",
        "Guofeng Zhang",
        "Jinhu L√º",
        "Qing Gao",
        "Yu Pan"
      ],
      "tags": [
        "quant-ph",
        "cs.ET"
      ],
      "matchedGroup": "hardware-design",
      "arxivId": "2602.05595",
      "pdfUrl": "https://arxiv.org/pdf/2602.05595v1",
      "primaryCategory": "quant-ph"
    },
    {
      "id": "arxiv-2602.05499",
      "source": "arxiv",
      "title": "SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration",
      "description": "Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.",
      "url": "https://arxiv.org/abs/2602.05499",
      "date": "2026-02-05",
      "authors": [
        "Hanyu Wei",
        "Zunhai Su",
        "Peng Lu",
        "Chao Li",
        "Spandan Tiwari",
        "Ashish Sirasao",
        "Yuhan Dong"
      ],
      "tags": [
        "cs.AI"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.05499",
      "pdfUrl": "https://arxiv.org/pdf/2602.05499v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.05466",
      "source": "arxiv",
      "title": "Optimization is Not Enough: Why Problem Formulation Deserves Equal Attention",
      "description": "Black-box optimization is increasingly used in engineering design problems where simulation-based evaluations are costly and gradients are unavailable. In this context, the optimization community has largely analyzed algorithm performance in context-free setups, while not enough attention has been devoted to how problem formulation and domain knowledge may affect the optimization outcomes. We address this gap through a case study in the topology optimization of laminated composite structures, formulated as a black-box optimization problem. Specifically, we consider the design of a cantilever beam under a volume constraint, intending to minimize compliance while optimizing both the structural topology and fiber orientations. To assess the impact of problem formulation, we explicitly separate topology and material design variables and compare two strategies: a concurrent approach that optimizes all variables simultaneously without leveraging physical insight, and a sequential approach that optimizes variables of the same nature in stages. Our results show that context-agnostic strategies consistently lead to suboptimal or non-physical designs. In contrast, the sequential strategy yields better-performing and more interpretable solutions. These findings underscore the value of incorporating, when available, domain knowledge into the optimization process and motivate the development of new black-box benchmarks that reward physically informed and context-aware optimization strategies.",
      "url": "https://arxiv.org/abs/2602.05466",
      "date": "2026-02-05",
      "authors": [
        "Iv√°n Olarte Rodr√≠guez",
        "Gokhan Serhat",
        "Mariusz Bujny",
        "Fabian Duddeck",
        "Thomas B√§ck",
        "Elena Raponi"
      ],
      "tags": [
        "cs.NE",
        "cs.CE"
      ],
      "matchedGroup": "synthesis-pnr",
      "arxivId": "2602.05466",
      "pdfUrl": "https://arxiv.org/pdf/2602.05466v1",
      "primaryCategory": "cs.NE"
    },
    {
      "id": "arxiv-2602.06093",
      "source": "arxiv",
      "title": "NanoNet: Parameter-Efficient Learning with Label-Scarce Supervision for Lightweight Text Mining Model",
      "description": "The lightweight semi-supervised learning (LSL) strategy provides an effective approach of conserving labeled samples and minimizing model inference costs. Prior research has effectively applied knowledge transfer learning and co-training regularization from large to small models in LSL. However, such training strategies are computationally intensive and prone to local optima, thereby increasing the difficulty of finding the optimal solution. This has prompted us to investigate the feasibility of integrating three low-cost scenarios for text mining tasks: limited labeled supervision, lightweight fine-tuning, and rapid-inference small models. We propose NanoNet, a novel framework for lightweight text mining that implements parameter-efficient learning with limited supervision. It employs online knowledge distillation to generate multiple small models and enhances their performance through mutual learning regularization. The entire process leverages parameter-efficient learning, reducing training costs and minimizing supervision requirements, ultimately yielding a lightweight model for downstream inference.",
      "url": "https://arxiv.org/abs/2602.06093",
      "date": "2026-02-05",
      "authors": [
        "Qianren Mao",
        "Yashuo Luo",
        "Ziqi Qin",
        "Junnan Liu",
        "Weifeng Jiang",
        "Zhijun Chen",
        "Zhuoran Li",
        "Likang Xiao",
        "Chuou Xu",
        "Qili Zhang",
        "Hanwen Hao",
        "Jingzheng Li",
        "Chunghua Lin",
        "Jianxin Li",
        "Philip S. Yu"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.06093",
      "pdfUrl": "https://arxiv.org/pdf/2602.06093v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.05367",
      "source": "arxiv",
      "title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
      "description": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\\times$ inference speed-up over full-precision models on an RTX 4090.",
      "url": "https://arxiv.org/abs/2602.05367",
      "date": "2026-02-05",
      "authors": [
        "Youngcheon You",
        "Banseok Lee",
        "Minseop Choi",
        "Seonyoung Kim",
        "Hyochan Chong",
        "Changdong Kim",
        "Youngmin Kim",
        "Dongkyu Kim"
      ],
      "tags": [
        "cs.AI"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.05367",
      "pdfUrl": "https://arxiv.org/pdf/2602.05367v1",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.05353",
      "source": "arxiv",
      "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
      "description": "Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.",
      "url": "https://arxiv.org/abs/2602.05353",
      "date": "2026-02-05",
      "authors": [
        "Ruijie Shi",
        "Houbin Zhang",
        "Yuecheng Han",
        "Yuheng Wang",
        "Jingru Fan",
        "Runde Yang",
        "Yufan Dang",
        "Huatao Li",
        "Dewen Liu",
        "Yuan Cheng",
        "Chen Qian"
      ],
      "tags": [
        "cs.AI",
        "cs.CL"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.05353",
      "pdfUrl": "https://arxiv.org/pdf/2602.05353v2",
      "primaryCategory": "cs.AI"
    },
    {
      "id": "arxiv-2602.05342",
      "source": "arxiv",
      "title": "Joint Optimization of Latency and Accuracy for Split Federated Learning in User-Centric Cell-Free MIMO Networks",
      "description": "This paper proposes a user-centric split federated learning (UCSFL) framework for user-centric cell-free multiple-input multiple-output (CF-MIMO) networks to support split federated learning (SFL). In the proposed UCSFL framework, users deploy split sub-models locally, while complete models are maintained and updated at access point (AP)-side distributed processing units (DPUs), followed by a two-level aggregation procedure across DPUs and the central processing unit (CPU). Under standard machine learning (ML) assumptions, we provide a theoretical convergence analysis for UCSFL, which reveals that the AP-cluster size is a key factor influencing model training accuracy. Motivated by this result, we introduce a new performance metric, termed the latency-to-accuracy ratio, defined as the ratio of a user's per-iteration training latency to the weighted size of its AP cluster. Based on this metric, we formulate a joint optimization problem to minimize the maximum latency-to-accuracy ratio by jointly optimizing uplink power control, downlink beamforming, model splitting, and AP clustering. The resulting problem is decomposed into two sub-problems operating on different time scales, for which dedicated algorithms are developed to handle the short-term and long-term optimizations, respectively. Simulation results verify the convergence of the proposed algorithms and demonstrate that UCSFL effectively reduces the latency-to-accuracy ratio of the VGG16 model compared with baseline schemes. Moreover, the proposed framework adaptively adjusts splitting and clustering strategies in response to varying communication and computation resources. An MNIST-based handwritten digit classification example further shows that UCSFL significantly accelerates the convergence of the VGG16 model.",
      "url": "https://arxiv.org/abs/2602.05342",
      "date": "2026-02-05",
      "authors": [
        "Zitong Wang",
        "Cheng Zhang",
        "Wen Wang",
        "Shuigen Yang",
        "Haiming Wang",
        "Yongming Huang"
      ],
      "tags": [
        "eess.SP"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.05342",
      "pdfUrl": "https://arxiv.org/pdf/2602.05342v1",
      "primaryCategory": "eess.SP"
    },
    {
      "id": "arxiv-2602.05292",
      "source": "arxiv",
      "title": "ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices",
      "description": "Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments. We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.",
      "url": "https://arxiv.org/abs/2602.05292",
      "date": "2026-02-05",
      "authors": [
        "Haoyu Bai",
        "Muhammed Tawfiqul Islam",
        "Minxian Xu",
        "Rajkumar Buyya"
      ],
      "tags": [
        "cs.DC"
      ],
      "matchedGroup": "other",
      "arxivId": "2602.05292",
      "pdfUrl": "https://arxiv.org/pdf/2602.05292v1",
      "primaryCategory": "cs.DC"
    },
    {
      "id": "arxiv-2602.05269",
      "source": "arxiv",
      "title": "Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction",
      "description": "The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the \"Memory Wall\" -- a hardware limitation where memory bandwidth, not compute, becomes the bottleneck. Recent 1.58-bit quantization techniques (e.g., BitNet b1.58) dramatically reduce memory footprint but typically incur a perplexity degradation of 20-25% compared to FP16 baselines. In this work, we introduce Hybrid Gated Flow (HGF), a dual-stream architecture that couples a 1.58-bit ternary backbone with a learnable, low-rank FP16 correction path controlled by adaptive gates. Through extensive experiments on the TinyStories dataset across two training regimes (2500 and 3500 steps), we demonstrate that HGF 5.4 achieves a validation loss of 0.9306 compared to BitNet's 1.0294, recovering approximately 55% of the quality gap between pure ternary quantization and the FP16 baseline (0.8490). This recovery is achieved with only ~12-15% memory overhead beyond the ternary backbone. Furthermore, we provide empirical evidence for an emergent phenomenon: quantization as structural regularization. While a full-precision differential attention baseline (Diff_Only) exhibited training instability with validation loss exceeding 1.68, the ternary-anchored HGF maintained robust convergence throughout training. Finally, we report preliminary results extending this architecture to 1.2B and 3B parameter models trained on SlimPajama and FineWeb-Edu. These larger-scale experiments confirm that the architectural stability and quality recovery observed in small-scale proxies scale linearly to production-grade language modeling regimes.",
      "url": "https://arxiv.org/abs/2602.05269",
      "date": "2026-02-05",
      "authors": [
        "David Alejandro Trejo Pizzo"
      ],
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.05269",
      "pdfUrl": "https://arxiv.org/pdf/2602.05269v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.05243",
      "source": "arxiv",
      "title": "CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers",
      "description": "Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \\textbf{CORP}, a closed-form one-shot structured pruning framework for Vision Transformers. CORP removes entire MLP hidden dimensions and attention substructures without labels, gradients, or fine-tuning. It operates under strict post-training constraints using only a small unlabeled calibration set. CORP formulates structured pruning as a representation recovery problem. It models removed activations and attention logits as affine functions of retained components and derives closed-form ridge regression solutions that fold compensation into model weights. This minimizes expected representation error under the calibration distribution. Experiments on ImageNet with DeiT models show strong redundancy in MLP and attention representations. Without compensation, one-shot structured pruning causes severe accuracy degradation. With CORP, models preserve accuracy under aggressive sparsity. On DeiT-Huge, CORP retains 82.8\\% Top-1 accuracy after pruning 50\\% of both MLP and attention structures. CORP completes pruning in under 20 minutes on a single GPU and delivers substantial real-world efficiency gains.",
      "url": "https://arxiv.org/abs/2602.05243",
      "date": "2026-02-05",
      "authors": [
        "Boxiang Zhang",
        "Baijian Yang"
      ],
      "tags": [
        "cs.LG",
        "cs.CV"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.05243",
      "pdfUrl": "https://arxiv.org/pdf/2602.05243v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.05227",
      "source": "arxiv",
      "title": "Radon--Wasserstein Gradient Flows for Interacting-Particle Sampling in High Dimensions",
      "description": "Gradient flows of the Kullback--Leibler (KL) divergence, such as the Fokker--Planck equation and Stein Variational Gradient Descent, evolve a distribution toward a target density known only up to a normalizing constant. We introduce new gradient flows of the KL divergence with a remarkable combination of properties: they admit accurate interacting-particle approximations in high dimensions, and the per-step cost scales linearly in both the number of particles and the dimension. These gradient flows are based on new transportation-based Riemannian geometries on the space of probability measures: the Radon--Wasserstein geometry and the related Regularized Radon--Wasserstein (RRW) geometry. We define these geometries using the Radon transform so that the gradient-flow velocities depend only on one-dimensional projections. This yields interacting-particle-based algorithms whose per-step cost follows from efficient Fast Fourier Transform-based evaluation of the required 1D convolutions. We additionally provide numerical experiments that study the performance of the proposed algorithms and compare convergence behavior and quantization. Finally, we prove some theoretical results including well-posedness of the flows and long-time convergence guarantees for the RRW flow.",
      "url": "https://arxiv.org/abs/2602.05227",
      "date": "2026-02-05",
      "authors": [
        "Elias Hess-Childs",
        "Dejan Slepƒçev",
        "Lantian Xu"
      ],
      "tags": [
        "stat.ML",
        "cs.LG",
        "math.AP",
        "math.NA",
        "stat.ME"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.05227",
      "pdfUrl": "https://arxiv.org/pdf/2602.05227v2",
      "primaryCategory": "stat.ML"
    },
    {
      "id": "arxiv-2602.06085",
      "source": "arxiv",
      "title": "LAAFD: LLM-based Agents for Accelerated FPGA Design",
      "description": "FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.",
      "url": "https://arxiv.org/abs/2602.06085",
      "date": "2026-02-04",
      "authors": [
        "Maxim Moraru",
        "Kamalavasan Kamalakkannan",
        "Jered Dominguez-Trujillo",
        "Patrick Diehl",
        "Atanu Barai",
        "Julien Loiseau",
        "Zachary Kent Baker",
        "Howard Pritchard",
        "Galen M Shipman"
      ],
      "tags": [
        "cs.DC"
      ],
      "matchedGroup": "hardware-design",
      "arxivId": "2602.06085",
      "pdfUrl": "https://arxiv.org/pdf/2602.06085v1",
      "primaryCategory": "cs.DC"
    },
    {
      "id": "arxiv-2602.05068",
      "source": "arxiv",
      "title": "E-Globe: Scalable $Œµ$-Global Verification of Neural Networks via Tight Upper Bounds and Pattern-Aware Branching",
      "description": "Neural networks achieve strong empirical performance, but robustness concerns still hinder deployment in safety-critical applications. Formal verification provides robustness guarantees, but current methods face a scalability-completeness trade-off. We propose a hybrid verifier in a branch-and-bound (BaB) framework that efficiently tightens both upper and lower bounds until an $Œµ-$global optimum is reached or early stop is triggered. The key is an exact nonlinear program with complementarity constraints (NLP-CC) for upper bounding that preserves the ReLU input-output graph, so any feasible solution yields a valid counterexample and enables rapid pruning of unsafe subproblems. We further accelerate verification with (i) warm-started NLP solves requiring minimal constraint-matrix updates and (ii) pattern-aligned strong branching that prioritizes splits most effective at tightening relaxations. We also provide conditions under which NLP-CC upper bounds are tight. Experiments on MNIST and CIFAR-10 show markedly tighter upper bounds than PGD across perturbation radii spanning up to three orders of magnitude, fast per-node solves in practice, and substantial end-to-end speedups over MIP-based verification, amplified by warm-starting, GPU batching, and pattern-aligned branching.",
      "url": "https://arxiv.org/abs/2602.05068",
      "date": "2026-02-04",
      "authors": [
        "Wenting Li",
        "Saif R. Kazi",
        "Russell Bent",
        "Duo Zhou",
        "Huan Zhang"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.05068",
      "pdfUrl": "https://arxiv.org/pdf/2602.05068v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.04884",
      "source": "arxiv",
      "title": "Reinforced Attention Learning",
      "description": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance. We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.",
      "url": "https://arxiv.org/abs/2602.04884",
      "date": "2026-02-04",
      "authors": [
        "Bangzheng Li",
        "Jianmo Ni",
        "Chen Qu",
        "Ian Miao",
        "Liu Yang",
        "Xingyu Fu",
        "Muhao Chen",
        "Derek Zhiyuan Cheng"
      ],
      "tags": [
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.04884",
      "pdfUrl": "https://arxiv.org/pdf/2602.04884v1",
      "primaryCategory": "cs.CL"
    },
    {
      "id": "arxiv-2602.04881",
      "source": "arxiv",
      "title": "Contrastive Continual Learning for Model Adaptability in Internet of Things",
      "description": "Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \\emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.",
      "url": "https://arxiv.org/abs/2602.04881",
      "date": "2026-02-04",
      "authors": [
        "Ajesh Koyatan Chathoth"
      ],
      "tags": [
        "cs.LG",
        "cs.AI"
      ],
      "matchedGroup": "edge-ai",
      "arxivId": "2602.04881",
      "pdfUrl": "https://arxiv.org/pdf/2602.04881v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.04852",
      "source": "arxiv",
      "title": "The Key to State Reduction in Linear Attention: A Rank-based Perspective",
      "description": "Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.",
      "url": "https://arxiv.org/abs/2602.04852",
      "date": "2026-02-04",
      "authors": [
        "Philipp Nazari",
        "T. Konstantin Rusch"
      ],
      "tags": [
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.04852",
      "pdfUrl": "https://arxiv.org/pdf/2602.04852v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.04832",
      "source": "arxiv",
      "title": "It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task",
      "description": "Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.",
      "url": "https://arxiv.org/abs/2602.04832",
      "date": "2026-02-04",
      "authors": [
        "Hannah Pinson"
      ],
      "tags": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.NE"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.04832",
      "pdfUrl": "https://arxiv.org/pdf/2602.04832v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.04831",
      "source": "arxiv",
      "title": "Review of Superconducting Qubit Devices and Their Large-Scale Integration",
      "description": "The superconducting qubit quantum computer is one of the most promising quantum computing architectures for large-scale integration due to its maturity and close proximity to the well-established semiconductor manufacturing infrastructure. From an education perspective, it also bridges classical microwave electronics and quantum electrodynamics. In this paper, we will review the basics of quantum computers, superconductivity, and Josephson junctions. We then introduce important technologies and concepts related to DiVincenzo's criteria, which are the necessary conditions for the superconducting qubits to work as a useful quantum computer. Firstly, we will discuss various types of superconducting qubits formed with Josephson junctions, from which we will understand the trade-off across multiple design parameters, including their noise immunity. Secondly, we will discuss different schemes to achieve entanglement gate operations, which are a major bottleneck in achieving more efficient fault-tolerant quantum computing. Thirdly, we will review readout engineering, including the implementations of the Purcell filters and quantum-limited amplifiers. Finally, we will discuss the nature and review the studies of two-level system defects, which are currently the limiting factor of qubit coherence time. DiVincenzo's criteria are only the necessary conditions for a technology to be eligible for quantum computing. To have a useful quantum computer, large-scale integration is required. We will review proposals and developments for the large-scale integration of superconducting qubit devices. By comparing with the application of electronic design automation (EDA) in semiconductors, we will also review the use of EDA in superconducting qubit quantum computer design, which is necessary for its large-scale integration.",
      "url": "https://arxiv.org/abs/2602.04831",
      "date": "2026-02-04",
      "authors": [
        "Hiu Yung Wong"
      ],
      "tags": [
        "quant-ph",
        "eess.SY"
      ],
      "matchedGroup": "hardware-design",
      "arxivId": "2602.04831",
      "pdfUrl": "https://arxiv.org/pdf/2602.04831v1",
      "primaryCategory": "quant-ph"
    },
    {
      "id": "arxiv-2602.04803",
      "source": "arxiv",
      "title": "Safe-NEureka: a Hybrid Modular Redundant DNN Accelerator for On-board Satellite AI Processing",
      "description": "Low Earth Orbit (LEO) constellations are revolutionizing the space sector, with on-board Artificial Intelligence (AI) becoming pivotal for next-generation satellites. AI acceleration is essential for safety-critical functions such as autonomous Guidance, Navigation, and Control (GNC), where errors cannot be tolerated, and performance-critical processing of high-bandwidth sensor data, where occasional errors are tolerable. Consequently, AI accelerators for satellites must combine robust protection against radiation-induced faults with high throughput. This paper presents Safe-NEureka, a Hybrid Modular Redundant Deep Neural Network (DNN) accelerator for heterogeneous RISC-V systems. It operates in two modes: a redundancy mode utilizing Dual Modular Redundancy (DMR) with hardware-based recovery, and a performance mode repurposing redundant datapaths to maximize parallel throughput. Furthermore, its memory interface is protected by Error Correction Codes (ECCs), and the controller by Triple Modular Redundancy (TMR). Implementation in GlobalFoundries 12nm technology shows a 96 reduction in faulty executions in redundancy mode, with a manageable 15 area overhead. In performance mode, the architecture achieves near-baseline speeds on 3x3 dense convolutions with a 5 throughput and 11 efficiency reduction, compared to 48 and 53 in redundancy mode. This flexibility ensures high overheads are limited to critical tasks, establishing Safe-NEureka as a versatile solution for space applications.",
      "url": "https://arxiv.org/abs/2602.04803",
      "date": "2026-02-04",
      "authors": [
        "Riccardo Tedeschi",
        "Luigi Ghionda",
        "Alessandro Nadalini",
        "Yvan Tortorella",
        "Arpan Suravi Prasad",
        "Luca Benini",
        "Davide Rossi",
        "Francesco Conti"
      ],
      "tags": [
        "eess.SP"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.04803",
      "pdfUrl": "https://arxiv.org/pdf/2602.04803v1",
      "primaryCategory": "eess.SP"
    },
    {
      "id": "arxiv-2602.04717",
      "source": "arxiv",
      "title": "Evolutionary Mapping of Neural Networks to Spatial Accelerators",
      "description": "Spatial accelerators, composed of arrays of compute-memory integrated units, offer an attractive platform for deploying inference workloads with low latency and low energy consumption. However, fully exploiting their architectural advantages typically requires careful, expert-driven mapping of computational graphs to distributed processing elements. In this work, we automate this process by framing the mapping challenge as a black-box optimization problem. We introduce the first evolutionary, hardware-in-the-loop mapping framework for neuromorphic accelerators, enabling users without deep hardware knowledge to deploy workloads more efficiently. We evaluate our approach on Intel Loihi 2, a representative spatial accelerator featuring 152 cores per chip in a 2D mesh. Our method achieves up to 35% reduction in total latency compared to default heuristics on two sparse multi-layer perceptron networks. Furthermore, we demonstrate the scalability of our approach to multi-chip systems and observe an up to 40% improvement in energy efficiency, without explicitly optimizing for it.",
      "url": "https://arxiv.org/abs/2602.04717",
      "date": "2026-02-04",
      "authors": [
        "Alessandro Pierro",
        "Jonathan Timcheck",
        "Jason Yik",
        "Marius Lindauer",
        "Eyke H√ºllermeier",
        "Marcel Wever"
      ],
      "tags": [
        "cs.NE"
      ],
      "matchedGroup": "ai-hardware",
      "arxivId": "2602.04717",
      "pdfUrl": "https://arxiv.org/pdf/2602.04717v1",
      "primaryCategory": "cs.NE"
    },
    {
      "id": "arxiv-2602.04703",
      "source": "arxiv",
      "title": "Knowledge Distillation for mmWave Beam Prediction Using Sub-6 GHz Channels",
      "description": "Beamforming in millimeter-wave (mmWave) high-mobility environments typically incurs substantial training overhead. While prior studies suggest that sub-6 GHz channels can be exploited to predict optimal mmWave beams, existing methods depend on large deep learning (DL) models with prohibitive computational and memory requirements. In this paper, we propose a computationally efficient framework for sub-6 GHz channel-mmWave beam mapping based on the knowledge distillation (KD) technique. We develop two compact student DL architectures based on individual and relational distillation strategies, which retain only a few hidden layers yet closely mimic the performance of large teacher DL models. Extensive simulations demonstrate that the proposed student models achieve the teacher's beam prediction accuracy and spectral efficiency while reducing trainable parameters and computational complexity by 99%.",
      "url": "https://arxiv.org/abs/2602.04703",
      "date": "2026-02-04",
      "authors": [
        "Sina Tavakolian",
        "Nhan Thanh Nguyen",
        "Ahmed Alkhateeb",
        "Markku Juntti"
      ],
      "tags": [
        "eess.SP",
        "cs.LG"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.04703",
      "pdfUrl": "https://arxiv.org/pdf/2602.04703v1",
      "primaryCategory": "eess.SP"
    },
    {
      "id": "arxiv-2602.04677",
      "source": "arxiv",
      "title": "REDistill: Robust Estimator Distillation for Balancing Robustness and Efficiency",
      "description": "Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student by aligning their predictive distributions. However, conventional KD formulations - typically based on Kullback-Leibler divergence - assume that the teacher provides reliable soft targets. In practice, teacher predictions are often noisy or overconfident, and existing correction-based approaches rely on ad-hoc heuristics and extensive hyper-parameter tuning, which hinders generalization. We introduce REDistill (Robust Estimator Distillation), a simple yet principled framework grounded in robust statistics. REDistill replaces the standard KD objective with a power divergence loss, a generalization of KL divergence that adaptively downweights unreliable teacher output while preserving informative logit relationships. This formulation provides a unified and interpretable treatment of teacher noise, requires only logits, integrates seamlessly into existing KD pipelines, and incurs negligible computational overhead. Extensive experiments on CIFAR-100 and ImageNet-1k demonstrate that REDistill consistently improves student accuracy in diverse teacher-student architectures. Remarkably, it achieves these gains without model-specific hyper-parameter tuning, underscoring its robustness and strong generalization to unseen teacher-student pairs.",
      "url": "https://arxiv.org/abs/2602.04677",
      "date": "2026-02-04",
      "authors": [
        "Ondrej Tybl",
        "Lukas Neumann"
      ],
      "tags": [
        "cs.LG",
        "cs.CV"
      ],
      "matchedGroup": "accelerators",
      "arxivId": "2602.04677",
      "pdfUrl": "https://arxiv.org/pdf/2602.04677v1",
      "primaryCategory": "cs.LG"
    },
    {
      "id": "arxiv-2602.04652",
      "source": "arxiv",
      "title": "Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN",
      "description": "Low-density parity-check (LDPC) decoding is one of the most computationally intensive kernels in the 5G New Radio (NR) physical layer and must complete within a 0.5\\,ms transmission time interval while sharing the budget with FFT, channel estimation, demapping, HARQ, and MAC scheduling. Many open and proprietary stacks still execute LDPC on general-purpose CPUs, raising concerns about missed-slot events and limited scalability as bandwidths, modulation orders, and user multiplexing increase. This paper empirically quantifies the benefit of offloading 5G-style LDPC5G decoding from a Grace CPU to the integrated Blackwell GB10 GPU on an NVIDIA DGX~Spark platform. Using NVIDIA Sionna PHY/SYS on TensorFlow, we construct an NR-like link-level chain with an LDPC5G encoder/decoder, 16-QAM modulation, and AWGN, and sweep both the number of codewords decoded in parallel and the number of belief-propagation iterations, timing only the decoding phase while logging CPU and GPU utilization and power. Across the sweep we observe an average GPU/CPU throughput speedup of approximately $6\\times$, with per-codeword CPU latency reaching $\\approx 0.71$\\,ms at 20 iterations (exceeding the 0.5\\,ms slot), while the GB10 GPU remains within 6--24\\% of the slot for the same workloads. Resource-usage measurements show that CPU-based LDPC decoding often consumes around ten Grace cores, whereas GPU-based decoding adds only $\\approx10-15$\\,W over GPU idle while leaving most CPU capacity available for higher-layer tasks. Because our implementation relies on high-level Sionna layers rather than hand-tuned CUDA, these results represent conservative lower bounds on achievable accelerator performance and provide a reusable, scriptable methodology for evaluating LDPC and other physical-layer kernels on future Grace/Blackwell and Aerial/ACAR/AODT platforms.",
      "url": "https://arxiv.org/abs/2602.04652",
      "date": "2026-02-04",
      "authors": [
        "Ryan Barker",
        "Fatemeh Afghah"
      ],
      "tags": [
        "cs.DC"
      ],
      "matchedGroup": "optimization",
      "arxivId": "2602.04652",
      "pdfUrl": "https://arxiv.org/pdf/2602.04652v1",
      "primaryCategory": "cs.DC"
    },
    {
      "id": "arxiv-2602.04595",
      "source": "arxiv",
      "title": "Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference",
      "description": "Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.",
      "url": "https://arxiv.org/abs/2602.04595",
      "date": "2026-02-04",
      "authors": [
        "Xinyu Wang",
        "Jieyu Li",
        "Yanan Sun",
        "Weifeng He"
      ],
      "tags": [
        "cs.AR"
      ],
      "matchedGroup": "model-compression",
      "arxivId": "2602.04595",
      "pdfUrl": "https://arxiv.org/pdf/2602.04595v1",
      "primaryCategory": "cs.AR"
    },
    {
      "id": "arxiv-2602.04582",
      "source": "arxiv",
      "title": "Real-time processing of analog signals on accelerated neuromorphic hardware",
      "description": "Sensory processing with neuromorphic systems is typically done by using either event-based sensors or translating input signals to spikes before presenting them to the neuromorphic processor. Here, we offer an alternative approach: direct analog signal injection eliminates superfluous and power-intensive analog-to-digital and digital-to-analog conversions, making it particularly suitable for efficient near-sensor processing. We demonstrate this by using the accelerated BrainScaleS-2 mixed-signal neuromorphic research platform and interfacing it directly to microphones and a servo-motor-driven actuator. Utilizing BrainScaleS-2's 1000-fold acceleration factor, we employ a spiking neural network to transform interaural time differences into a spatial code and thereby predict the location of sound sources. Our primary contributions are the first demonstrations of direct, continuous-valued sensor data injection into the analog compute units of the BrainScaleS-2 ASIC, and actuator control using its embedded microprocessors. This enables a fully on-chip processing pipeline$\\unicode{x2014}$from sensory input handling, via spiking neural network processing to physical action. We showcase this by programming the system to localize and align a servo motor with the spatial direction of transient noise peaks in real-time.",
      "url": "https://arxiv.org/abs/2602.04582",
      "date": "2026-02-04",
      "authors": [
        "Yannik Stradmann",
        "Johannes Schemmel",
        "Mihai A. Petrovici",
        "Laura Kriener"
      ],
      "tags": [
        "cs.NE"
      ],
      "matchedGroup": "hardware-design",
      "arxivId": "2602.04582",
      "pdfUrl": "https://arxiv.org/pdf/2602.04582v1",
      "primaryCategory": "cs.NE"
    }
  ]
}